{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"- - mode: markdown; mode: visual-line; - - Amazon Web Services for Hydrosat Data Analytics Platform (DAP) \u00b6 This website hosts documents, slides, material, Hands-on videos and information related to the tutorial \u201c AWS for Hydrosat DAP \u201d given during the Hydrosat Training from RDI Consultancy . Covered Topics \u00b6 The objective of this tutorial is to offer a set of practical sessions and tutorials around AWS, for helping Hydrosat deploying their DAP on a cloud environment in order to have a stable and high perfromance work environment. The tutorial is covering the following areas in AWS: Start working on AWS by creating an account AWS Computing Run Python Scripts on EC2 Web servers on EC2 instances AWS Lightsail Other Computing Services: Serverless and Containers Storage volumes in AWS Networking in AWS Relational databases in AWS Load balancing and Scalling in AWS AWS Pricing and Support AWS Security and Access Management Beside the tutorial provided in these pages, there are some general concepts slides are provided such as: Table of Contents \u00b6 The following table summerizes the AWS Hands on training sessions for Hydrosat: Topic Concepts Technical Video Tutorial Understanding Cloud Concepts slides slides Introduction to AWS Infrastructure slides Creating an account on AWS slides watch link AWS Computing slides slides watch link Run python scripts on EC2 watch link Storage volumes in AWS slides slides watch link Docker Images on AWS slides slides link Networking in AWS slides slides link Load balancing and Scalling in AWS slides slides link AWS Pricing and Cost Mangement slides slides link AWS Support Plans slides slides link AWS Security and Access Management slides slides link Credits \u00b6 See the main contributor in the contact section Licence \u00b6 This project and the sources proposed within this repository are released under the terms of the GPL-3.0 licence.","title":"Home"},{"location":"#amazon-web-services-for-hydrosat-data-analytics-platform-dap","text":"This website hosts documents, slides, material, Hands-on videos and information related to the tutorial \u201c AWS for Hydrosat DAP \u201d given during the Hydrosat Training from RDI Consultancy .","title":"Amazon Web Services for Hydrosat Data Analytics Platform (DAP)"},{"location":"#covered-topics","text":"The objective of this tutorial is to offer a set of practical sessions and tutorials around AWS, for helping Hydrosat deploying their DAP on a cloud environment in order to have a stable and high perfromance work environment. The tutorial is covering the following areas in AWS: Start working on AWS by creating an account AWS Computing Run Python Scripts on EC2 Web servers on EC2 instances AWS Lightsail Other Computing Services: Serverless and Containers Storage volumes in AWS Networking in AWS Relational databases in AWS Load balancing and Scalling in AWS AWS Pricing and Support AWS Security and Access Management Beside the tutorial provided in these pages, there are some general concepts slides are provided such as:","title":"Covered Topics"},{"location":"#table-of-contents","text":"The following table summerizes the AWS Hands on training sessions for Hydrosat: Topic Concepts Technical Video Tutorial Understanding Cloud Concepts slides slides Introduction to AWS Infrastructure slides Creating an account on AWS slides watch link AWS Computing slides slides watch link Run python scripts on EC2 watch link Storage volumes in AWS slides slides watch link Docker Images on AWS slides slides link Networking in AWS slides slides link Load balancing and Scalling in AWS slides slides link AWS Pricing and Cost Mangement slides slides link AWS Support Plans slides slides link AWS Security and Access Management slides slides link","title":"Table of Contents"},{"location":"#credits","text":"See the main contributor in the contact section","title":"Credits"},{"location":"#licence","text":"This project and the sources proposed within this repository are released under the terms of the GPL-3.0 licence.","title":"Licence"},{"location":"AWS_Account/","text":"Setting Up an AWS Account \u00b6 Create your account \u00b6 In order to sign up to AWS services, please follow the following steps: Open the Amazon Web Services home page. Choose Create an AWS Account. Note: If you signed in to AWS recently, choose Sign in to the Console. If Create a new AWS account isn\u2019t visible, first choose Sign in to a different account, and then choose Create a new AWS account. Enter your account information, and then choose Continue. Be sure that you enter your account information correctly, especially your email address. If you enter your email address incorrectly, you can\u2019t access your account. Choose Personal or Professional. Note: Personal accounts and professional accounts have the same features and functions. Enter your company or personal information. Important: For professional AWS accounts, it\u2019s a best practice to enter the company phone number rather than a personal cell phone. Configuring a root account with an individual email address or a personal phone number can make your account insecure. Read and accept the AWS Customer Agreement. Note: Be sure that you read and understand the terms of the AWS Customer Agreement. Choose Create Account and Continue. You receive an email to confirm that your account is created. You can sign in to your new account using the email address and password you registered with. However, you can\u2019t use AWS services until you finish activating your account. Add a Payment Method \u00b6 On the Payment Information page, enter the information about your payment method, and then choose Verify and Add. Note: If you want to use a different billing address for your AWS billing information, choose Use a new address before you choose Verify and Add. Important: You cannot proceed with the sign-up process until you add a valid payment method. Verify your phone number \u00b6 Choose your country or region code from the list. Enter a phone number where you can be reached in the next few minutes. Enter the code displayed in the captcha and then submit. In a few moments, an automated system contacts you. Enter the PIN you receive, and then choose Continue. Choose an AWS Support plan \u00b6 On the Select a Support Plan page, choose one of the available Support plans. For a description of the available Support plans and their benefits, see Compare AWS Support Plans . Seting up Your Budget \u00b6 You need to set up your budget, in order to avoid any extra charges,. Any extra charges will be aletrted to you before going into action. Please follow the following steps to fix your budget: After you received you registration confirmation, sign in to the console . After a successed login to the console, naviaget to your account name, and choose My Billing Daschboard Choose on the left Budgets which is located under Cost Management, Then Press Craete Budget and choose the Cost budget, then press Set your budget Name the budget, period and budget amount, then press Configure alerts Choose the alert threshold, for example 50% Put your email address to send you alert when your charges become 50% from your budget Watch it here \u00b6","title":"Setting Up an AWS Account"},{"location":"AWS_Account/#setting-up-an-aws-account","text":"","title":"Setting Up an AWS Account"},{"location":"AWS_Account/#create-your-account","text":"In order to sign up to AWS services, please follow the following steps: Open the Amazon Web Services home page. Choose Create an AWS Account. Note: If you signed in to AWS recently, choose Sign in to the Console. If Create a new AWS account isn\u2019t visible, first choose Sign in to a different account, and then choose Create a new AWS account. Enter your account information, and then choose Continue. Be sure that you enter your account information correctly, especially your email address. If you enter your email address incorrectly, you can\u2019t access your account. Choose Personal or Professional. Note: Personal accounts and professional accounts have the same features and functions. Enter your company or personal information. Important: For professional AWS accounts, it\u2019s a best practice to enter the company phone number rather than a personal cell phone. Configuring a root account with an individual email address or a personal phone number can make your account insecure. Read and accept the AWS Customer Agreement. Note: Be sure that you read and understand the terms of the AWS Customer Agreement. Choose Create Account and Continue. You receive an email to confirm that your account is created. You can sign in to your new account using the email address and password you registered with. However, you can\u2019t use AWS services until you finish activating your account.","title":"Create your account"},{"location":"AWS_Account/#add-a-payment-method","text":"On the Payment Information page, enter the information about your payment method, and then choose Verify and Add. Note: If you want to use a different billing address for your AWS billing information, choose Use a new address before you choose Verify and Add. Important: You cannot proceed with the sign-up process until you add a valid payment method.","title":"Add a Payment Method"},{"location":"AWS_Account/#verify-your-phone-number","text":"Choose your country or region code from the list. Enter a phone number where you can be reached in the next few minutes. Enter the code displayed in the captcha and then submit. In a few moments, an automated system contacts you. Enter the PIN you receive, and then choose Continue.","title":"Verify your phone number"},{"location":"AWS_Account/#choose-an-aws-support-plan","text":"On the Select a Support Plan page, choose one of the available Support plans. For a description of the available Support plans and their benefits, see Compare AWS Support Plans .","title":"Choose an AWS Support plan"},{"location":"AWS_Account/#seting-up-your-budget","text":"You need to set up your budget, in order to avoid any extra charges,. Any extra charges will be aletrted to you before going into action. Please follow the following steps to fix your budget: After you received you registration confirmation, sign in to the console . After a successed login to the console, naviaget to your account name, and choose My Billing Daschboard Choose on the left Budgets which is located under Cost Management, Then Press Craete Budget and choose the Cost budget, then press Set your budget Name the budget, period and budget amount, then press Configure alerts Choose the alert threshold, for example 50% Put your email address to send you alert when your charges become 50% from your budget","title":"Seting up Your Budget"},{"location":"AWS_Account/#watch-it-here","text":"","title":"Watch it here"},{"location":"AmazonEBS/","text":"","title":"AmazonEBS"},{"location":"AutoScale/","text":"Amazon EC2 Auto Scaling \u00b6 Amazon EC2 Auto Scaling helps you maintain application availability, and it allows you to dynamically scale your Amazon EC2 capacity up or down automatically according to conditions that you define. You can use Amazon EC2 Auto Scaling for fleet management of Amazon EC2 instances, which can help maintain the health and availability of your fleet, and ensure that you are running your desired number of Amazon EC2 instances. You can also use Amazon EC2 Auto Scaling to dynamically scale Amazon EC2 instances. Dynamic scaling automatically increases the number of Amazon EC2 instances during demand spikes to maintain performance and decrease capacity during lulls, which can help reduce costs. Amazon EC2 Auto Scaling is well-suited to applications that have stable demand patterns, or applications that experience hourly, daily, or weekly variability in usage. Auto Scaling: Add or remove compute capacity to meet changes in demand \u00b6 Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove EC2 instances according to conditions you define. You can use the fleet management features of EC2 Auto Scaling to maintain the health and availability of your fleet. You can also use the dynamic and predictive scaling features of EC2 Auto Scaling to add or remove EC2 instances. Dynamic scaling responds to changing demand and predictive scaling automatically schedules the right number of EC2 instances based on predicted demand. Dynamic scaling and predictive scaling can be used together to scale faster. Benefits \u00b6 Improve Fault Tolerance Increase Application Availability Lower Costs How it works \u00b6 Fleet Management \u00b6 Whether you are running one Amazon EC2 instance or thousands, you can use Amazon EC2 Auto Scaling to detect impaired Amazon EC2 instances and unhealthy applications, and replace the instances without your intervention. This ensures that your application is getting the compute capacity that you expect. Amazon EC2 Auto Scaling will perform three main functions to automate fleet management for EC2 instances: Monitor the health of running instances Amazon EC2 Auto Scaling ensures that your application is able to receive traffic and that EC2 instances are working properly. Amazon EC2 Auto Scaling periodically performs health checks to identify any instances that are unhealthy. Replace impaired instances automatically When an impaired instance fails a health check, Amazon EC2 Auto Scaling automatically terminates it and replaces it with a new one. That means that you don\u2019t need to respond manually when an instance needs replacing. Balance capacity across Availability Zones Amazon EC2 Auto Scaling can automatically balance instances across zones, and always launches new instances so that they are balanced between zones as evenly as possible across your entire fleet. Learn more about Fleet Management, see this blog Scheduled Scaling \u00b6 Scaling based on a schedule allows you to scale your application ahead of known load changes. For example, every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling activities based on the known traffic patterns of your web application. Dynamic Scaling \u00b6 Amazon EC2 Auto Scaling enables you to follow the demand curve for your applications closely, reducing the need to manually provision Amazon EC2 capacity in advance. For example, you can use target tracking scaling policies to select a load metric for your application, such as CPU utilization. Or, you could set a target value using the new \u201cRequest Count Per Target\u201d metric from Application Load Balancer, a load balancing option for the Elastic Load Balancing service. Amazon EC2 Auto Scaling will then automatically adjust the number of EC2 instances as needed to maintain your target. Auto Scaling Prime Time: Target Tracking Hits the Bullseye at Netflix Predictive Scaling \u00b6 Predictive Scaling, a feature of AWS Auto Scaling uses machine learning to schedule the right number of EC2 instances in anticipation of approaching traffic changes. Predictive Scaling predicts future traffic, including regularly-occurring spikes, and provisions the right number of EC2 instances in advance. Predictive Scaling\u2019s machine learning algorithms detect changes in daily and weekly patterns, automatically adjusting their forecasts. This removes the need for manual adjustment of Auto Scaling parameters as cyclicality changes over time, making Auto Scaling simpler to configure. Auto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications. Amazon EC2 Auto Scaling Features \u00b6 Automatically scale in and out Choose when and how to scale Fleet management Predictive Scaling Support for multiple purchase models, instance types, and AZs Included with Amazon EC2 Amazon EC2 Auto Scaling Getting Started \u00b6 There are several ways to get started with Amazon EC2 Auto Scaling. Check, the Getting started with Amazon EC2 Auto Scaling and do the following: Step 1: Sign into the AWS Management Console \u00b6 Create an account and sign into the console. With Amazon EC2, you pay only for what you use. If you are a new AWS customer, you can get started with Amazon EC2 for free. For more information, see AWS Free Tier. Step 2: Create a launch template \u00b6 In the Amazon EC2 Dashboard, choose \u201cLaunch Templates\u201d to create a launch template, specifying a name, AMI, instance type, and other details. Below are some guidelines on setting up your first launch template. Choose an Amazon Machine Image (AMI): We recommend the Amazon Linux 2 AMI (free-tier eligible). Choose an instance type: We recommend the t2.micro (free-tier eligible). Security group: You have the option to configure your virtual firewall. Step 3: Create an Auto Scaling group \u00b6 Using the Auto Scaling wizard, create an Auto Scaling group specifying a name, size, and network for your Auto Scaling group. Step 4: Add Elastic Load Balancers (Optional) \u00b6 When you set up Auto Scaling with Elastic Load Balancing, you can automatically distribute incoming application traffic across Amazon EC2 instances within your Auto Scaling groups to build highly available, fault-tolerant applications. Associate your load balancer with your Auto Scaling group to distribute traffic for your application across a fleet of EC2 instances that can scale with demand. You must first create a load balancer. Step 5: Configure Scaling Policies (Optional) \u00b6 Configure scaling policies for your Amazon EC2 Auto Scaling group.","title":"Auto Scaling in AWS"},{"location":"AutoScale/#amazon-ec2-auto-scaling","text":"Amazon EC2 Auto Scaling helps you maintain application availability, and it allows you to dynamically scale your Amazon EC2 capacity up or down automatically according to conditions that you define. You can use Amazon EC2 Auto Scaling for fleet management of Amazon EC2 instances, which can help maintain the health and availability of your fleet, and ensure that you are running your desired number of Amazon EC2 instances. You can also use Amazon EC2 Auto Scaling to dynamically scale Amazon EC2 instances. Dynamic scaling automatically increases the number of Amazon EC2 instances during demand spikes to maintain performance and decrease capacity during lulls, which can help reduce costs. Amazon EC2 Auto Scaling is well-suited to applications that have stable demand patterns, or applications that experience hourly, daily, or weekly variability in usage.","title":"Amazon EC2 Auto Scaling"},{"location":"AutoScale/#auto-scaling-add-or-remove-compute-capacity-to-meet-changes-in-demand","text":"Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove EC2 instances according to conditions you define. You can use the fleet management features of EC2 Auto Scaling to maintain the health and availability of your fleet. You can also use the dynamic and predictive scaling features of EC2 Auto Scaling to add or remove EC2 instances. Dynamic scaling responds to changing demand and predictive scaling automatically schedules the right number of EC2 instances based on predicted demand. Dynamic scaling and predictive scaling can be used together to scale faster.","title":"Auto Scaling: Add or remove compute capacity to meet changes in demand"},{"location":"AutoScale/#benefits","text":"Improve Fault Tolerance Increase Application Availability Lower Costs","title":"Benefits"},{"location":"AutoScale/#how-it-works","text":"","title":"How it works"},{"location":"AutoScale/#fleet-management","text":"Whether you are running one Amazon EC2 instance or thousands, you can use Amazon EC2 Auto Scaling to detect impaired Amazon EC2 instances and unhealthy applications, and replace the instances without your intervention. This ensures that your application is getting the compute capacity that you expect. Amazon EC2 Auto Scaling will perform three main functions to automate fleet management for EC2 instances: Monitor the health of running instances Amazon EC2 Auto Scaling ensures that your application is able to receive traffic and that EC2 instances are working properly. Amazon EC2 Auto Scaling periodically performs health checks to identify any instances that are unhealthy. Replace impaired instances automatically When an impaired instance fails a health check, Amazon EC2 Auto Scaling automatically terminates it and replaces it with a new one. That means that you don\u2019t need to respond manually when an instance needs replacing. Balance capacity across Availability Zones Amazon EC2 Auto Scaling can automatically balance instances across zones, and always launches new instances so that they are balanced between zones as evenly as possible across your entire fleet. Learn more about Fleet Management, see this blog","title":"Fleet Management"},{"location":"AutoScale/#scheduled-scaling","text":"Scaling based on a schedule allows you to scale your application ahead of known load changes. For example, every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling activities based on the known traffic patterns of your web application.","title":"Scheduled Scaling"},{"location":"AutoScale/#dynamic-scaling","text":"Amazon EC2 Auto Scaling enables you to follow the demand curve for your applications closely, reducing the need to manually provision Amazon EC2 capacity in advance. For example, you can use target tracking scaling policies to select a load metric for your application, such as CPU utilization. Or, you could set a target value using the new \u201cRequest Count Per Target\u201d metric from Application Load Balancer, a load balancing option for the Elastic Load Balancing service. Amazon EC2 Auto Scaling will then automatically adjust the number of EC2 instances as needed to maintain your target. Auto Scaling Prime Time: Target Tracking Hits the Bullseye at Netflix","title":"Dynamic Scaling"},{"location":"AutoScale/#predictive-scaling","text":"Predictive Scaling, a feature of AWS Auto Scaling uses machine learning to schedule the right number of EC2 instances in anticipation of approaching traffic changes. Predictive Scaling predicts future traffic, including regularly-occurring spikes, and provisions the right number of EC2 instances in advance. Predictive Scaling\u2019s machine learning algorithms detect changes in daily and weekly patterns, automatically adjusting their forecasts. This removes the need for manual adjustment of Auto Scaling parameters as cyclicality changes over time, making Auto Scaling simpler to configure. Auto Scaling enhanced with Predictive Scaling delivers faster, simpler, and more accurate capacity provisioning resulting in lower cost and more responsive applications.","title":"Predictive Scaling"},{"location":"AutoScale/#amazon-ec2-auto-scaling-features","text":"Automatically scale in and out Choose when and how to scale Fleet management Predictive Scaling Support for multiple purchase models, instance types, and AZs Included with Amazon EC2","title":"Amazon EC2 Auto Scaling Features"},{"location":"AutoScale/#amazon-ec2-auto-scaling-getting-started","text":"There are several ways to get started with Amazon EC2 Auto Scaling. Check, the Getting started with Amazon EC2 Auto Scaling and do the following:","title":"Amazon EC2 Auto Scaling Getting Started"},{"location":"AutoScale/#step-1-sign-into-the-aws-management-console","text":"Create an account and sign into the console. With Amazon EC2, you pay only for what you use. If you are a new AWS customer, you can get started with Amazon EC2 for free. For more information, see AWS Free Tier.","title":"Step 1: Sign into the AWS Management Console"},{"location":"AutoScale/#step-2-create-a-launch-template","text":"In the Amazon EC2 Dashboard, choose \u201cLaunch Templates\u201d to create a launch template, specifying a name, AMI, instance type, and other details. Below are some guidelines on setting up your first launch template. Choose an Amazon Machine Image (AMI): We recommend the Amazon Linux 2 AMI (free-tier eligible). Choose an instance type: We recommend the t2.micro (free-tier eligible). Security group: You have the option to configure your virtual firewall.","title":"Step 2: Create a launch template"},{"location":"AutoScale/#step-3-create-an-auto-scaling-group","text":"Using the Auto Scaling wizard, create an Auto Scaling group specifying a name, size, and network for your Auto Scaling group.","title":"Step 3: Create an Auto Scaling group"},{"location":"AutoScale/#step-4-add-elastic-load-balancers-optional","text":"When you set up Auto Scaling with Elastic Load Balancing, you can automatically distribute incoming application traffic across Amazon EC2 instances within your Auto Scaling groups to build highly available, fault-tolerant applications. Associate your load balancer with your Auto Scaling group to distribute traffic for your application across a fleet of EC2 instances that can scale with demand. You must first create a load balancer.","title":"Step 4: Add Elastic Load Balancers (Optional)"},{"location":"AutoScale/#step-5-configure-scaling-policies-optional","text":"Configure scaling policies for your Amazon EC2 Auto Scaling group.","title":"Step 5: Configure Scaling Policies (Optional)"},{"location":"DynamoDB/","text":"Popular key-value databases: Amazon DynamoDB \u00b6 Amazon DynamoDB is a fast and flexible NoSQL database service for applications that need consistent, single-digit millisecond latency at any scale. Its a fully managed cloud database, and it supports both document and key-value store models. Its flexible data model, reliable performance, and automatic scaling of throughput capacity make it a great fit for mobile, web, gaming, advertising technology (ad tech), Internet of Things (IoT), and many other applications. Check, Amazon DynamoDB Pricing before starte deply your NoSQL database on it. The key-value database defined \u00b6 A key-value database is a type of nonrelational database that uses a simple key-value method to store data. A key-value database stores data as a collection of key-value pairs in which a key serves as a unique identifier. Both keys and values can be anything, ranging from simple objects to complex compound objects. Key-value databases are highly partitionable and allow horizontal scaling at scales that other types of databases cannot achieve. For example, Amazon DynamoDB allocates additional partitions to a table if an existing partition fills to capacity and more storage space is required. The following diagram shows an example of data stored as key-value pairs in DynamoDB. DynamoDB \u00b6 Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It\u2019s a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. Many of the world\u2019s fastest growing businesses such as Lyft, Airbnb, and Redfin as well as enterprises such as Samsung, Toyota, and Capital One depend on the scale and performance of DynamoDB to support their mission-critical workloads. Hundreds of thousands of AWS customers have chosen DynamoDB as their key-value and document database for mobile, web, gaming, ad tech, IoT, and other applications that need low-latency data access at any scale. Create a new table for your application and let DynamoDB handle the rest. Benefits & Applications \u00b6 Benefits Performance at scale No servers to manage Enterprise ready Applications Serverless Web Apps Mobile Backends Microservices Use Cases \u00b6 Ad Tech Retail Gaming Banking and Finance Media and entertainment Software and internet Session store Shopping cart Creating a Table from SQL to NoSQL \u00b6 Tables are the fundamental data structures in relational databases and in Amazon DynamoDB. A relational database management system (RDBMS) requires you to define the table\u2019s schema when you create it. In contrast, DynamoDB tables are schemaless\u2014other than the primary key, you do not need to define any extra attributes or data types when you create a table. SQL \u00b6 Use the CREATE TABLE statement to create a table, as shown in the following example. CREATE TABLE Music ( Artist VARCHAR(20) NOT NULL, SongTitle VARCHAR(30) NOT NULL, AlbumTitle VARCHAR(25), Year INT, Price FLOAT, Genre VARCHAR(10), Tags TEXT, PRIMARY KEY(Artist, SongTitle) ); The primary key for this table consists of Artist and SongTitle. You must define all of the table\u2019s columns and data types, and the table\u2019s primary key. (You can use the ALTER TABLE statement to change these definitions later, if necessary.) Many SQL implementations let you define storage specifications for your table, as part of the CREATE TABLE statement. Unless you indicate otherwise, the table is created with default storage settings. In a production environment, a database administrator can help determine the optimal storage parameters. NoSQL: DynamoDB \u00b6 Use the CreateTable action to create a provisioned mode table, specifying parameters as shown following: { TableName : \"Music\", KeySchema: [ { AttributeName: \"Artist\", KeyType: \"HASH\", //Partition key }, { AttributeName: \"SongTitle\", KeyType: \"RANGE\" //Sort key } ], AttributeDefinitions: [ { AttributeName: \"Artist\", AttributeType: \"S\" }, { AttributeName: \"SongTitle\", AttributeType: \"S\" } ], ProvisionedThroughput: { // Only specified if using provisioned mode ReadCapacityUnits: 1, WriteCapacityUnits: 1 } } The primary key for this table consists of Artist (partition key) and SongTitle (sort key). Setting Up DynamoDB on AWS \u00b6 Before starting the development of your tables in DynamoDB, you have to: Have an AWS account AWS Access Key if you will use DynamoDB from CLI or SDK Configuring Your Credentials Getting an AWS Access Key \u00b6 Before you can access DynamoDB programmatically or through the AWS Command Line Interface (AWS CLI), you must have an AWS access key. You don\u2019t need an access key if you plan to use the DynamoDB console only. Access keys consist of an access key ID and secret access key, which are used to sign programmatic requests that you make to AWS. If you don\u2019t have access keys, you can create them from the AWS Management Console. As a best practice, do not use the AWS account root user access keys for any task where it\u2019s not required. Instead, create a new administrator IAM user with access keys for yourself. Check how to create the AWS Access Key . Configuring Your Credentials \u00b6 Before you can access DynamoDB programmatically or through the AWS CLI, you must configure your credentials to enable authorization for your applications. There are several ways to do this. For example, you can manually create the credentials file to store your access key ID and secret access key. You also can use the aws configure command of the AWS CLI to automatically create the file. Alternatively, you can use environment variables. For more information about configuring your credentials, see the programming-specific AWS SDK developer guide. To install and configure the AWS CLI, see Using the AWS CLI . Getting Started with Amazon DynamoDB \u00b6 Use the hands-on tutorials in this section to help you get started and learn more about Amazon DynamoDB. Step 1: Create a Table Step 2: Write Data to a Table Using the Console or AWS CLI Step 3: Read Data from a Table Step 4: Update Data in a Table Step 5: Query Data in a Table Step 6: Create a Global Secondary Index Step 7: Query the Global Secondary Index Step 8: (Optional) Clean Up Resources Getting Started with DynamoDB: Next Steps Demo: Create and Query a NoSQL Table \u00b6 More advanced tutorials and demos \u00b6","title":"Amazon DynamoDB"},{"location":"DynamoDB/#popular-key-value-databases-amazon-dynamodb","text":"Amazon DynamoDB is a fast and flexible NoSQL database service for applications that need consistent, single-digit millisecond latency at any scale. Its a fully managed cloud database, and it supports both document and key-value store models. Its flexible data model, reliable performance, and automatic scaling of throughput capacity make it a great fit for mobile, web, gaming, advertising technology (ad tech), Internet of Things (IoT), and many other applications. Check, Amazon DynamoDB Pricing before starte deply your NoSQL database on it.","title":"Popular key-value databases: Amazon DynamoDB"},{"location":"DynamoDB/#the-key-value-database-defined","text":"A key-value database is a type of nonrelational database that uses a simple key-value method to store data. A key-value database stores data as a collection of key-value pairs in which a key serves as a unique identifier. Both keys and values can be anything, ranging from simple objects to complex compound objects. Key-value databases are highly partitionable and allow horizontal scaling at scales that other types of databases cannot achieve. For example, Amazon DynamoDB allocates additional partitions to a table if an existing partition fills to capacity and more storage space is required. The following diagram shows an example of data stored as key-value pairs in DynamoDB.","title":"The key-value database defined"},{"location":"DynamoDB/#dynamodb","text":"Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It\u2019s a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. Many of the world\u2019s fastest growing businesses such as Lyft, Airbnb, and Redfin as well as enterprises such as Samsung, Toyota, and Capital One depend on the scale and performance of DynamoDB to support their mission-critical workloads. Hundreds of thousands of AWS customers have chosen DynamoDB as their key-value and document database for mobile, web, gaming, ad tech, IoT, and other applications that need low-latency data access at any scale. Create a new table for your application and let DynamoDB handle the rest.","title":"DynamoDB"},{"location":"DynamoDB/#benefits-applications","text":"Benefits Performance at scale No servers to manage Enterprise ready Applications Serverless Web Apps Mobile Backends Microservices","title":"Benefits &amp; Applications"},{"location":"DynamoDB/#use-cases","text":"Ad Tech Retail Gaming Banking and Finance Media and entertainment Software and internet Session store Shopping cart","title":"Use Cases"},{"location":"DynamoDB/#creating-a-table-from-sql-to-nosql","text":"Tables are the fundamental data structures in relational databases and in Amazon DynamoDB. A relational database management system (RDBMS) requires you to define the table\u2019s schema when you create it. In contrast, DynamoDB tables are schemaless\u2014other than the primary key, you do not need to define any extra attributes or data types when you create a table.","title":"Creating a Table  from SQL to NoSQL"},{"location":"DynamoDB/#sql","text":"Use the CREATE TABLE statement to create a table, as shown in the following example. CREATE TABLE Music ( Artist VARCHAR(20) NOT NULL, SongTitle VARCHAR(30) NOT NULL, AlbumTitle VARCHAR(25), Year INT, Price FLOAT, Genre VARCHAR(10), Tags TEXT, PRIMARY KEY(Artist, SongTitle) ); The primary key for this table consists of Artist and SongTitle. You must define all of the table\u2019s columns and data types, and the table\u2019s primary key. (You can use the ALTER TABLE statement to change these definitions later, if necessary.) Many SQL implementations let you define storage specifications for your table, as part of the CREATE TABLE statement. Unless you indicate otherwise, the table is created with default storage settings. In a production environment, a database administrator can help determine the optimal storage parameters.","title":"SQL"},{"location":"DynamoDB/#nosql-dynamodb","text":"Use the CreateTable action to create a provisioned mode table, specifying parameters as shown following: { TableName : \"Music\", KeySchema: [ { AttributeName: \"Artist\", KeyType: \"HASH\", //Partition key }, { AttributeName: \"SongTitle\", KeyType: \"RANGE\" //Sort key } ], AttributeDefinitions: [ { AttributeName: \"Artist\", AttributeType: \"S\" }, { AttributeName: \"SongTitle\", AttributeType: \"S\" } ], ProvisionedThroughput: { // Only specified if using provisioned mode ReadCapacityUnits: 1, WriteCapacityUnits: 1 } } The primary key for this table consists of Artist (partition key) and SongTitle (sort key).","title":"NoSQL: DynamoDB"},{"location":"DynamoDB/#setting-up-dynamodb-on-aws","text":"Before starting the development of your tables in DynamoDB, you have to: Have an AWS account AWS Access Key if you will use DynamoDB from CLI or SDK Configuring Your Credentials","title":"Setting Up DynamoDB on AWS"},{"location":"DynamoDB/#getting-an-aws-access-key","text":"Before you can access DynamoDB programmatically or through the AWS Command Line Interface (AWS CLI), you must have an AWS access key. You don\u2019t need an access key if you plan to use the DynamoDB console only. Access keys consist of an access key ID and secret access key, which are used to sign programmatic requests that you make to AWS. If you don\u2019t have access keys, you can create them from the AWS Management Console. As a best practice, do not use the AWS account root user access keys for any task where it\u2019s not required. Instead, create a new administrator IAM user with access keys for yourself. Check how to create the AWS Access Key .","title":"Getting an AWS Access Key"},{"location":"DynamoDB/#configuring-your-credentials","text":"Before you can access DynamoDB programmatically or through the AWS CLI, you must configure your credentials to enable authorization for your applications. There are several ways to do this. For example, you can manually create the credentials file to store your access key ID and secret access key. You also can use the aws configure command of the AWS CLI to automatically create the file. Alternatively, you can use environment variables. For more information about configuring your credentials, see the programming-specific AWS SDK developer guide. To install and configure the AWS CLI, see Using the AWS CLI .","title":"Configuring Your Credentials"},{"location":"DynamoDB/#getting-started-with-amazon-dynamodb","text":"Use the hands-on tutorials in this section to help you get started and learn more about Amazon DynamoDB. Step 1: Create a Table Step 2: Write Data to a Table Using the Console or AWS CLI Step 3: Read Data from a Table Step 4: Update Data in a Table Step 5: Query Data in a Table Step 6: Create a Global Secondary Index Step 7: Query the Global Secondary Index Step 8: (Optional) Clean Up Resources Getting Started with DynamoDB: Next Steps","title":"Getting Started with Amazon DynamoDB"},{"location":"DynamoDB/#demo-create-and-query-a-nosql-table","text":"","title":"Demo: Create and Query a NoSQL Table"},{"location":"DynamoDB/#more-advanced-tutorials-and-demos","text":"","title":"More advanced tutorials and demos"},{"location":"EBS/","text":"Amazon EBS volumes \u00b6 An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. For current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes. You can use EBS volumes as primary storage for data that requires frequent updates, such as the system drive for an instance or storage for a database application. You can also use them for throughput-intensive applications that perform continuous disk scans. EBS volumes persist independently from the running life of an EC2 instance. You can attach multiple EBS volumes to a single instance. The volume and instance must be in the same Availability Zone. Depending on the volume and instance types, you can use Multi-Attach to mount a volume to multiple instances at the same time. Amazon EBS provides the following volume types: General Purpose SSD ( gp2 ), Provisioned IOPS SSD ( io1 ), Throughput Optimized HDD ( st1 ), Cold HDD ( sc1 ), and Magnetic ( standard , a previous-generation type). They differ in performance characteristics and price, allowing you to tailor your storage performance and cost to the needs of your applications. For more information, see Amazon EBS volume types . Your account has a limit on the number of EBS volumes that you can use, and the total storage available to you. For more information about these limits, and how to request an increase in your limits, see Amazon EC2 Service Quotas . What to know befor creating an EBS volume \u00b6 Benefits of using EBS volumes Amazon EBS volume types Constraints on the size and configuration of an EBS volume Creating an Amazon EBS volume \u00b6 You can create an Amazon EBS volume and then attach to any EC2 instance in the same Availability Zone. If you create an encrypted EBS volume, you can only attach it to supported instance types. For more information, see Supported instance types . If you are creating a volume for a high-performance storage scenario, you should make sure to use a Provisioned IOPS SSD (io1) volume and attach it to an instance with enough bandwidth to support your application, such as an EBS-optimized instance or an instance with 10-Gigabit network connectivity. The same advice holds for Throughput Optimized HDD (st1) and Cold HDD (sc1) volumes. For more information, see Amazon EBS\u2013optimized instances . Empty EBS volumes receive their maximum performance the moment that they are available and do not require initialization (formerly known as pre-warming). However, storage blocks on volumes that were created from snapshots must be initialized (pulled down from Amazon S3 and written to the volume) before you can access the block. This preliminary action takes time and can cause a significant increase in the latency of an I/O operation the first time each block is accessed. Volume performance is achieved after all blocks have been downloaded and written to the volume. For most applications, amortizing this cost over the lifetime of the volume is acceptable. To avoid this initial performance hit in a production environment, you can force immediate initialization of the entire volume or enable fast snapshot restore. For more information, see Initializing Amazon EBS volumes . Methods of creating a volume \u00b6 Create and attach EBS volumes when you launch instances by specifying a block device mapping. For more information, see Launching an instance using the Launch Instance Wizard and Block device mapping . Create an empty EBS volume and attach it to a running instance. For more information, see Creating an empty volume below. Create an EBS volume from a previously created snapshot and attach it to a running instance. For more information, see Creating a volume from a snapshot below. Creating an empty volume \u00b6 Empty volumes receive their maximum performance the moment that they are available and do not require initialization . To create a empty EBS volume using the console \u00b6 Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the navigation bar, select the Region in which you would like to create your volume. This choice is important because some Amazon EC2 resources can be shared between Regions, while others can\u2019t. For more information, see Resource locations . In the navigation pane, choose ELASTIC BLOCK STORE, Volumes . Choose Create Volume . For Volume Type , choose a volume type. For more information, see Amazon EBS volume types . For Size (GiB) , type the size of the volume. For more information, see Constraints on the size and configuration of an EBS volume . With a Provisioned IOPS SSD volume, for IOPS , type the maximum number of input/output operations per second (IOPS) that the volume should support. For Availability Zone , choose the Availability Zone in which to create the volume. EBS volumes can only be attached to EC2 instances within the same Availability Zone. (Optional) If the instance type supports EBS encryption and you want to encrypt the volume, select Encrypt this volume and choose a CMK . If encryption by default is enabled in this Region, EBS encryption is enabled and the default CMK for EBS encryption is chosen. You can choose a different CMK from Master Key or paste the full ARN of any key that you can access. For more information, see Amazon EBS encryption . (Optional) Choose Create additional tags to add tags to the volume. For each tag, provide a tag key and a tag value. For more information, see Tagging your Amazon EC2 resources . Choose Create Volume . The volume is ready for use when the volume status is Available . To use your new volume, attach it to an instance, format it, and mount it. For more information, see Attaching an Amazon EBS volume to an instance . To create an empty EBS volume using the command line \u00b6 You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . create-volume (AWS CLI) New-EC2Volume (AWS Tools for Windows PowerShell) Creating a volume from a snapshot \u00b6 Volumes created from snapshots load lazily in the background. This means that there is no need to wait for all of the data to transfer from Amazon S3 to your EBS volume before the instance can start accessing an attached volume and all its data. If your instance accesses data that hasn\u2019t yet been loaded, the volume immediately downloads the requested data from Amazon S3, and then continues loading the rest of the volume data in the background. Volume performance is achieved after all blocks are downloaded and written to the volume. To avoid the initial performance hit in a production environment, see Initializing Amazon EBS volumes . New EBS volumes that are created from encrypted snapshots are automatically encrypted. You can also encrypt a volume on-the-fly while restoring it from an unencrypted snapshot. Encrypted volumes can only be attached to instance types that support EBS encryption. For more information, see Supported instance types . To create an EBS volume from a snapshot using the console \u00b6 Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the navigation bar, select the Region that your snapshot is located in. To use the snapshot to create a volume in a different region, copy your snapshot to the new Region and then use it to create a volume in that Region. For more information, see Copying an Amazon EBS snapshot . In the navigation pane, choose ELASTIC BLOCK STORE, Volumes . Choose Create Volume . For Volume Type , choose a volume type. For more information, see Amazon EBS volume types . For Snapshot ID , start typing the ID or description of the snapshot from which you are restoring the volume, and choose it from the list of suggested options. (Optional) Select Encrypt this volume to change the encryption state of your volume. This is optional if encryption by default is enabled. Select a CMK from Master Key to specify a CMK other than the default CMK for EBS encryption. For Size (GiB) , type the size of the volume, or verify that the default size of the snapshot is adequate. If you specify both a volume size and a snapshot, the size must be equal to or greater than the snapshot size. When you select a volume type and a snapshot, the minimum and maximum sizes for the volume are shown next to Size. For more information, see Constraints on the size and configuration of an EBS volume . With a Provisioned IOPS SSD volume, for IOPS , type the maximum number of input/output operations per second (IOPS) that the volume should support. For Availability Zone , choose the Availability Zone in which to create the volume. EBS volumes can only be attached to EC2 instances in the same Availability Zone. (Optional) Choose Create additional tags to add tags to the volume. For each tag, provide a tag key and a tag value. Choose Create Volume . To use your new volume, attach it to an instance and mount it. For more information, see Attaching an Amazon EBS volume to an instance . If you created a volume that is larger than the snapshot, you must extend the file system on the volume to take advantage of the extra space. For more information, see Amazon EBS Elastic Volumes . To create an EBS volume from a snapshot using the command line \u00b6 You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . create-volume (AWS CLI) New-EC2Volume (AWS Tools for Windows PowerShell) Attaching an Amazon EBS volume to an instance \u00b6 You can attach an available EBS volume to one or more of your instances that is in the same Availability Zone as the volume. Prerequisites \u00b6 Determine how many volumes you can attach to your instance. For more information, see Instance volume limits . Determine whether you can attach your volume to multiple instances and enable Multi-Attach. For more information, see Attaching a volume to multiple instances with Amazon EBS Multi-Attach . If a volume is encrypted, it can only be attached to an instance that supports Amazon EBS encryption. For more information, see Supported instance types . If a volume has an AWS Marketplace product code: The volume can only be attached to a stopped instance. You must be subscribed to the AWS Marketplace code that is on the volume. The configuration (instance type, operating system) of the instance must support that specific AWS Marketplace code. For example, you cannot take a volume from a Windows instance and attach it to a Linux instance. AWS Marketplace product codes are copied from the volume to the instance. To attach an EBS volume to an instance using the console \u00b6 Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the navigation pane, choose Elastic Block Store, Volumes . Select an available volume and choose Actions, Attach Volume . For Instance , start typing the name or ID of the instance. Select the instance from the list of options (only instances that are in the same Availability Zone as the volume are displayed). For Device , you can keep the suggested device name, or type a different supported device name. For more information, see Device naming on Linux instances . Choose Attach . Connect to your instance and mount the volume. For more information, see Making an Amazon EBS volume available for use on Linux . To attach an EBS volume to an instance using the command line \u00b6 You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . attach-volume (AWS CLI) Add-EC2Volume (AWS Tools for Windows PowerShell) More advanced attachment \u00b6 Attaching a volume to multiple instances with Amazon EBS Multi-Attach Making an Amazon EBS volume available for use on Linux \u00b6 After you attach an Amazon EBS volume to your instance, it is exposed as a block device. You can format the volume with any file system and then mount it. After you make the EBS volume available for use, you can access it in the same ways that you access any other volume. Any data written to this file system is written to the EBS volume and is transparent to applications using the device. You can take snapshots of your EBS volume for backup purposes or to use as a baseline when you create another volume. For more information, see Amazon EBS snapshots . Format and mount an attached volume \u00b6 Suppose that you have an EC2 instance with an EBS volume for the root device, /dev/xvda , and that you have just attached an empty EBS volume to the instance using /dev/sdf . Use the following procedure to make the newly attached volume available for use. To format and mount an EBS volume on Linux Connect to your instance using SSH. For more information, see Connect to your Linux instance . The device could be attached to the instance with a different device name than you specified in the block device mapping. For more information, see Device naming on Linux instances . Use the lsblk command to view your available disk devices and their mount points (if applicable) to help you determine the correct device name to use. The output of lsblk removes the /dev/ prefix from full device paths. The following is example output for a T2 instance. The root device is /dev/xvda . The attached volume is not attached yet (it will be /dev/xvdf ), ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 The following is example output for a T2 instance. The root device is /dev/xvda . The attached volume ( 5G ) is /dev/xvdf , which is not yet mounted. ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 5G 0 disk loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 Determine whether there is a file system on the volume. New volumes are raw block devices, and you must create a file system on them before you can mount and use them. Volumes that were created from snapshots likely have a file system on them already; if you create a new file system on top of an existing file system, the operation overwrites your data. Use the file -s command to get information about a device, such as its file system type. If the output shows simply data , as in the following example output, there is no file system on the device and you must create one. ubuntu@ip-172-31-38-59:~$ sudo file -s /dev/xvdf /dev/xvdf: data If the device has a file system, the command shows information about the file system type. For example, the following output shows a root device with the XFS file system. ubuntu@ip-172-31-38-59:~$ sudo file -s /dev/xvda1 /dev/xvda1: Linux rev 1.0 ext4 filesystem data, UUID=aff0a17d-b917-4350-93b2-3a2eab2067bc, volume name \"cloudimg-rootfs\" (needs journal recovery) (extents) (large files) (huge files) (Conditional) If you discovered that there is a file system on the device in the previous step, skip this step. If you have an empty volume, use the mkfs -t command to create a file system on the volume. /!\\ Important Warning Do not use this command if you're mounting a volume that already has data on it (for example, a volume that was created from a snapshot). Otherwise, you'll format the volume and delete the existing data. ubuntu@ip-172-31-38-59:~$ sudo mkfs -t xfs /dev/xvdf meta-data=/dev/xvdf isize=512 agcount=4, agsize=327680 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=0 data = bsize=4096 blocks=1310720, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 If you get an error that mkfs.xfs is not found, use the following command to install the XFS tools and then repeat the previous command: ubuntu@ip-172-31-38-59:~$ sudo yum install xfsprogs Use the mkdir command to create a mount point directory for the volume. The mount point is where the volume is located in the file system tree and where you read and write files to after you mount the volume. The following example creates a directory named /data . ubuntu@ip-172-31-38-59:~$ sudo mkdir /data Use the following command to mount the volume at the directory you created in the previous step. ubuntu@ip-172-31-38-59:~$ sudo mount /dev/xvdf /data Review the file permissions of your new volume mount to make sure that your users and applications can write to the volume. For more information about file permissions, see File security at The Linux Documentation Project. The mount point is not automatically preserved after rebooting your instance. To automatically mount this EBS volume after reboot, see Automatically mount an attached volume after reboot . Check your instance\u2019s volume size \u00b6 First of all, you can check again your volume, if it is mounted or not , by ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 5G 0 disk /data loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 you can see, the xvdf volume is mounted to the /data/ , that is meant that the mounting has been successed. Use df command to check the volumes sizes: ubuntu@ip-172-31-38-59:~$ df Filesystem 1K-blocks Used Available Use% Mounted on udev 499320 0 499320 0% /dev tmpfs 101444 10956 90488 11% /run /dev/xvda1 8065444 2087224 5961836 26% / tmpfs 507208 0 507208 0% /dev/shm tmpfs 5120 0 5120 0% /run/lock tmpfs 507208 0 507208 0% /sys/fs/cgroup /dev/loop1 18432 18432 0 100% /snap/amazon-ssm-agent/1566 /dev/loop2 99328 99328 0 100% /snap/core/9665 /dev/loop3 28800 28800 0 100% /snap/amazon-ssm-agent/2012 /dev/loop4 98944 98944 0 100% /snap/core/9804 tmpfs 101444 0 101444 0% /run/user/1000 /dev/xvdf 5232640 32960 5199680 1% /data you can see the last volume /dev/xvdf detailes and it is mounted to the /data , if you want to see specific volume by: ubuntu@ip-172-31-38-59:~$ df /data/ Filesystem 1K-blocks Used Available Use% Mounted on /dev/xvdf 5232640 32964 5199676 1% /data Use your new volume, by navigating to /data and start add files: ubuntu@ip-172-31-38-59:~$ cd /data/ ubuntu@ip-172-31-38-59:/data$ ls hydrosat test.py Making an Amazon EBS volume available for use on Windows \u00b6 You can get directions for volumes on a Windows instance from Making a Volume Available for Use on Windows in the Amazon EC2 User Guide for Windows Instances. You can get directions for volumes on a Windows instance from Detaching a volume from a Windows instance in the Amazon EC2 User Guide for Windows Instances. Releasing an Amazon EBS volume \u00b6 In order to releasing an EBS volume, you need to: Detaching the EBS volume Deleting the EBS Volume Detaching an Amazon EBS volume from a Linux instance \u00b6 You can detach an Amazon EBS volume from an instance explicitly or by terminating the instance. However, if the instance is running, you must first unmount the volume from the instance. If an EBS volume is the root device of an instance, you must stop the instance before you can detach the volume. When a volume with an AWS Marketplace product code is detached from an instance, the product code is no longer associated with the instance. /!\\ Important After you detach a volume, you are still charged for volume storage as long as the storage amount exceeds the limit of the AWS Free Tier. You must delete a volume to avoid incurring further charges. For more information, see Deleting an Amazon EBS volume . This example unmounts the volume and then explicitly detaches it from the instance. This is useful when you want to terminate an instance or attach a volume to a different instance. To verify that the volume is no longer attached to the instance, see Viewing information about an Amazon EBS volume . You can reattach a volume that you detached (without unmounting it), but it might not get the same mount point. If there were writes to the volume in progress when it was detached, the data on the volume might be out of sync. To detach an EBS volume using the console \u00b6 From your Linux instance, use the following command to unmount the /dev/sdh device. ubuntu@ip-172-31-38-59:~$ umount -d /dev/sdh Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the navigation pane, choose Volumes . Select a volume and choose Actions, Detach Volume . In the confirmation dialog box, choose Yes, Detach . Example: \u00b6 Before umount ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 5G 0 disk /data loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 After umount ubuntu@ip-172-31-38-59:~$ sudo umount -d /data ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 5G 0 disk loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 After Detached ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 To detach an EBS volume using the command line \u00b6 After unmounting the volume, you can use one of the following commands to detach it. For more information about these command line interfaces, see Accessing Amazon EC2 . detach-volume (AWS CLI) Dismount-EC2Volume (AWS Tools for Windows PowerShell) Deleting an Amazon EBS volume \u00b6 After you no longer need an Amazon EBS volume, you can delete it. After deletion, its data is gone and the volume can\u2019t be attached to any instance. However, before deletion, you can store a snapshot of the volume, which you can use to re-create the volume later. /!\\ Important You can\u2019t delete a volume if it\u2019s attached to an instance. To delete a volume, you must first detach it. For more information, see Detaching an Amazon EBS volume from a Linux instance . You can check if a volume is attached to an instance. In the console, on the Volumes page , you can view the state of your volumes. If a volume is attached to an instance, it\u2019s in the in-use state. If a volume is detached from an instance, it\u2019s in the available state. You can delete this volume. To delete an EBS volume using the console \u00b6 Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the navigation pane, choose Volumes . Select a volume and choose Actions, Delete Volume . If Delete Volume is greyed out, the volume is attached to the instance. In the confirmation dialog box, choose Yes, Delete . To delete an EBS volume using the command line \u00b6 You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . delete-volume (AWS CLI) Remove-EC2Volume (AWS Tools for Windows PowerShell) Watch it here \u00b6 Create, attach, and mount a new EBS volume \u00b6 Detach, unmount, and delet an existing EBS volume \u00b6 References \u00b6 Viewing information about an Amazon EBS volume Replacing an Amazon EBS volume using a previous snapshot Monitoring the status of your volumes Detaching an Amazon EBS volume from a Linux instance Deleting an Amazon EBS volume","title":"Amazon EBS volumes"},{"location":"EBS/#amazon-ebs-volumes","text":"An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. For current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes. You can use EBS volumes as primary storage for data that requires frequent updates, such as the system drive for an instance or storage for a database application. You can also use them for throughput-intensive applications that perform continuous disk scans. EBS volumes persist independently from the running life of an EC2 instance. You can attach multiple EBS volumes to a single instance. The volume and instance must be in the same Availability Zone. Depending on the volume and instance types, you can use Multi-Attach to mount a volume to multiple instances at the same time. Amazon EBS provides the following volume types: General Purpose SSD ( gp2 ), Provisioned IOPS SSD ( io1 ), Throughput Optimized HDD ( st1 ), Cold HDD ( sc1 ), and Magnetic ( standard , a previous-generation type). They differ in performance characteristics and price, allowing you to tailor your storage performance and cost to the needs of your applications. For more information, see Amazon EBS volume types . Your account has a limit on the number of EBS volumes that you can use, and the total storage available to you. For more information about these limits, and how to request an increase in your limits, see Amazon EC2 Service Quotas .","title":"Amazon EBS volumes"},{"location":"EBS/#what-to-know-befor-creating-an-ebs-volume","text":"Benefits of using EBS volumes Amazon EBS volume types Constraints on the size and configuration of an EBS volume","title":"What to know befor creating an EBS volume"},{"location":"EBS/#creating-an-amazon-ebs-volume","text":"You can create an Amazon EBS volume and then attach to any EC2 instance in the same Availability Zone. If you create an encrypted EBS volume, you can only attach it to supported instance types. For more information, see Supported instance types . If you are creating a volume for a high-performance storage scenario, you should make sure to use a Provisioned IOPS SSD (io1) volume and attach it to an instance with enough bandwidth to support your application, such as an EBS-optimized instance or an instance with 10-Gigabit network connectivity. The same advice holds for Throughput Optimized HDD (st1) and Cold HDD (sc1) volumes. For more information, see Amazon EBS\u2013optimized instances . Empty EBS volumes receive their maximum performance the moment that they are available and do not require initialization (formerly known as pre-warming). However, storage blocks on volumes that were created from snapshots must be initialized (pulled down from Amazon S3 and written to the volume) before you can access the block. This preliminary action takes time and can cause a significant increase in the latency of an I/O operation the first time each block is accessed. Volume performance is achieved after all blocks have been downloaded and written to the volume. For most applications, amortizing this cost over the lifetime of the volume is acceptable. To avoid this initial performance hit in a production environment, you can force immediate initialization of the entire volume or enable fast snapshot restore. For more information, see Initializing Amazon EBS volumes .","title":"Creating an Amazon EBS volume"},{"location":"EBS/#methods-of-creating-a-volume","text":"Create and attach EBS volumes when you launch instances by specifying a block device mapping. For more information, see Launching an instance using the Launch Instance Wizard and Block device mapping . Create an empty EBS volume and attach it to a running instance. For more information, see Creating an empty volume below. Create an EBS volume from a previously created snapshot and attach it to a running instance. For more information, see Creating a volume from a snapshot below.","title":"Methods of creating a volume"},{"location":"EBS/#creating-an-empty-volume","text":"Empty volumes receive their maximum performance the moment that they are available and do not require initialization .","title":"Creating an empty volume"},{"location":"EBS/#to-create-a-empty-ebs-volume-using-the-console","text":"Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the navigation bar, select the Region in which you would like to create your volume. This choice is important because some Amazon EC2 resources can be shared between Regions, while others can\u2019t. For more information, see Resource locations . In the navigation pane, choose ELASTIC BLOCK STORE, Volumes . Choose Create Volume . For Volume Type , choose a volume type. For more information, see Amazon EBS volume types . For Size (GiB) , type the size of the volume. For more information, see Constraints on the size and configuration of an EBS volume . With a Provisioned IOPS SSD volume, for IOPS , type the maximum number of input/output operations per second (IOPS) that the volume should support. For Availability Zone , choose the Availability Zone in which to create the volume. EBS volumes can only be attached to EC2 instances within the same Availability Zone. (Optional) If the instance type supports EBS encryption and you want to encrypt the volume, select Encrypt this volume and choose a CMK . If encryption by default is enabled in this Region, EBS encryption is enabled and the default CMK for EBS encryption is chosen. You can choose a different CMK from Master Key or paste the full ARN of any key that you can access. For more information, see Amazon EBS encryption . (Optional) Choose Create additional tags to add tags to the volume. For each tag, provide a tag key and a tag value. For more information, see Tagging your Amazon EC2 resources . Choose Create Volume . The volume is ready for use when the volume status is Available . To use your new volume, attach it to an instance, format it, and mount it. For more information, see Attaching an Amazon EBS volume to an instance .","title":"To create a empty EBS volume using the console"},{"location":"EBS/#to-create-an-empty-ebs-volume-using-the-command-line","text":"You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . create-volume (AWS CLI) New-EC2Volume (AWS Tools for Windows PowerShell)","title":"To create an empty EBS volume using the command line"},{"location":"EBS/#creating-a-volume-from-a-snapshot","text":"Volumes created from snapshots load lazily in the background. This means that there is no need to wait for all of the data to transfer from Amazon S3 to your EBS volume before the instance can start accessing an attached volume and all its data. If your instance accesses data that hasn\u2019t yet been loaded, the volume immediately downloads the requested data from Amazon S3, and then continues loading the rest of the volume data in the background. Volume performance is achieved after all blocks are downloaded and written to the volume. To avoid the initial performance hit in a production environment, see Initializing Amazon EBS volumes . New EBS volumes that are created from encrypted snapshots are automatically encrypted. You can also encrypt a volume on-the-fly while restoring it from an unencrypted snapshot. Encrypted volumes can only be attached to instance types that support EBS encryption. For more information, see Supported instance types .","title":"Creating a volume from a snapshot"},{"location":"EBS/#to-create-an-ebs-volume-from-a-snapshot-using-the-console","text":"Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the navigation bar, select the Region that your snapshot is located in. To use the snapshot to create a volume in a different region, copy your snapshot to the new Region and then use it to create a volume in that Region. For more information, see Copying an Amazon EBS snapshot . In the navigation pane, choose ELASTIC BLOCK STORE, Volumes . Choose Create Volume . For Volume Type , choose a volume type. For more information, see Amazon EBS volume types . For Snapshot ID , start typing the ID or description of the snapshot from which you are restoring the volume, and choose it from the list of suggested options. (Optional) Select Encrypt this volume to change the encryption state of your volume. This is optional if encryption by default is enabled. Select a CMK from Master Key to specify a CMK other than the default CMK for EBS encryption. For Size (GiB) , type the size of the volume, or verify that the default size of the snapshot is adequate. If you specify both a volume size and a snapshot, the size must be equal to or greater than the snapshot size. When you select a volume type and a snapshot, the minimum and maximum sizes for the volume are shown next to Size. For more information, see Constraints on the size and configuration of an EBS volume . With a Provisioned IOPS SSD volume, for IOPS , type the maximum number of input/output operations per second (IOPS) that the volume should support. For Availability Zone , choose the Availability Zone in which to create the volume. EBS volumes can only be attached to EC2 instances in the same Availability Zone. (Optional) Choose Create additional tags to add tags to the volume. For each tag, provide a tag key and a tag value. Choose Create Volume . To use your new volume, attach it to an instance and mount it. For more information, see Attaching an Amazon EBS volume to an instance . If you created a volume that is larger than the snapshot, you must extend the file system on the volume to take advantage of the extra space. For more information, see Amazon EBS Elastic Volumes .","title":"To create an EBS volume from a snapshot using the console"},{"location":"EBS/#to-create-an-ebs-volume-from-a-snapshot-using-the-command-line","text":"You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . create-volume (AWS CLI) New-EC2Volume (AWS Tools for Windows PowerShell)","title":"To create an EBS volume from a snapshot using the command line"},{"location":"EBS/#attaching-an-amazon-ebs-volume-to-an-instance","text":"You can attach an available EBS volume to one or more of your instances that is in the same Availability Zone as the volume.","title":"Attaching an Amazon EBS volume to an instance"},{"location":"EBS/#prerequisites","text":"Determine how many volumes you can attach to your instance. For more information, see Instance volume limits . Determine whether you can attach your volume to multiple instances and enable Multi-Attach. For more information, see Attaching a volume to multiple instances with Amazon EBS Multi-Attach . If a volume is encrypted, it can only be attached to an instance that supports Amazon EBS encryption. For more information, see Supported instance types . If a volume has an AWS Marketplace product code: The volume can only be attached to a stopped instance. You must be subscribed to the AWS Marketplace code that is on the volume. The configuration (instance type, operating system) of the instance must support that specific AWS Marketplace code. For example, you cannot take a volume from a Windows instance and attach it to a Linux instance. AWS Marketplace product codes are copied from the volume to the instance.","title":"Prerequisites"},{"location":"EBS/#to-attach-an-ebs-volume-to-an-instance-using-the-console","text":"Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the navigation pane, choose Elastic Block Store, Volumes . Select an available volume and choose Actions, Attach Volume . For Instance , start typing the name or ID of the instance. Select the instance from the list of options (only instances that are in the same Availability Zone as the volume are displayed). For Device , you can keep the suggested device name, or type a different supported device name. For more information, see Device naming on Linux instances . Choose Attach . Connect to your instance and mount the volume. For more information, see Making an Amazon EBS volume available for use on Linux .","title":"To attach an EBS volume to an instance using the console"},{"location":"EBS/#to-attach-an-ebs-volume-to-an-instance-using-the-command-line","text":"You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . attach-volume (AWS CLI) Add-EC2Volume (AWS Tools for Windows PowerShell)","title":"To attach an EBS volume to an instance using the command line"},{"location":"EBS/#more-advanced-attachment","text":"Attaching a volume to multiple instances with Amazon EBS Multi-Attach","title":"More advanced attachment"},{"location":"EBS/#making-an-amazon-ebs-volume-available-for-use-on-linux","text":"After you attach an Amazon EBS volume to your instance, it is exposed as a block device. You can format the volume with any file system and then mount it. After you make the EBS volume available for use, you can access it in the same ways that you access any other volume. Any data written to this file system is written to the EBS volume and is transparent to applications using the device. You can take snapshots of your EBS volume for backup purposes or to use as a baseline when you create another volume. For more information, see Amazon EBS snapshots .","title":"Making an Amazon EBS volume available for use on Linux"},{"location":"EBS/#format-and-mount-an-attached-volume","text":"Suppose that you have an EC2 instance with an EBS volume for the root device, /dev/xvda , and that you have just attached an empty EBS volume to the instance using /dev/sdf . Use the following procedure to make the newly attached volume available for use. To format and mount an EBS volume on Linux Connect to your instance using SSH. For more information, see Connect to your Linux instance . The device could be attached to the instance with a different device name than you specified in the block device mapping. For more information, see Device naming on Linux instances . Use the lsblk command to view your available disk devices and their mount points (if applicable) to help you determine the correct device name to use. The output of lsblk removes the /dev/ prefix from full device paths. The following is example output for a T2 instance. The root device is /dev/xvda . The attached volume is not attached yet (it will be /dev/xvdf ), ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 The following is example output for a T2 instance. The root device is /dev/xvda . The attached volume ( 5G ) is /dev/xvdf , which is not yet mounted. ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 5G 0 disk loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 Determine whether there is a file system on the volume. New volumes are raw block devices, and you must create a file system on them before you can mount and use them. Volumes that were created from snapshots likely have a file system on them already; if you create a new file system on top of an existing file system, the operation overwrites your data. Use the file -s command to get information about a device, such as its file system type. If the output shows simply data , as in the following example output, there is no file system on the device and you must create one. ubuntu@ip-172-31-38-59:~$ sudo file -s /dev/xvdf /dev/xvdf: data If the device has a file system, the command shows information about the file system type. For example, the following output shows a root device with the XFS file system. ubuntu@ip-172-31-38-59:~$ sudo file -s /dev/xvda1 /dev/xvda1: Linux rev 1.0 ext4 filesystem data, UUID=aff0a17d-b917-4350-93b2-3a2eab2067bc, volume name \"cloudimg-rootfs\" (needs journal recovery) (extents) (large files) (huge files) (Conditional) If you discovered that there is a file system on the device in the previous step, skip this step. If you have an empty volume, use the mkfs -t command to create a file system on the volume. /!\\ Important Warning Do not use this command if you're mounting a volume that already has data on it (for example, a volume that was created from a snapshot). Otherwise, you'll format the volume and delete the existing data. ubuntu@ip-172-31-38-59:~$ sudo mkfs -t xfs /dev/xvdf meta-data=/dev/xvdf isize=512 agcount=4, agsize=327680 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=0 data = bsize=4096 blocks=1310720, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 If you get an error that mkfs.xfs is not found, use the following command to install the XFS tools and then repeat the previous command: ubuntu@ip-172-31-38-59:~$ sudo yum install xfsprogs Use the mkdir command to create a mount point directory for the volume. The mount point is where the volume is located in the file system tree and where you read and write files to after you mount the volume. The following example creates a directory named /data . ubuntu@ip-172-31-38-59:~$ sudo mkdir /data Use the following command to mount the volume at the directory you created in the previous step. ubuntu@ip-172-31-38-59:~$ sudo mount /dev/xvdf /data Review the file permissions of your new volume mount to make sure that your users and applications can write to the volume. For more information about file permissions, see File security at The Linux Documentation Project. The mount point is not automatically preserved after rebooting your instance. To automatically mount this EBS volume after reboot, see Automatically mount an attached volume after reboot .","title":"Format and mount an attached volume"},{"location":"EBS/#check-your-instances-volume-size","text":"First of all, you can check again your volume, if it is mounted or not , by ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 5G 0 disk /data loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 you can see, the xvdf volume is mounted to the /data/ , that is meant that the mounting has been successed. Use df command to check the volumes sizes: ubuntu@ip-172-31-38-59:~$ df Filesystem 1K-blocks Used Available Use% Mounted on udev 499320 0 499320 0% /dev tmpfs 101444 10956 90488 11% /run /dev/xvda1 8065444 2087224 5961836 26% / tmpfs 507208 0 507208 0% /dev/shm tmpfs 5120 0 5120 0% /run/lock tmpfs 507208 0 507208 0% /sys/fs/cgroup /dev/loop1 18432 18432 0 100% /snap/amazon-ssm-agent/1566 /dev/loop2 99328 99328 0 100% /snap/core/9665 /dev/loop3 28800 28800 0 100% /snap/amazon-ssm-agent/2012 /dev/loop4 98944 98944 0 100% /snap/core/9804 tmpfs 101444 0 101444 0% /run/user/1000 /dev/xvdf 5232640 32960 5199680 1% /data you can see the last volume /dev/xvdf detailes and it is mounted to the /data , if you want to see specific volume by: ubuntu@ip-172-31-38-59:~$ df /data/ Filesystem 1K-blocks Used Available Use% Mounted on /dev/xvdf 5232640 32964 5199676 1% /data Use your new volume, by navigating to /data and start add files: ubuntu@ip-172-31-38-59:~$ cd /data/ ubuntu@ip-172-31-38-59:/data$ ls hydrosat test.py","title":"Check your instance's volume size"},{"location":"EBS/#making-an-amazon-ebs-volume-available-for-use-on-windows","text":"You can get directions for volumes on a Windows instance from Making a Volume Available for Use on Windows in the Amazon EC2 User Guide for Windows Instances. You can get directions for volumes on a Windows instance from Detaching a volume from a Windows instance in the Amazon EC2 User Guide for Windows Instances.","title":"Making an Amazon EBS volume available for use on Windows"},{"location":"EBS/#releasing-an-amazon-ebs-volume","text":"In order to releasing an EBS volume, you need to: Detaching the EBS volume Deleting the EBS Volume","title":"Releasing an Amazon EBS volume"},{"location":"EBS/#detaching-an-amazon-ebs-volume-from-a-linux-instance","text":"You can detach an Amazon EBS volume from an instance explicitly or by terminating the instance. However, if the instance is running, you must first unmount the volume from the instance. If an EBS volume is the root device of an instance, you must stop the instance before you can detach the volume. When a volume with an AWS Marketplace product code is detached from an instance, the product code is no longer associated with the instance. /!\\ Important After you detach a volume, you are still charged for volume storage as long as the storage amount exceeds the limit of the AWS Free Tier. You must delete a volume to avoid incurring further charges. For more information, see Deleting an Amazon EBS volume . This example unmounts the volume and then explicitly detaches it from the instance. This is useful when you want to terminate an instance or attach a volume to a different instance. To verify that the volume is no longer attached to the instance, see Viewing information about an Amazon EBS volume . You can reattach a volume that you detached (without unmounting it), but it might not get the same mount point. If there were writes to the volume in progress when it was detached, the data on the volume might be out of sync.","title":"Detaching an Amazon EBS volume from a Linux instance"},{"location":"EBS/#to-detach-an-ebs-volume-using-the-console","text":"From your Linux instance, use the following command to unmount the /dev/sdh device. ubuntu@ip-172-31-38-59:~$ umount -d /dev/sdh Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the navigation pane, choose Volumes . Select a volume and choose Actions, Detach Volume . In the confirmation dialog box, choose Yes, Detach .","title":"To detach an EBS volume using the console"},{"location":"EBS/#example","text":"Before umount ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 5G 0 disk /data loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 After umount ubuntu@ip-172-31-38-59:~$ sudo umount -d /data ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / xvdf 202:80 0 5G 0 disk loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804 After Detached ubuntu@ip-172-31-38-59:~$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk \u2514\u2500xvda1 202:1 0 8G 0 part / loop1 7:1 0 18M 1 loop /snap/amazon-ssm-agent/1566 loop2 7:2 0 97M 1 loop /snap/core/9665 loop3 7:3 0 28.1M 1 loop /snap/amazon-ssm-agent/2012 loop4 7:4 0 96.6M 1 loop /snap/core/9804","title":"Example:"},{"location":"EBS/#to-detach-an-ebs-volume-using-the-command-line","text":"After unmounting the volume, you can use one of the following commands to detach it. For more information about these command line interfaces, see Accessing Amazon EC2 . detach-volume (AWS CLI) Dismount-EC2Volume (AWS Tools for Windows PowerShell)","title":"To detach an EBS volume using the command line"},{"location":"EBS/#deleting-an-amazon-ebs-volume","text":"After you no longer need an Amazon EBS volume, you can delete it. After deletion, its data is gone and the volume can\u2019t be attached to any instance. However, before deletion, you can store a snapshot of the volume, which you can use to re-create the volume later. /!\\ Important You can\u2019t delete a volume if it\u2019s attached to an instance. To delete a volume, you must first detach it. For more information, see Detaching an Amazon EBS volume from a Linux instance . You can check if a volume is attached to an instance. In the console, on the Volumes page , you can view the state of your volumes. If a volume is attached to an instance, it\u2019s in the in-use state. If a volume is detached from an instance, it\u2019s in the available state. You can delete this volume.","title":"Deleting an Amazon EBS volume"},{"location":"EBS/#to-delete-an-ebs-volume-using-the-console","text":"Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the navigation pane, choose Volumes . Select a volume and choose Actions, Delete Volume . If Delete Volume is greyed out, the volume is attached to the instance. In the confirmation dialog box, choose Yes, Delete .","title":"To delete an EBS volume using the console"},{"location":"EBS/#to-delete-an-ebs-volume-using-the-command-line","text":"You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . delete-volume (AWS CLI) Remove-EC2Volume (AWS Tools for Windows PowerShell)","title":"To delete an EBS volume using the command line"},{"location":"EBS/#watch-it-here","text":"","title":"Watch it here"},{"location":"EBS/#create-attach-and-mount-a-new-ebs-volume","text":"","title":"Create, attach, and mount a new EBS volume"},{"location":"EBS/#detach-unmount-and-delet-an-existing-ebs-volume","text":"","title":"Detach, unmount, and delet an existing EBS volume"},{"location":"EBS/#references","text":"Viewing information about an Amazon EBS volume Replacing an Amazon EBS volume using a previous snapshot Monitoring the status of your volumes Detaching an Amazon EBS volume from a Linux instance Deleting an Amazon EBS volume","title":"References"},{"location":"EC2/","text":"Amazon Elastic Compute Cloud (EC2) Instances \u00b6 What is Amazon EC2? \u00b6 Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic. If you would like to know more about Cloud Computing from AWS point of view, see What is Cloud Computing? How to get started with Amazon EC2 \u00b6 First, you need to get set up to use Amazon EC2. After you are set up, you are ready to complete the Getting Started tutorial for Amazon EC2. Whenever you need more information about an Amazon EC2 feature, you can read the technical documentation. You need to get familiar with the follwoing: Instances and AMIs Regions and Availability Zones Instance types Tags Setting up with Amazon EC2 \u00b6 Complete the tasks in this section to get set up for launching an Amazon EC2 instance for the first time: Sign up for AWS Create a key pair Create a security group Sign up for AWS \u00b6 Please follow the instructions in Setting Up an AWS Account . Create a key pair \u00b6 AWS uses public-key cryptography to secure the login information for your instance. A Linux instance has no password; you use a key pair to log in to your instance securely. You specify the name of the key pair when you launch your instance, then provide the private key when you log in using SSH. If you haven\u2019t created a key pair already, you can create one using the Amazon EC2 console. Note that if you plan to launch instances in multiple Regions, you\u2019ll need to create a key pair in each Region. For more information about Regions, see Regions, Availability Zones, and Local Zones . You can create a key pair using one of the following method: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the navigation pane, choose Key Pairs . Choose Create key pair . For Name , enter a descriptive name for the key pair. Amazon EC2 associates the public key with the name that you specify as the key name. A key name can include up to 255 ASCII characters. It can\u2019t include leading or trailing spaces. For File format, choose the format in which to save the private key. To save the private key in a format that can be used with OpenSSH, choose pem . To save the private key in a format that can be used with PuTTY, choose ppk . Choose Create key pair . The private key file is automatically downloaded by your browser. The base file name is the name you specified as the name of your key pair, and the file name extension is determined by the file format you chose. Save the private key file in a safe place. /!\\ Important This is the only chance for you to save the private key file. If you will use an SSH client on a macOS or Linux computer to connect to your Linux instance, use the following command to set the permissions of your private key file so that only you can read it. chmod 400 my-key-pair.pem If you do not set these permissions, then you cannot connect to your instance using this key pair. For more information, see Error: Unprotected private key file . Watch it here \u00b6 Create a security group \u00b6 Security groups act as a firewall for associated instances, controlling both inbound and outbound traffic at the instance level. You must add rules to a security group that enable you to connect to your instance from your IP address using SSH. You can also add rules that allow inbound and outbound HTTP and HTTPS access from anywhere. Note that if you plan to launch instances in multiple Regions, you\u2019ll need to create a security group in each Region. For more information about Regions, see Regions, Availability Zones, and Local Zones . Prerequisites \u00b6 You\u2019ll need the public IPv4 address of your local computer. The security group editor in the Amazon EC2 console can automatically detect the public IPv4 address for you. Alternatively, you can use the search phrase \u201cwhat is my IP address\u201d in an Internet browser, or use the following service: Check IP . If you are connecting through an Internet service provider (ISP) or from behind a firewall without a static IP address, you need to find out the range of IP addresses used by client computers. To create a security group with least privilege Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the navigation bar, select a Region for the security group. Security groups are specific to a Region, so you should select the same Region in which you created your key pair. In the navigation pane, choose Security Groups . Choose Create security group . In the Basic details section, do the following: a. Enter a name for the new security group and a description. Use a name that is easy for you to remember, such as your user name, followed by SG , plus the Region name. For example, me_SG_uswest2. b. In the VPC list, select your default VPC for the Region. In the Inbound rules section, create the following rules (choose Add rule for each new rule): Choose HTTP from the Type list, and make sure that Source is set to Anywhere ( 0.0.0.0/0 ) . Choose HTTPS from the Type list, and make sure that Source is set to Anywhere ( 0.0.0.0/0 ) . Choose SSH from the Type list. In the Source box, choose My IP to automatically populate the field with the public IPv4 address of your local computer. Alternatively, choose Custom and specify the public IPv4 address of your computer or network in CIDR notation. To specify an individual IP address in CIDR notation, add the routing suffix /32 , for example, 203.0.113.25/32 . If your company allocates addresses from a range, specify the entire range, such as 203.0.113.0/24 . /!\\ Warning For security reasons, we don't recommend that you allow SSH access from all IPv4 addresses (0.0.0.0/0) to your instance, except for testing purposes and only for a short time. Choose Create security group . For more information, see Amazon EC2 security groups for Linux instances . Getting started with Amazon EC2 Linux instances \u00b6 When you sign up for AWS, you can get started with Amazon EC2 using the AWS Free Tier . If you created your AWS account less than 12 months ago, and have not already exceeded the free tier benefits for Amazon EC2, it will not cost you anything to complete this tutorial, because we help you select options that are within the free tier benefits. Otherwise, you\u2019ll incur the standard Amazon EC2 usage fees from the time that you launch the instance until you terminate the instance (which is the final task of this tutorial), even if it remains idle. The instance is an Amazon EBS-backed instance (meaning that the root volume is an EBS (Elastic Block Store) volume). You can either specify the Availability Zone in which your instance runs, or let Amazon EC2 select an Availability Zone for you. When you launch your instance, you secure it by specifying a key pair and security group. When you connect to your instance, you must specify the private key of the key pair that you specified when launching your instance. In ordere to implement the previous configuration, you should proceeed with the following: Launch an instance Connect to your instance Clean up your instance Terminate your instance Next Steps Launch an instance \u00b6 You can launch a Linux instance using the AWS Management Console as described in the following procedure. This tutorial is intended to help you launch your first instance quickly, so it doesn\u2019t cover all possible options. For more information about the advanced options, see Launching an Instance . To launch an instance Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the console dashboard, choose Launch Instance . The Choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select an HVM version of Amazon Linux 2. Notice that these AMIs are marked \u201cFree tier eligible.\u201d On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro instance type, which is selected by default. The t2.micro instance type is eligible for the free tier. In Regions where t2.micro is unavailable, you can use a t3.micro instance under the free tier. For more information, see AWS Free Tier . Choose Review and Launch to let the wizard complete the other configuration settings for you. On the Review Instance Launch page, under Security Groups , you\u2019ll see that the wizard created and selected a security group for you. You can use this security group, or alternatively you can select the security group that you created when getting set up using the following steps: a. Choose Edit security groups . b. On the Configure Security Group page, ensure that Select an existing security group is selected. c. Select your security group from the list of existing security groups, and then choose Review and Launch . On the Review Instance Launch page, choose Launch . When prompted for a key pair, select Choose an existing key pair , then select the key pair that you created when getting set up. /!\\ Warning Don't select 'Proceed without a key pair'. If you launch your instance without a key pair, then you 'cannot' connect to it. When you are ready, select the acknowledgement check box, and then choose Launch Instances . A confirmation page lets you know that your instance is launching. Choose View Instances to close the confirmation page and return to the console. On the Instances screen, you can view the status of the launch. It takes a short time for an instance to launch. When you launch an instance, its initial state is pending . After the instance starts, its state changes to running and it receives a public DNS name. (If the Public DNS (IPv4) column is hidden, choose Show/Hide Columns (the gear-shaped icon) in the top right corner of the page and then select Public DNS (IPv4) .) It can take a few minutes for the instance to be ready so that you can connect to it. Check that your instance has passed its status checks; you can view this information in the Status Checks column. Connect to your instance \u00b6 There are several ways to connect to your Linux instance. For more information, see Connect to your Linux instance . /!\\ Important You can\u2019t connect to your instance unless you launched it with a key pair for which you have the .pem file and you launched it with a security group that allows SSH access from your computer. If you can\u2019t connect to your instance, see Troubleshooting connecting to your instance for assistance. Clean up your instance \u00b6 After you\u2019ve finished with the instance that you created for this tutorial, you should clean up by terminating the instance. If you want to do more with this instance before you clean up, see Next steps . /!\\ Important Terminating an instance effectively deletes it; you can\u2019t reconnect to an instance after you\u2019ve terminated it. Terminate your instance \u00b6 If you launched an instance that is not within the AWS Free Tier , you\u2019ll stop incurring charges for that instance as soon as the instance status changes to shutting down or terminated . If you\u2019d like to keep your instance for later, but not incur charges, you can stop the instance now and then start it again later. For more information, see Stopping Instances . To terminate your instance In the navigation pane, choose Instances . In the list of instances, select the instance. Choose Actions, Instance State, Terminate . Choose Yes, Terminate when prompted for confirmation. Amazon EC2 shuts down and terminates your instance. After your instance is terminated, it remains visible on the console for a short while, and then the entry is deleted. Next Steps \u00b6 After you start your instance, you might want to try some of the following exercises: Learn how to remotely manage your EC2 instance using Run Command. For more information, see AWS Systems Manager Run Command in the AWS Systems Manager User Guide. Configure a CloudWatch alarm to notify you if your usage exceeds the Free Tier. For more information, see Create a Billing Alarm in the AWS Billing and Cost Management User Guide. Watch it here \u00b6 Connecting to your Linux EC2 Instances \u00b6 Connect to the Linux instances that you launched and transfer files between your local computer and your instance. The operating system of your local computer determines the options that you have to connect from your local computer to your Linux instance. Connection options \u00b6 If your local computer operating system is Linux or macOS X , you can connect by using one pf the following: SSH Client EC2 Instance Connect AWS Systems Manager Session Manager If your local computer operating system is Windows , you can connect by using one pf the following: PuTTY SSH Client AWS Systems Manager Session Manager Windows Subsystem for Linux Connecting to your Linux instance using SSH \u00b6 After you launch your instance, you can connect to it and use it the way that you\u2019d use a computer sitting in front of you. Prerequisites \u00b6 Before you connect to your Linux instance, complete the following prerequisites. Verify that the instance is ready After you launch an instance, it can take a few minutes for the instance to be ready so that you can connect to it. Check that your instance has passed its status checks. You can view this information in the Status Checks column on the Instances page. Verify the general prerequisites for connecting to your instance Check the General prerequisites for connecting to your instance . Install an SSH client on your local computer (either Linux or Windows) as needed Your local computer might have an SSH client installed by default. You can verify this by typing ssh at the command line. If your compute doesn\u2019t recognize the command, you can install an SSH client. Recent versions of Windows Server 2019 and Windows 10 - OpenSSH is included as an installable component. For information, see OpenSSH in Windows . Earlier versions of Windows - Download and install OpenSSH. For more information, see Win32-OpenSSH . Linux and macOS X - Download and install OpenSSH. For more information, see Linux-openssh . Connect to your Linux instance using an SSH client \u00b6 Use the following procedure to connect to your Linux instance using an SSH client. If you receive an error while attempting to connect to your instance, see Troubleshooting connecting to your instance . To connect to your instance using SSH In a terminal window, use the ssh command to connect to the instance. You specify the path and file name of the private key ( .pem ), the user name for your instance, and the public DNS name or IPv6 address for your instance. For more information about how to find the private key, the user name for your instance, and the DNS name or IPv6 address for an instance, see Locate the private key and Get information about your instance . To connect to your instance, use one of the following commands. a. (Public DNS) To connect using your instance\u2019s public DNS name, enter the following command. ssh -i /path/my-key-pair.pem my-instance-user-name@my-instance-public-dns-name b. (IPv6) Alternatively, if your instance has an IPv6 address, to connect using your instance\u2019s IPv6 address, enter the following command. ssh -i /path/my-key-pair.pem my-instance-user-name@my-instance-IPv6-address You see a response like the following: The authenticity of host 'ec2-198-51-100-1.compute-1.amazonaws.com (198-51-100-1)' can't be established. ECDSA key fingerprint is l4UB/neBad9tvkgJf1QZWxheQmR59WgrgzEimCG6kZY. Are you sure you want to continue connecting (yes/no)? Enter yes . You see a response like the following: Warning: Permanently added 'ec2-198-51-100-1.compute-1.amazonaws.com' (ECDSA) to the list of known hosts. Connecting to your Linux instance from Windows using PuTTY or MobaXterm \u00b6 After you launch your instance, you can connect to it and use it the way that you\u2019d use a computer sitting in front of you. The following instructions explain how to connect to your instance using PuTTY, and MobaXterm, free SSH client for Windows. If you receive an error while attempting to connect to your instance, see General prerequisites for connecting to your instance . Prerequisites \u00b6 Before you connect to your Linux instance using PuTTY, complete the following prerequisites. Verify that the instance is ready After you launch an instance, it can take a few minutes for the instance to be ready so that you can connect to it. Check that your instance has passed its status checks. You can view this information in the Status Checks column on the Instances page. Verify the general prerequisites for connecting to your instance Check the General prerequisites for connecting to your instance . Install PuTTY on your local computer Download and install PuTTY from the PuTTY download page . If you already have an older version of PuTTY installed, we recommend that you download the latest version. Be sure to install the entire suite. Or Install MobaXterm on your local computer Download and install MobaXterm from the MobaXterm download page . Convert your private key using PuTTYgen In case you do not have a .ppk , locate the private key ( .pem file) for the key pair that you specified when you launched the instance. Convert the .pem file to a .ppk file for use with PuTTY. For more information, follow the steps in the next subsection. Convert your private key using PuTTYgen \u00b6 PuTTY does not natively support the private key format for SSH keys. PuTTY provides a tool named PuTTYgen, which converts keys to the required format for PuTTY. You must convert your private key ( .pem file) into this format ( .ppk file) as follows in order to connect to your instance using PuTTY. To convert your private key From the Start menu, choose All Programs, PuTTY, PuTTYgen . Under Type of key to generate , choose RSA . If you\u2019re using an older version of PuTTYgen, choose SSH-2 RSA . Choose Load . By default, PuTTYgen displays only files with the extension .ppk . To locate your .pem file, choose the option to display files of all types. Select all file types Select your .pem file for the key pair that you specified when you launched your instance and choose Open . PuTTYgen displays a notice that the .pem file was successfully imported. Choose OK . To save the key in the format that PuTTY can use, choose Save private key . PuTTYgen displays a warning about saving the key without a passphrase. Choose Yes . /!\\ Note A passphrase on a private key is an extra layer of protection. Even if your private key is discovered, it can't be used without the passphrase. The downside to using a passphrase is that it makes automation harder because human intervention is needed to log on to an instance, or to copy files to an instance. Specify the same name for the key that you used for the key pair (for example, my-key-pair ) and choose Save . PuTTY automatically adds the .ppk file extension. Your private key is now in the correct format for use with PuTTY. You can now connect to your instance using PuTTY\u2019s SSH client. Connecting to your Linux instance \u00b6 Use the following procedure to connect to your Linux instance using PuTTY. You need the .ppk file that you created for your private key. For more information, see Convert your private key using PuTTYgen in the preceding section. If you receive an error while attempting to connect to your instance, see Troubleshooting Connecting to Your Instance . To connect to your instance using PuTTY Start PuTTY (from the Start menu, choose All Programs, PuTTY, PuTTY ). In the Category pane , choose Session and complete the following fields: a. In the Host Name box, do one of the following: (Public DNS) To connect using your instance\u2019s public DNS name, enter my-instance-user-name@my-instance-public-dns-name . (IPv6) Alternatively, if your instance has an IPv6 address, to connect using your instance\u2019s IPv6 address, enter my-instance-user-name@my-instance-IPv6-address . For information about how to get the user name for your instance, and the public DNS name or IPv6 address of your instance, see Get information about your instance . b. Ensure that the Port value is 22. c. Under Connection type , select SSH . (Optional) You can configure PuTTY to automatically send \u2018keepalive\u2019 data at regular intervals to keep the session active. This is useful to avoid disconnecting from your instance due to session inactivity. In the Category pane, choose Connection , and then enter the required interval in the Seconds between keepalives field. For example, if your session disconnects after 10 minutes of inactivity, enter 180 to configure PuTTY to send keepalive data every 3 minutes. In the Category pane, expand Connection , expand SSH , and then choose Auth . Complete the following: a. Choose Browse . b. Select the .ppk file that you generated for your key pair and choose Open . c. (Optional) If you plan to start this session again later, you can save the session information for future use. Under Category , choose Session , enter a name for the session in Saved Sessions , and then choose Save . d. Choose Open . If this is the first time you have connected to this instance, PuTTY displays a security alert dialog box that asks whether you trust the host to which you are connecting. a. Choose Yes . A window opens and you are connected to your instance. /!\\ Note If you specified a passphrase when you converted your private key to PuTTY\u2019s format, you must provide that passphrase when you log in to the instance. If you receive an error while attempting to connect to your instance, see Troubleshooting Connecting to Your Instance . Transferring files to Linux instances \u00b6 Transferring files to your Linux instances depends on the type of the operating system you use to connect to the instance . Therfore: If you connect through Linux Two ways to transfer files between your local computer and a Linux instance are to use: The secure copy protocol (SCP) . FileZilla . If you connect through Windwos Many options are exist to transfer files between the instances and local machines, such as: The PuTTY Secure Copy client (PSCP) . WinSCP . FileZilla . Transferring files to Linux instances from Linux using SCP \u00b6 One way to transfer files between your local computer and a Linux instance is to use the secure copy protocol (SCP). This section describes how to transfer files with SCP. The procedure is similar to the procedure for connecting to an instance with SSH. Prerequisites \u00b6 Verify the general prerequisites for transferring files to your instance. The general prerequisites for transferring files to an instance are the same as the general prerequisites for connecting to an instance. For more information, see General prerequisites for connecting to your instance . Install an SCP client Most Linux, Unix, and Apple computers include an SCP client by default. If yours doesn\u2019t, the OpenSSH project provides a free implementation of the full suite of SSH tools, including an SCP client. For more information, see http://www.openssh.org . Use SCP to transfer a file \u00b6 Transfer a file to your instance using the instance\u2019s public DNS name, or the IPv6 address if your instance has one. For example , if the name of your private key file is my-key-pair , the file to transfer is SampleFile.txt , the user name for your instance is my-instance-user-name , and the public DNS name of the instance is my-instance-public-dns-name , or my-instance-IPv6-address if your instance has an IPv6 address, use one of the following commands to copy the file to the my-instance-user-name home directory. (Public DNS) To transfer a file to your instance using your instance\u2019s public DNS name, enter the following command. scp -i /path/my-key-pair.pem /path/SampleFile.txt my-instance-user-name@my-instance-public-dns-name:~ (IPv6) Alternatively, if your instance has an IPv6 address, to transfer a file using the instance\u2019s IPv6 address, enter the following command. The IPv6 address must be enclosed in square brackets ([ ]), which must be escaped (). scp -i /path/my-key-pair.pem /path/SampleFile.txt my-instance-user-name@\\[my-instance-IPv6-address\\]:~ You see a response like the following: The authenticity of host 'ec2-198-51-100-1.compute-1.amazonaws.com (10.254.142.33)' can't be established. RSA key fingerprint is 1f:51:ae:28:bf:89:e9:d8:1f:25:5d:37:2d:7d:b8:ca:9f:f5:f1:6f. Are you sure you want to continue connecting (yes/no)? Enter yes You see a response like the following: Warning: Permanently added 'ec2-198-51-100-1.compute-1.amazonaws.com' (RSA) to the list of known hosts. Sending file modes: C0644 20 SampleFile.txt Sink: C0644 20 SampleFile.txt SampleFile.txt 100% 20 0.0KB/s 00:00 If you receive a \u201cbash: scp: command not found\u201d error, you must first install scp on your Linux instance. For some operating systems, this is located in the openssh-clients package. For Amazon Linux variants, such as the Amazon ECS-optimized AMI, use the following command to install scp : [ec2-user ~]$ sudo yum install -y openssh-clients To transfer files in the other direction (from your Amazon EC2 instance to your local computer), reverse the order of the host parameters. For example, to transfer the SampleFile.txt file from your EC2 instance back to the home directory on your local computer as SampleFile2.txt , use of the following commands on your local computer. (Public DNS) To transfer a file to your instance using your instance\u2019s public DNS name, enter the following command. scp -i /path/my-key-pair.pem my-instance-user-name@my-instance-public-dns-name:~/SampleFile.txt ~/SampleFile2.txt (IPv6) Alternatively, if your instance has an IPv6 address, to transfer a file using the instance\u2019s IPv6 address, enter the following command. The IPv6 address must be enclosed in square brackets ([ ]), which must be escaped (). scp -i /path/my-key-pair.pem my-instance-user-name@\\[my-instance-IPv6-address\\]:~/SampleFile.txt ~/SampleFile2.txt Transferring files to your Linux instance using the PuTTY Secure Copy client \u00b6 The PuTTY Secure Copy client (PSCP) is a command line tool that you can use to transfer files between your Windows computer and your Linux instance. If you prefer a graphical user interface (GUI), you can use an open source GUI tool named WinSCP. For more information, see Transferring files to your Linux instance using WinSCP or FileZilla . To use PSCP, you need the private key you generated in Convert your private key using PuTTYgen . You also need the public DNS name of your Linux instance, or the IPv6 address if your instance has one. The following example transfers the file Sample_file.txt from the C:\\ drive on a Windows computer to the my-instance-user-name home directory on an Amazon Linux instance. To transfer a file, use one of the following commands. (Public DNS) To transfer a file using your instance\u2019s public DNS name, enter the following command. pscp -i C:\\path\\my-key-pair.ppk C:\\path\\Sample_file.txt my-instance-user-name@my-instance-public-dns-name:/home/my-instance-user-name/Sample_file.txt (IPv6) Alternatively, if your instance has an IPv6 address, to transfer a file using your instance\u2019s IPv6 address, enter the following command. The IPv6 address must be enclosed in square brackets ([ ]). pscp -i C:\\path\\my-key-pair.ppk C:\\path\\Sample_file.txt my-instance-user-name@[my-instance-IPv6-address]:/home/my-instance-user-name/Sample_file.txt Transferring files to your Linux instance using WinSCP \u00b6 WinSCP is a GUI-based file manager for Windows that allows you to upload and transfer files to a remote computer using the SFTP, SCP, FTP, and FTPS protocols. WinSCP allows you to drag and drop files from your Windows computer to your Linux instance or synchronize entire directory structures between the two systems. To use WinSCP, you need the private key that you generated in Convert your private key using PuTTYgen . You also need the public DNS name of your Linux instance. Prerequisites \u00b6 Download and install WinSCP from http://winscp.net/eng/download.php . For most users, the default installation options are OK. Start WinSCP. Configuration \u00b6 At the WinSCP login screen, for Host name , enter one of the following: (Public DNS or IPv4 address) To log in using your instance\u2019s public DNS name or public IPv4 address, enter the public DNS name or public IPv4 address for your instance. (IPv6) Alternatively, if your instance has an IPv6 address, to log in using your instance\u2019s IPv6 address, enter the IPv6 address for your instance. For User name, enter the default user name for your AMI. For Amazon Linux 2 or the Amazon Linux AMI , the user name is ec2-user . For a CentOS AMI , the user name is centos . For a Debian AMI , the user name is admin . For a Fedora AMI , the user name is ec2-user or fedora . For a RHEL AMI , the user name is ec2-user or root . For a SUSE AMI , the user name is ec2-user or root . For an Ubuntu AMI , the user name is ubuntu . Otherwise, if ec2-user and root don\u2019t work, check with the AMI provider. Specify the private key for your instance. For Private key , enter the path to your private key, or choose the \u201c \u2026 \u201d button to browse for the file. To open the advanced site settings, for newer versions of WinSCP, choose Advanced . To find the Private key file setting, under SSH , choose Authentication . Here is a screenshot from WinSCP version 5.9.4: WinSCP requires a PuTTY private key file (.ppk). You can convert a .pem security key file to the .ppk format using PuTTYgen. For more information, see (Convert your private key using PuTTYgen)[#convert-your-private-key-using-puttygen]. Connect and start transfering files \u00b6 Choose Login . To add the host fingerprint to the host cache, choose Yes . After the connection is established, in the connection window your Linux instance is on the right and your local machine is on the left. You can drag and drop files directly into the remote file system from your local machine. For more information on WinSCP, see the project documentation at http://winscp.net/eng/docs/start . If you receive a \u201cCannot execute SCP to start transfer\u201d error, you must first install scp on your Linux instance. For some operating systems, this is located in the openssh-clients package. For Amazon Linux variants, such as the Amazon ECS-optimized AMI, use the following command to install scp . [ec2-user ~]$ sudo yum install -y openssh-clients Watch it here \u00b6 Transferring files to your Linux instance using FileZilla \u00b6 FileZilla is a free software, cross-platform FTP application, consisting of FileZilla Client and FileZilla Server. Client binaries are available for Windows, Linux, and macOS, server binaries are available for Windows only. Both server and client support FTP and FTPS (FTP over SSL/TLS), while the client can in addition connect to SFTP servers. To use FileZilla, you need the private key that you generated in Convert your private key using PuTTYgen . You also need the public DNS name of your Linux instance. Prerequisites \u00b6 Download and install FileZilla from https://filezilla-project.org/download.php for Windows or Linux (Choose your flavor). For most users, the default installation options are OK. Start FileZilla. Configuration \u00b6 Edit (Preferences) > Settings > Connection > SFTP, Click Add key file Browse to the location of your .pem file and select it. A message box will appear asking your permission to convert the file into .ppk format. Click Yes , then give the file a name and store it somewhere. If you already convert the key by PuTTYgen , just browse the .ppk instead. If the new file is shown in the list of Keyfiles , then continue to the next step. If not, then click Add keyfile\u2026 and select the converted file. From File menu choose Site Manager , and choose Add a new site At the FileZilla Site Manager screen, for Host name , enter one of the following: (Public DNS or IPv4 address) To log in using your instance\u2019s public DNS name or public IPv4 address, enter the public DNS name or public IPv4 address for your instance. (IPv6) Alternatively, if your instance has an IPv6 address, to log in using your instance\u2019s IPv6 address, enter the IPv6 address for your instance. For the Protocol choose SFTP For the Login Type choose Normal For User name, enter the default user name for your AMI. For Amazon Linux 2 or the Amazon Linux AMI , the user name is ec2-user . For a CentOS AMI , the user name is centos . For a Debian AMI , the user name is admin . For a Fedora AMI , the user name is ec2-user or fedora . For a RHEL AMI , the user name is ec2-user or root . For a SUSE AMI , the user name is ec2-user or root . For an Ubuntu AMI , the user name is ubuntu . Otherwise, if ec2-user and root don\u2019t work, check with the AMI provider. Here is a screenshot from FileZilla version 3.49.1: Connect and start transfering files \u00b6 Choose Connect . To add the host fingerprint to the Site Manager . After the connection is established, in the connection window your Linux instance is on the right and your local machine is on the left. You can drag and drop files directly into the remote file system from your local machine. For more information on FileZilla, see the project documentation at https://wiki.filezilla-project.org/Documentation . If you receive a \u201cCannot execute SCP to start transfer\u201d error, you must first install scp on your Linux instance. For some operating systems, this is located in the openssh-clients package. For Amazon Linux variants, such as the Amazon ECS-optimized AMI, use the following command to install scp . [ec2-user ~]$ sudo yum install -y openssh-clients Watch it here \u00b6 Getting started with Amazon EC2 Windows Instances \u00b6 To get started with a Windows instance, see Getting started with Amazon EC2 Windows instances . Connecting to your Windows instance \u00b6 To connect to a Windows instance, see Connecting to Your Windows Instance in the Amazon EC2 User Guide for Windows Instances. Transfer files to Windows instances \u00b6 To transfer files to and from a Windows instance, see Transfer files to Windows instances in the Amazon EC2 User Guide for Windows Instances.","title":"Using EC2 Compute Instances"},{"location":"EC2/#amazon-elastic-compute-cloud-ec2-instances","text":"","title":"Amazon Elastic Compute Cloud (EC2) Instances"},{"location":"EC2/#what-is-amazon-ec2","text":"Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic. If you would like to know more about Cloud Computing from AWS point of view, see What is Cloud Computing?","title":"What is Amazon EC2?"},{"location":"EC2/#how-to-get-started-with-amazon-ec2","text":"First, you need to get set up to use Amazon EC2. After you are set up, you are ready to complete the Getting Started tutorial for Amazon EC2. Whenever you need more information about an Amazon EC2 feature, you can read the technical documentation. You need to get familiar with the follwoing: Instances and AMIs Regions and Availability Zones Instance types Tags","title":"How to get started with Amazon EC2"},{"location":"EC2/#setting-up-with-amazon-ec2","text":"Complete the tasks in this section to get set up for launching an Amazon EC2 instance for the first time: Sign up for AWS Create a key pair Create a security group","title":"Setting up with Amazon EC2"},{"location":"EC2/#sign-up-for-aws","text":"Please follow the instructions in Setting Up an AWS Account .","title":"Sign up for AWS"},{"location":"EC2/#create-a-key-pair","text":"AWS uses public-key cryptography to secure the login information for your instance. A Linux instance has no password; you use a key pair to log in to your instance securely. You specify the name of the key pair when you launch your instance, then provide the private key when you log in using SSH. If you haven\u2019t created a key pair already, you can create one using the Amazon EC2 console. Note that if you plan to launch instances in multiple Regions, you\u2019ll need to create a key pair in each Region. For more information about Regions, see Regions, Availability Zones, and Local Zones . You can create a key pair using one of the following method: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the navigation pane, choose Key Pairs . Choose Create key pair . For Name , enter a descriptive name for the key pair. Amazon EC2 associates the public key with the name that you specify as the key name. A key name can include up to 255 ASCII characters. It can\u2019t include leading or trailing spaces. For File format, choose the format in which to save the private key. To save the private key in a format that can be used with OpenSSH, choose pem . To save the private key in a format that can be used with PuTTY, choose ppk . Choose Create key pair . The private key file is automatically downloaded by your browser. The base file name is the name you specified as the name of your key pair, and the file name extension is determined by the file format you chose. Save the private key file in a safe place. /!\\ Important This is the only chance for you to save the private key file. If you will use an SSH client on a macOS or Linux computer to connect to your Linux instance, use the following command to set the permissions of your private key file so that only you can read it. chmod 400 my-key-pair.pem If you do not set these permissions, then you cannot connect to your instance using this key pair. For more information, see Error: Unprotected private key file .","title":"Create a key pair"},{"location":"EC2/#watch-it-here","text":"","title":"Watch it here"},{"location":"EC2/#create-a-security-group","text":"Security groups act as a firewall for associated instances, controlling both inbound and outbound traffic at the instance level. You must add rules to a security group that enable you to connect to your instance from your IP address using SSH. You can also add rules that allow inbound and outbound HTTP and HTTPS access from anywhere. Note that if you plan to launch instances in multiple Regions, you\u2019ll need to create a security group in each Region. For more information about Regions, see Regions, Availability Zones, and Local Zones .","title":"Create a security group"},{"location":"EC2/#prerequisites","text":"You\u2019ll need the public IPv4 address of your local computer. The security group editor in the Amazon EC2 console can automatically detect the public IPv4 address for you. Alternatively, you can use the search phrase \u201cwhat is my IP address\u201d in an Internet browser, or use the following service: Check IP . If you are connecting through an Internet service provider (ISP) or from behind a firewall without a static IP address, you need to find out the range of IP addresses used by client computers. To create a security group with least privilege Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the navigation bar, select a Region for the security group. Security groups are specific to a Region, so you should select the same Region in which you created your key pair. In the navigation pane, choose Security Groups . Choose Create security group . In the Basic details section, do the following: a. Enter a name for the new security group and a description. Use a name that is easy for you to remember, such as your user name, followed by SG , plus the Region name. For example, me_SG_uswest2. b. In the VPC list, select your default VPC for the Region. In the Inbound rules section, create the following rules (choose Add rule for each new rule): Choose HTTP from the Type list, and make sure that Source is set to Anywhere ( 0.0.0.0/0 ) . Choose HTTPS from the Type list, and make sure that Source is set to Anywhere ( 0.0.0.0/0 ) . Choose SSH from the Type list. In the Source box, choose My IP to automatically populate the field with the public IPv4 address of your local computer. Alternatively, choose Custom and specify the public IPv4 address of your computer or network in CIDR notation. To specify an individual IP address in CIDR notation, add the routing suffix /32 , for example, 203.0.113.25/32 . If your company allocates addresses from a range, specify the entire range, such as 203.0.113.0/24 . /!\\ Warning For security reasons, we don't recommend that you allow SSH access from all IPv4 addresses (0.0.0.0/0) to your instance, except for testing purposes and only for a short time. Choose Create security group . For more information, see Amazon EC2 security groups for Linux instances .","title":"Prerequisites"},{"location":"EC2/#getting-started-with-amazon-ec2-linux-instances","text":"When you sign up for AWS, you can get started with Amazon EC2 using the AWS Free Tier . If you created your AWS account less than 12 months ago, and have not already exceeded the free tier benefits for Amazon EC2, it will not cost you anything to complete this tutorial, because we help you select options that are within the free tier benefits. Otherwise, you\u2019ll incur the standard Amazon EC2 usage fees from the time that you launch the instance until you terminate the instance (which is the final task of this tutorial), even if it remains idle. The instance is an Amazon EBS-backed instance (meaning that the root volume is an EBS (Elastic Block Store) volume). You can either specify the Availability Zone in which your instance runs, or let Amazon EC2 select an Availability Zone for you. When you launch your instance, you secure it by specifying a key pair and security group. When you connect to your instance, you must specify the private key of the key pair that you specified when launching your instance. In ordere to implement the previous configuration, you should proceeed with the following: Launch an instance Connect to your instance Clean up your instance Terminate your instance Next Steps","title":"Getting started with Amazon EC2 Linux instances"},{"location":"EC2/#launch-an-instance","text":"You can launch a Linux instance using the AWS Management Console as described in the following procedure. This tutorial is intended to help you launch your first instance quickly, so it doesn\u2019t cover all possible options. For more information about the advanced options, see Launching an Instance . To launch an instance Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the console dashboard, choose Launch Instance . The Choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select an HVM version of Amazon Linux 2. Notice that these AMIs are marked \u201cFree tier eligible.\u201d On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro instance type, which is selected by default. The t2.micro instance type is eligible for the free tier. In Regions where t2.micro is unavailable, you can use a t3.micro instance under the free tier. For more information, see AWS Free Tier . Choose Review and Launch to let the wizard complete the other configuration settings for you. On the Review Instance Launch page, under Security Groups , you\u2019ll see that the wizard created and selected a security group for you. You can use this security group, or alternatively you can select the security group that you created when getting set up using the following steps: a. Choose Edit security groups . b. On the Configure Security Group page, ensure that Select an existing security group is selected. c. Select your security group from the list of existing security groups, and then choose Review and Launch . On the Review Instance Launch page, choose Launch . When prompted for a key pair, select Choose an existing key pair , then select the key pair that you created when getting set up. /!\\ Warning Don't select 'Proceed without a key pair'. If you launch your instance without a key pair, then you 'cannot' connect to it. When you are ready, select the acknowledgement check box, and then choose Launch Instances . A confirmation page lets you know that your instance is launching. Choose View Instances to close the confirmation page and return to the console. On the Instances screen, you can view the status of the launch. It takes a short time for an instance to launch. When you launch an instance, its initial state is pending . After the instance starts, its state changes to running and it receives a public DNS name. (If the Public DNS (IPv4) column is hidden, choose Show/Hide Columns (the gear-shaped icon) in the top right corner of the page and then select Public DNS (IPv4) .) It can take a few minutes for the instance to be ready so that you can connect to it. Check that your instance has passed its status checks; you can view this information in the Status Checks column.","title":"Launch an instance"},{"location":"EC2/#connect-to-your-instance","text":"There are several ways to connect to your Linux instance. For more information, see Connect to your Linux instance . /!\\ Important You can\u2019t connect to your instance unless you launched it with a key pair for which you have the .pem file and you launched it with a security group that allows SSH access from your computer. If you can\u2019t connect to your instance, see Troubleshooting connecting to your instance for assistance.","title":"Connect to your instance"},{"location":"EC2/#clean-up-your-instance","text":"After you\u2019ve finished with the instance that you created for this tutorial, you should clean up by terminating the instance. If you want to do more with this instance before you clean up, see Next steps . /!\\ Important Terminating an instance effectively deletes it; you can\u2019t reconnect to an instance after you\u2019ve terminated it.","title":"Clean up your instance"},{"location":"EC2/#terminate-your-instance","text":"If you launched an instance that is not within the AWS Free Tier , you\u2019ll stop incurring charges for that instance as soon as the instance status changes to shutting down or terminated . If you\u2019d like to keep your instance for later, but not incur charges, you can stop the instance now and then start it again later. For more information, see Stopping Instances . To terminate your instance In the navigation pane, choose Instances . In the list of instances, select the instance. Choose Actions, Instance State, Terminate . Choose Yes, Terminate when prompted for confirmation. Amazon EC2 shuts down and terminates your instance. After your instance is terminated, it remains visible on the console for a short while, and then the entry is deleted.","title":"Terminate your instance"},{"location":"EC2/#next-steps","text":"After you start your instance, you might want to try some of the following exercises: Learn how to remotely manage your EC2 instance using Run Command. For more information, see AWS Systems Manager Run Command in the AWS Systems Manager User Guide. Configure a CloudWatch alarm to notify you if your usage exceeds the Free Tier. For more information, see Create a Billing Alarm in the AWS Billing and Cost Management User Guide.","title":"Next Steps"},{"location":"EC2/#watch-it-here_1","text":"","title":"Watch it here"},{"location":"EC2/#connecting-to-your-linux-ec2-instances","text":"Connect to the Linux instances that you launched and transfer files between your local computer and your instance. The operating system of your local computer determines the options that you have to connect from your local computer to your Linux instance.","title":"Connecting to your Linux EC2 Instances"},{"location":"EC2/#connection-options","text":"If your local computer operating system is Linux or macOS X , you can connect by using one pf the following: SSH Client EC2 Instance Connect AWS Systems Manager Session Manager If your local computer operating system is Windows , you can connect by using one pf the following: PuTTY SSH Client AWS Systems Manager Session Manager Windows Subsystem for Linux","title":"Connection options"},{"location":"EC2/#connecting-to-your-linux-instance-using-ssh","text":"After you launch your instance, you can connect to it and use it the way that you\u2019d use a computer sitting in front of you.","title":"Connecting to your Linux instance using SSH"},{"location":"EC2/#prerequisites_1","text":"Before you connect to your Linux instance, complete the following prerequisites. Verify that the instance is ready After you launch an instance, it can take a few minutes for the instance to be ready so that you can connect to it. Check that your instance has passed its status checks. You can view this information in the Status Checks column on the Instances page. Verify the general prerequisites for connecting to your instance Check the General prerequisites for connecting to your instance . Install an SSH client on your local computer (either Linux or Windows) as needed Your local computer might have an SSH client installed by default. You can verify this by typing ssh at the command line. If your compute doesn\u2019t recognize the command, you can install an SSH client. Recent versions of Windows Server 2019 and Windows 10 - OpenSSH is included as an installable component. For information, see OpenSSH in Windows . Earlier versions of Windows - Download and install OpenSSH. For more information, see Win32-OpenSSH . Linux and macOS X - Download and install OpenSSH. For more information, see Linux-openssh .","title":"Prerequisites"},{"location":"EC2/#connect-to-your-linux-instance-using-an-ssh-client","text":"Use the following procedure to connect to your Linux instance using an SSH client. If you receive an error while attempting to connect to your instance, see Troubleshooting connecting to your instance . To connect to your instance using SSH In a terminal window, use the ssh command to connect to the instance. You specify the path and file name of the private key ( .pem ), the user name for your instance, and the public DNS name or IPv6 address for your instance. For more information about how to find the private key, the user name for your instance, and the DNS name or IPv6 address for an instance, see Locate the private key and Get information about your instance . To connect to your instance, use one of the following commands. a. (Public DNS) To connect using your instance\u2019s public DNS name, enter the following command. ssh -i /path/my-key-pair.pem my-instance-user-name@my-instance-public-dns-name b. (IPv6) Alternatively, if your instance has an IPv6 address, to connect using your instance\u2019s IPv6 address, enter the following command. ssh -i /path/my-key-pair.pem my-instance-user-name@my-instance-IPv6-address You see a response like the following: The authenticity of host 'ec2-198-51-100-1.compute-1.amazonaws.com (198-51-100-1)' can't be established. ECDSA key fingerprint is l4UB/neBad9tvkgJf1QZWxheQmR59WgrgzEimCG6kZY. Are you sure you want to continue connecting (yes/no)? Enter yes . You see a response like the following: Warning: Permanently added 'ec2-198-51-100-1.compute-1.amazonaws.com' (ECDSA) to the list of known hosts.","title":"Connect to your Linux instance using an SSH client"},{"location":"EC2/#connecting-to-your-linux-instance-from-windows-using-putty-or-mobaxterm","text":"After you launch your instance, you can connect to it and use it the way that you\u2019d use a computer sitting in front of you. The following instructions explain how to connect to your instance using PuTTY, and MobaXterm, free SSH client for Windows. If you receive an error while attempting to connect to your instance, see General prerequisites for connecting to your instance .","title":"Connecting to your Linux instance from Windows using PuTTY or MobaXterm"},{"location":"EC2/#prerequisites_2","text":"Before you connect to your Linux instance using PuTTY, complete the following prerequisites. Verify that the instance is ready After you launch an instance, it can take a few minutes for the instance to be ready so that you can connect to it. Check that your instance has passed its status checks. You can view this information in the Status Checks column on the Instances page. Verify the general prerequisites for connecting to your instance Check the General prerequisites for connecting to your instance . Install PuTTY on your local computer Download and install PuTTY from the PuTTY download page . If you already have an older version of PuTTY installed, we recommend that you download the latest version. Be sure to install the entire suite. Or Install MobaXterm on your local computer Download and install MobaXterm from the MobaXterm download page . Convert your private key using PuTTYgen In case you do not have a .ppk , locate the private key ( .pem file) for the key pair that you specified when you launched the instance. Convert the .pem file to a .ppk file for use with PuTTY. For more information, follow the steps in the next subsection.","title":"Prerequisites"},{"location":"EC2/#convert-your-private-key-using-puttygen","text":"PuTTY does not natively support the private key format for SSH keys. PuTTY provides a tool named PuTTYgen, which converts keys to the required format for PuTTY. You must convert your private key ( .pem file) into this format ( .ppk file) as follows in order to connect to your instance using PuTTY. To convert your private key From the Start menu, choose All Programs, PuTTY, PuTTYgen . Under Type of key to generate , choose RSA . If you\u2019re using an older version of PuTTYgen, choose SSH-2 RSA . Choose Load . By default, PuTTYgen displays only files with the extension .ppk . To locate your .pem file, choose the option to display files of all types. Select all file types Select your .pem file for the key pair that you specified when you launched your instance and choose Open . PuTTYgen displays a notice that the .pem file was successfully imported. Choose OK . To save the key in the format that PuTTY can use, choose Save private key . PuTTYgen displays a warning about saving the key without a passphrase. Choose Yes . /!\\ Note A passphrase on a private key is an extra layer of protection. Even if your private key is discovered, it can't be used without the passphrase. The downside to using a passphrase is that it makes automation harder because human intervention is needed to log on to an instance, or to copy files to an instance. Specify the same name for the key that you used for the key pair (for example, my-key-pair ) and choose Save . PuTTY automatically adds the .ppk file extension. Your private key is now in the correct format for use with PuTTY. You can now connect to your instance using PuTTY\u2019s SSH client.","title":"Convert your private key using PuTTYgen"},{"location":"EC2/#connecting-to-your-linux-instance","text":"Use the following procedure to connect to your Linux instance using PuTTY. You need the .ppk file that you created for your private key. For more information, see Convert your private key using PuTTYgen in the preceding section. If you receive an error while attempting to connect to your instance, see Troubleshooting Connecting to Your Instance . To connect to your instance using PuTTY Start PuTTY (from the Start menu, choose All Programs, PuTTY, PuTTY ). In the Category pane , choose Session and complete the following fields: a. In the Host Name box, do one of the following: (Public DNS) To connect using your instance\u2019s public DNS name, enter my-instance-user-name@my-instance-public-dns-name . (IPv6) Alternatively, if your instance has an IPv6 address, to connect using your instance\u2019s IPv6 address, enter my-instance-user-name@my-instance-IPv6-address . For information about how to get the user name for your instance, and the public DNS name or IPv6 address of your instance, see Get information about your instance . b. Ensure that the Port value is 22. c. Under Connection type , select SSH . (Optional) You can configure PuTTY to automatically send \u2018keepalive\u2019 data at regular intervals to keep the session active. This is useful to avoid disconnecting from your instance due to session inactivity. In the Category pane, choose Connection , and then enter the required interval in the Seconds between keepalives field. For example, if your session disconnects after 10 minutes of inactivity, enter 180 to configure PuTTY to send keepalive data every 3 minutes. In the Category pane, expand Connection , expand SSH , and then choose Auth . Complete the following: a. Choose Browse . b. Select the .ppk file that you generated for your key pair and choose Open . c. (Optional) If you plan to start this session again later, you can save the session information for future use. Under Category , choose Session , enter a name for the session in Saved Sessions , and then choose Save . d. Choose Open . If this is the first time you have connected to this instance, PuTTY displays a security alert dialog box that asks whether you trust the host to which you are connecting. a. Choose Yes . A window opens and you are connected to your instance. /!\\ Note If you specified a passphrase when you converted your private key to PuTTY\u2019s format, you must provide that passphrase when you log in to the instance. If you receive an error while attempting to connect to your instance, see Troubleshooting Connecting to Your Instance .","title":"Connecting to your Linux instance"},{"location":"EC2/#transferring-files-to-linux-instances","text":"Transferring files to your Linux instances depends on the type of the operating system you use to connect to the instance . Therfore: If you connect through Linux Two ways to transfer files between your local computer and a Linux instance are to use: The secure copy protocol (SCP) . FileZilla . If you connect through Windwos Many options are exist to transfer files between the instances and local machines, such as: The PuTTY Secure Copy client (PSCP) . WinSCP . FileZilla .","title":"Transferring files to Linux instances"},{"location":"EC2/#transferring-files-to-linux-instances-from-linux-using-scp","text":"One way to transfer files between your local computer and a Linux instance is to use the secure copy protocol (SCP). This section describes how to transfer files with SCP. The procedure is similar to the procedure for connecting to an instance with SSH.","title":"Transferring files to Linux instances from Linux using SCP"},{"location":"EC2/#prerequisites_3","text":"Verify the general prerequisites for transferring files to your instance. The general prerequisites for transferring files to an instance are the same as the general prerequisites for connecting to an instance. For more information, see General prerequisites for connecting to your instance . Install an SCP client Most Linux, Unix, and Apple computers include an SCP client by default. If yours doesn\u2019t, the OpenSSH project provides a free implementation of the full suite of SSH tools, including an SCP client. For more information, see http://www.openssh.org .","title":"Prerequisites"},{"location":"EC2/#use-scp-to-transfer-a-file","text":"Transfer a file to your instance using the instance\u2019s public DNS name, or the IPv6 address if your instance has one. For example , if the name of your private key file is my-key-pair , the file to transfer is SampleFile.txt , the user name for your instance is my-instance-user-name , and the public DNS name of the instance is my-instance-public-dns-name , or my-instance-IPv6-address if your instance has an IPv6 address, use one of the following commands to copy the file to the my-instance-user-name home directory. (Public DNS) To transfer a file to your instance using your instance\u2019s public DNS name, enter the following command. scp -i /path/my-key-pair.pem /path/SampleFile.txt my-instance-user-name@my-instance-public-dns-name:~ (IPv6) Alternatively, if your instance has an IPv6 address, to transfer a file using the instance\u2019s IPv6 address, enter the following command. The IPv6 address must be enclosed in square brackets ([ ]), which must be escaped (). scp -i /path/my-key-pair.pem /path/SampleFile.txt my-instance-user-name@\\[my-instance-IPv6-address\\]:~ You see a response like the following: The authenticity of host 'ec2-198-51-100-1.compute-1.amazonaws.com (10.254.142.33)' can't be established. RSA key fingerprint is 1f:51:ae:28:bf:89:e9:d8:1f:25:5d:37:2d:7d:b8:ca:9f:f5:f1:6f. Are you sure you want to continue connecting (yes/no)? Enter yes You see a response like the following: Warning: Permanently added 'ec2-198-51-100-1.compute-1.amazonaws.com' (RSA) to the list of known hosts. Sending file modes: C0644 20 SampleFile.txt Sink: C0644 20 SampleFile.txt SampleFile.txt 100% 20 0.0KB/s 00:00 If you receive a \u201cbash: scp: command not found\u201d error, you must first install scp on your Linux instance. For some operating systems, this is located in the openssh-clients package. For Amazon Linux variants, such as the Amazon ECS-optimized AMI, use the following command to install scp : [ec2-user ~]$ sudo yum install -y openssh-clients To transfer files in the other direction (from your Amazon EC2 instance to your local computer), reverse the order of the host parameters. For example, to transfer the SampleFile.txt file from your EC2 instance back to the home directory on your local computer as SampleFile2.txt , use of the following commands on your local computer. (Public DNS) To transfer a file to your instance using your instance\u2019s public DNS name, enter the following command. scp -i /path/my-key-pair.pem my-instance-user-name@my-instance-public-dns-name:~/SampleFile.txt ~/SampleFile2.txt (IPv6) Alternatively, if your instance has an IPv6 address, to transfer a file using the instance\u2019s IPv6 address, enter the following command. The IPv6 address must be enclosed in square brackets ([ ]), which must be escaped (). scp -i /path/my-key-pair.pem my-instance-user-name@\\[my-instance-IPv6-address\\]:~/SampleFile.txt ~/SampleFile2.txt","title":"Use SCP to transfer a file"},{"location":"EC2/#transferring-files-to-your-linux-instance-using-the-putty-secure-copy-client","text":"The PuTTY Secure Copy client (PSCP) is a command line tool that you can use to transfer files between your Windows computer and your Linux instance. If you prefer a graphical user interface (GUI), you can use an open source GUI tool named WinSCP. For more information, see Transferring files to your Linux instance using WinSCP or FileZilla . To use PSCP, you need the private key you generated in Convert your private key using PuTTYgen . You also need the public DNS name of your Linux instance, or the IPv6 address if your instance has one. The following example transfers the file Sample_file.txt from the C:\\ drive on a Windows computer to the my-instance-user-name home directory on an Amazon Linux instance. To transfer a file, use one of the following commands. (Public DNS) To transfer a file using your instance\u2019s public DNS name, enter the following command. pscp -i C:\\path\\my-key-pair.ppk C:\\path\\Sample_file.txt my-instance-user-name@my-instance-public-dns-name:/home/my-instance-user-name/Sample_file.txt (IPv6) Alternatively, if your instance has an IPv6 address, to transfer a file using your instance\u2019s IPv6 address, enter the following command. The IPv6 address must be enclosed in square brackets ([ ]). pscp -i C:\\path\\my-key-pair.ppk C:\\path\\Sample_file.txt my-instance-user-name@[my-instance-IPv6-address]:/home/my-instance-user-name/Sample_file.txt","title":"Transferring files to your Linux instance using the PuTTY Secure Copy client"},{"location":"EC2/#transferring-files-to-your-linux-instance-using-winscp","text":"WinSCP is a GUI-based file manager for Windows that allows you to upload and transfer files to a remote computer using the SFTP, SCP, FTP, and FTPS protocols. WinSCP allows you to drag and drop files from your Windows computer to your Linux instance or synchronize entire directory structures between the two systems. To use WinSCP, you need the private key that you generated in Convert your private key using PuTTYgen . You also need the public DNS name of your Linux instance.","title":"Transferring files to your Linux instance using WinSCP"},{"location":"EC2/#prerequisites_4","text":"Download and install WinSCP from http://winscp.net/eng/download.php . For most users, the default installation options are OK. Start WinSCP.","title":"Prerequisites"},{"location":"EC2/#configuration","text":"At the WinSCP login screen, for Host name , enter one of the following: (Public DNS or IPv4 address) To log in using your instance\u2019s public DNS name or public IPv4 address, enter the public DNS name or public IPv4 address for your instance. (IPv6) Alternatively, if your instance has an IPv6 address, to log in using your instance\u2019s IPv6 address, enter the IPv6 address for your instance. For User name, enter the default user name for your AMI. For Amazon Linux 2 or the Amazon Linux AMI , the user name is ec2-user . For a CentOS AMI , the user name is centos . For a Debian AMI , the user name is admin . For a Fedora AMI , the user name is ec2-user or fedora . For a RHEL AMI , the user name is ec2-user or root . For a SUSE AMI , the user name is ec2-user or root . For an Ubuntu AMI , the user name is ubuntu . Otherwise, if ec2-user and root don\u2019t work, check with the AMI provider. Specify the private key for your instance. For Private key , enter the path to your private key, or choose the \u201c \u2026 \u201d button to browse for the file. To open the advanced site settings, for newer versions of WinSCP, choose Advanced . To find the Private key file setting, under SSH , choose Authentication . Here is a screenshot from WinSCP version 5.9.4: WinSCP requires a PuTTY private key file (.ppk). You can convert a .pem security key file to the .ppk format using PuTTYgen. For more information, see (Convert your private key using PuTTYgen)[#convert-your-private-key-using-puttygen].","title":"Configuration"},{"location":"EC2/#connect-and-start-transfering-files","text":"Choose Login . To add the host fingerprint to the host cache, choose Yes . After the connection is established, in the connection window your Linux instance is on the right and your local machine is on the left. You can drag and drop files directly into the remote file system from your local machine. For more information on WinSCP, see the project documentation at http://winscp.net/eng/docs/start . If you receive a \u201cCannot execute SCP to start transfer\u201d error, you must first install scp on your Linux instance. For some operating systems, this is located in the openssh-clients package. For Amazon Linux variants, such as the Amazon ECS-optimized AMI, use the following command to install scp . [ec2-user ~]$ sudo yum install -y openssh-clients","title":"Connect and start transfering files"},{"location":"EC2/#watch-it-here_2","text":"","title":"Watch it here"},{"location":"EC2/#transferring-files-to-your-linux-instance-using-filezilla","text":"FileZilla is a free software, cross-platform FTP application, consisting of FileZilla Client and FileZilla Server. Client binaries are available for Windows, Linux, and macOS, server binaries are available for Windows only. Both server and client support FTP and FTPS (FTP over SSL/TLS), while the client can in addition connect to SFTP servers. To use FileZilla, you need the private key that you generated in Convert your private key using PuTTYgen . You also need the public DNS name of your Linux instance.","title":"Transferring files to your Linux instance using FileZilla"},{"location":"EC2/#prerequisites_5","text":"Download and install FileZilla from https://filezilla-project.org/download.php for Windows or Linux (Choose your flavor). For most users, the default installation options are OK. Start FileZilla.","title":"Prerequisites"},{"location":"EC2/#configuration_1","text":"Edit (Preferences) > Settings > Connection > SFTP, Click Add key file Browse to the location of your .pem file and select it. A message box will appear asking your permission to convert the file into .ppk format. Click Yes , then give the file a name and store it somewhere. If you already convert the key by PuTTYgen , just browse the .ppk instead. If the new file is shown in the list of Keyfiles , then continue to the next step. If not, then click Add keyfile\u2026 and select the converted file. From File menu choose Site Manager , and choose Add a new site At the FileZilla Site Manager screen, for Host name , enter one of the following: (Public DNS or IPv4 address) To log in using your instance\u2019s public DNS name or public IPv4 address, enter the public DNS name or public IPv4 address for your instance. (IPv6) Alternatively, if your instance has an IPv6 address, to log in using your instance\u2019s IPv6 address, enter the IPv6 address for your instance. For the Protocol choose SFTP For the Login Type choose Normal For User name, enter the default user name for your AMI. For Amazon Linux 2 or the Amazon Linux AMI , the user name is ec2-user . For a CentOS AMI , the user name is centos . For a Debian AMI , the user name is admin . For a Fedora AMI , the user name is ec2-user or fedora . For a RHEL AMI , the user name is ec2-user or root . For a SUSE AMI , the user name is ec2-user or root . For an Ubuntu AMI , the user name is ubuntu . Otherwise, if ec2-user and root don\u2019t work, check with the AMI provider. Here is a screenshot from FileZilla version 3.49.1:","title":"Configuration"},{"location":"EC2/#connect-and-start-transfering-files_1","text":"Choose Connect . To add the host fingerprint to the Site Manager . After the connection is established, in the connection window your Linux instance is on the right and your local machine is on the left. You can drag and drop files directly into the remote file system from your local machine. For more information on FileZilla, see the project documentation at https://wiki.filezilla-project.org/Documentation . If you receive a \u201cCannot execute SCP to start transfer\u201d error, you must first install scp on your Linux instance. For some operating systems, this is located in the openssh-clients package. For Amazon Linux variants, such as the Amazon ECS-optimized AMI, use the following command to install scp . [ec2-user ~]$ sudo yum install -y openssh-clients","title":"Connect and start transfering files"},{"location":"EC2/#watch-it-here_3","text":"","title":"Watch it here"},{"location":"EC2/#getting-started-with-amazon-ec2-windows-instances","text":"To get started with a Windows instance, see Getting started with Amazon EC2 Windows instances .","title":"Getting started with Amazon EC2 Windows Instances"},{"location":"EC2/#connecting-to-your-windows-instance","text":"To connect to a Windows instance, see Connecting to Your Windows Instance in the Amazon EC2 User Guide for Windows Instances.","title":"Connecting to your Windows instance"},{"location":"EC2/#transfer-files-to-windows-instances","text":"To transfer files to and from a Windows instance, see Transfer files to Windows instances in the Amazon EC2 User Guide for Windows Instances.","title":"Transfer files to Windows instances"},{"location":"EFS/","text":"Amazon EFS volumes \u00b6 Amazon Elastic File System (Amazon EFS) provides simple, scalable, elastic file storage for use with AWS Cloud services and on-premises resources. It is straightforward to use, and it offers a simple interface that allows you to create and configure file systems quickly and easily. Amazon EFS is designed to provide massively parallel shared access to thousands of Amazon EC2 instances . This enables your applications to achieve high levels of aggregate throughput and IOPS that scale as a file system grows, with consistent low latencies . When an Amazon EFS file system is mounted on Amazon EC2 instances, it provides a standard file system interface and file system access semantics, which allows you to seamlessly integrate Amazon EFS with your existing applications and tools. Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time , thus allowing Amazon EFS to provide a common data source for workloads and applications that run on more than one Amazon EC2 instance. Current details on Amazon EFS can be found at: https://aws.amazon.com/efs/ . Amazon EFS features \u00b6 Amazon EFS is well suited to a broad range of use cases, from home directories to business-critical applications. Customers can use Amazon EFS to move NFS-based file storage workloads to managed file systems on the AWS Cloud. Other use cases include: analytics, web serving and content management, application development and testing, media and entertainment workflows, database backups, and containers and serverless storage. EFS has many features , such as: Highly available and durable Storage classes and lifecycle management Security and compliance Scalable performance Elastic and scalable Encryption Fully managed Fully Managed feature \u00b6 Amazon EFS is a fully managed service providing NFS shared file system storage for Linux workloads. Amazon EFS makes it simple to create and configure file systems. You don\u2019t have to worry about managing file servers or storage, updating hardware, configuring software, or performing backups. In seconds, you can create a fully managed file system by using the AWS Management Console, the AWS CLI, or an AWS SDK. EFS modes and services \u00b6 EFS has many modes, such as: Performance modes Throughput modes Containers and serverless file storage Data transfer and backup hared file system with NFS v4.0 and v4.1 support Please check EFS pricing, https://aws.amazon.com/efs/pricing/ before start working with it. Amazon EBS vs. EFS volumes \u00b6 Amazon EFS, and Amazon EBS are AWS\u2019 two different block-storage types that can be applicable for different types of workload needs. Let\u2019s take a closer look at the key features of each option, as well as the similarities and differences , see Amazon EFS FAQs Similarities and Differences \u00b6 Amazon EBS delivers high-availability block-level storage volumes for Amazon Elastic Compute Cloud (EC2) instances. It stores data on a file system which is retained after the EC2 instance is shut down. Amazon EFS offers scalable file storage, also optimized for EC2. It can be used as a common data source for any application or workload that runs on numerous instances. Using an EFS file system, you may configure instances to mount the file system. The main differences between EBS and EFS is that EBS is only accessible from a single EC2 instance in your particular AWS region, while EFS allows you to mount the file system across multiple regions and instances . So how can you choose between Amazon EBS vs EFS? That depends on what benefits you\u2019re looking for, and your use case for your workload. Let\u2019s take an in-depth look at each one to understand their benefits and use cases. EBS and EFS Use Cases \u00b6 Amazon EBS \u00b6 Testing and development NoSQL databases Relational databases Business consistency Enterprise-wide applications Amazon EFS \u00b6 Lift-and-shift application support: EFS is elastic, available, and scalable, and enables you to move enterprise applications easily and quickly without needing to re-architect them. Analytics for big data: It has the ability to run big data applications, which demand significant node throughput, low-latency file access, and read-after-write operations. Content management system and web server support: EFS is a robust throughput file system capable of enabling content management systems and web serving applications, such as archives, websites, or blogs. Application development and testing: Only EFS provides a shared file system needed to share code and files, across multiple compute resources to facilitate auto-scaling workloads. Getting Started with Amazon Elastic File System: Demo \u00b6 Here, you can learn how to quickly create an Amazon Elastic File System (Amazon EFS) file system. As part of this process, you mount your file system on an Amazon Elastic Compute Cloud (Amazon EC2) instance in your virtual private cloud (VPC). Prerequisites \u00b6 You need to know the follwoeing: You\u2019re already familiar with using the Amazon EC2 console to launch instances. Your Amazon VPC, Amazon EC2, and Amazon EFS resources are all in the same AWS Region. You can use the default VPC in the AWS Region that you\u2019re using for this demonstration. Step 1: Create Your Amazon EFS File System \u00b6 To create your Amazon EFS file system: Open the Amazon EFS Management Console at https://console.aws.amazon.com/efs/ . Choose Create file system to open the Create file system dialog box. (Optional) Enter a Name for your file system. For Virtual Private Cloud (VPC) , choose your VPC, or keep it set to your default VPC. Choose Create to create a file system. The File systems page appears with a banner across the top showing the status of the file system you created. A link to access the file system details page appears in the banner when the file system becomes available. Check the file system created and its service recommended settings, see step1 . Step 2: Create Your EC2 Resources and Launch Your EC2 Instance \u00b6 /!\\ Note You can't use Amazon EFS with Microsoft Windows\u2013based Amazon EC2 instances. To launch the EC2 instance and mount an EFS file system: Launch your EC2 instance as usual, see Launch Your EC2 Instance , please follow the next steps recomendations. In the Step,Configure Instance Details , provide the following information: a. For Network , choose the entry for the same VPC that you noted when you created your EFS file system. b. For Subnet , choose a default subnet in any Availability Zone. c. For File systems, make sure that the EFS file system that you created in Step 1: Create Your Amazon EFS File System is selected. The path shown next to the file system ID is the mount point that the EC2 instance will use, which you can change. Choose Add to user data to mount the file system when the EC2 is launched. d. Under Advanced Details , confirm that the user data is present in User data . In the Step, Configure Security Group , set Assign a security group to Select an existing security group . Choose the default security group to make sure that it can access your EFS file system. Finally, Choose Launch to start launching your instance. Step 3: Mount your Amazon EFS File System \u00b6 After creating your EFS volume and launcing your EC2 instance(s) either with Linux or Ubuntu AMIs, see Mounting EFS file systems . Follow the following steps: Connect to your instance(s), see Connecting to your Linux EC2 Instances . Mount your file system , by: a. Open the Amazon EFS console and then select the region where your EFS volume is located. b. Select the radio button next to your file system to display the details. c. Click the Attach button to see the mount instructions link. d. Use the NFS client or EFS mount helper DNS to mount the EFS volume to your instances. You need to prepare your mounting in your instances, see Mounting EFS File System . Step 4: Clean Up Resources and Protect Your AWS Account \u00b6 To clean up resources and protect your AWS account, see Terminate your resources : Connect to your Amazon EC2 instance. Unmount the Amazon EFS file system with the following command. $> sudo umount efs Open the Amazon EFS console at https://console.aws.amazon.com/efs/ . Choose the Amazon EFS file system that you want to delete from the list of file systems. For Actions , choose Delete file system . In the Permanently delete file system dialog box, type the file system ID for the Amazon EFS file system that you want to delete, and then choose Delete File System . In case you finished with your instance(s), you can Terminate Instances and Delete Security Group from Actions . /!\\ Warning Don't delete the default security group for your VPC.","title":"Amazon EFS volumes"},{"location":"EFS/#amazon-efs-volumes","text":"Amazon Elastic File System (Amazon EFS) provides simple, scalable, elastic file storage for use with AWS Cloud services and on-premises resources. It is straightforward to use, and it offers a simple interface that allows you to create and configure file systems quickly and easily. Amazon EFS is designed to provide massively parallel shared access to thousands of Amazon EC2 instances . This enables your applications to achieve high levels of aggregate throughput and IOPS that scale as a file system grows, with consistent low latencies . When an Amazon EFS file system is mounted on Amazon EC2 instances, it provides a standard file system interface and file system access semantics, which allows you to seamlessly integrate Amazon EFS with your existing applications and tools. Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time , thus allowing Amazon EFS to provide a common data source for workloads and applications that run on more than one Amazon EC2 instance. Current details on Amazon EFS can be found at: https://aws.amazon.com/efs/ .","title":"Amazon EFS volumes"},{"location":"EFS/#amazon-efs-features","text":"Amazon EFS is well suited to a broad range of use cases, from home directories to business-critical applications. Customers can use Amazon EFS to move NFS-based file storage workloads to managed file systems on the AWS Cloud. Other use cases include: analytics, web serving and content management, application development and testing, media and entertainment workflows, database backups, and containers and serverless storage. EFS has many features , such as: Highly available and durable Storage classes and lifecycle management Security and compliance Scalable performance Elastic and scalable Encryption Fully managed","title":"Amazon EFS features"},{"location":"EFS/#fully-managed-feature","text":"Amazon EFS is a fully managed service providing NFS shared file system storage for Linux workloads. Amazon EFS makes it simple to create and configure file systems. You don\u2019t have to worry about managing file servers or storage, updating hardware, configuring software, or performing backups. In seconds, you can create a fully managed file system by using the AWS Management Console, the AWS CLI, or an AWS SDK.","title":"Fully Managed feature"},{"location":"EFS/#efs-modes-and-services","text":"EFS has many modes, such as: Performance modes Throughput modes Containers and serverless file storage Data transfer and backup hared file system with NFS v4.0 and v4.1 support Please check EFS pricing, https://aws.amazon.com/efs/pricing/ before start working with it.","title":"EFS modes and services"},{"location":"EFS/#amazon-ebs-vs-efs-volumes","text":"Amazon EFS, and Amazon EBS are AWS\u2019 two different block-storage types that can be applicable for different types of workload needs. Let\u2019s take a closer look at the key features of each option, as well as the similarities and differences , see Amazon EFS FAQs","title":"Amazon EBS vs. EFS volumes"},{"location":"EFS/#similarities-and-differences","text":"Amazon EBS delivers high-availability block-level storage volumes for Amazon Elastic Compute Cloud (EC2) instances. It stores data on a file system which is retained after the EC2 instance is shut down. Amazon EFS offers scalable file storage, also optimized for EC2. It can be used as a common data source for any application or workload that runs on numerous instances. Using an EFS file system, you may configure instances to mount the file system. The main differences between EBS and EFS is that EBS is only accessible from a single EC2 instance in your particular AWS region, while EFS allows you to mount the file system across multiple regions and instances . So how can you choose between Amazon EBS vs EFS? That depends on what benefits you\u2019re looking for, and your use case for your workload. Let\u2019s take an in-depth look at each one to understand their benefits and use cases.","title":"Similarities and Differences"},{"location":"EFS/#ebs-and-efs-use-cases","text":"","title":"EBS and EFS Use Cases"},{"location":"EFS/#amazon-ebs","text":"Testing and development NoSQL databases Relational databases Business consistency Enterprise-wide applications","title":"Amazon EBS"},{"location":"EFS/#amazon-efs","text":"Lift-and-shift application support: EFS is elastic, available, and scalable, and enables you to move enterprise applications easily and quickly without needing to re-architect them. Analytics for big data: It has the ability to run big data applications, which demand significant node throughput, low-latency file access, and read-after-write operations. Content management system and web server support: EFS is a robust throughput file system capable of enabling content management systems and web serving applications, such as archives, websites, or blogs. Application development and testing: Only EFS provides a shared file system needed to share code and files, across multiple compute resources to facilitate auto-scaling workloads.","title":"Amazon EFS"},{"location":"EFS/#getting-started-with-amazon-elastic-file-system-demo","text":"Here, you can learn how to quickly create an Amazon Elastic File System (Amazon EFS) file system. As part of this process, you mount your file system on an Amazon Elastic Compute Cloud (Amazon EC2) instance in your virtual private cloud (VPC).","title":"Getting Started with Amazon Elastic File System: Demo"},{"location":"EFS/#prerequisites","text":"You need to know the follwoeing: You\u2019re already familiar with using the Amazon EC2 console to launch instances. Your Amazon VPC, Amazon EC2, and Amazon EFS resources are all in the same AWS Region. You can use the default VPC in the AWS Region that you\u2019re using for this demonstration.","title":"Prerequisites"},{"location":"EFS/#step-1-create-your-amazon-efs-file-system","text":"To create your Amazon EFS file system: Open the Amazon EFS Management Console at https://console.aws.amazon.com/efs/ . Choose Create file system to open the Create file system dialog box. (Optional) Enter a Name for your file system. For Virtual Private Cloud (VPC) , choose your VPC, or keep it set to your default VPC. Choose Create to create a file system. The File systems page appears with a banner across the top showing the status of the file system you created. A link to access the file system details page appears in the banner when the file system becomes available. Check the file system created and its service recommended settings, see step1 .","title":"Step 1: Create Your Amazon EFS File System"},{"location":"EFS/#step-2-create-your-ec2-resources-and-launch-your-ec2-instance","text":"/!\\ Note You can't use Amazon EFS with Microsoft Windows\u2013based Amazon EC2 instances. To launch the EC2 instance and mount an EFS file system: Launch your EC2 instance as usual, see Launch Your EC2 Instance , please follow the next steps recomendations. In the Step,Configure Instance Details , provide the following information: a. For Network , choose the entry for the same VPC that you noted when you created your EFS file system. b. For Subnet , choose a default subnet in any Availability Zone. c. For File systems, make sure that the EFS file system that you created in Step 1: Create Your Amazon EFS File System is selected. The path shown next to the file system ID is the mount point that the EC2 instance will use, which you can change. Choose Add to user data to mount the file system when the EC2 is launched. d. Under Advanced Details , confirm that the user data is present in User data . In the Step, Configure Security Group , set Assign a security group to Select an existing security group . Choose the default security group to make sure that it can access your EFS file system. Finally, Choose Launch to start launching your instance.","title":"Step 2: Create Your EC2 Resources and Launch Your EC2 Instance"},{"location":"EFS/#step-3-mount-your-amazon-efs-file-system","text":"After creating your EFS volume and launcing your EC2 instance(s) either with Linux or Ubuntu AMIs, see Mounting EFS file systems . Follow the following steps: Connect to your instance(s), see Connecting to your Linux EC2 Instances . Mount your file system , by: a. Open the Amazon EFS console and then select the region where your EFS volume is located. b. Select the radio button next to your file system to display the details. c. Click the Attach button to see the mount instructions link. d. Use the NFS client or EFS mount helper DNS to mount the EFS volume to your instances. You need to prepare your mounting in your instances, see Mounting EFS File System .","title":"Step 3: Mount your  Amazon EFS File System"},{"location":"EFS/#step-4-clean-up-resources-and-protect-your-aws-account","text":"To clean up resources and protect your AWS account, see Terminate your resources : Connect to your Amazon EC2 instance. Unmount the Amazon EFS file system with the following command. $> sudo umount efs Open the Amazon EFS console at https://console.aws.amazon.com/efs/ . Choose the Amazon EFS file system that you want to delete from the list of file systems. For Actions , choose Delete file system . In the Permanently delete file system dialog box, type the file system ID for the Amazon EFS file system that you want to delete, and then choose Delete File System . In case you finished with your instance(s), you can Terminate Instances and Delete Security Group from Actions . /!\\ Warning Don't delete the default security group for your VPC.","title":"Step 4: Clean Up Resources and Protect Your AWS Account"},{"location":"EFSmount/","text":"Mounting EFS File System in Linux and Ubuntu \u00b6 To mount the EFS system you need to check the recommendation before starting the installations in your instances. Then you can mount your EFS volume either by: NFS client {Network-based mounting} EFS mount helper {Local-based mounting} Recommendations \u00b6 There are some recommendations to have your EFS volume mounted successfully, such as: VPC Security groups VPC \u00b6 You should keep all the components of your environment in one VPC, either the default VPC or your created VPC, see Create Your Amazon EFS File System . Security groups \u00b6 In case you need to mount your EFS by using the nfs (Netwrk File System), you need to have two security groupes in each of your instances: the default security group to make sure that it can access your EFS file system. , and your security group for accessing the instance. To do that, see Create Your EC2 Resources : In Step 6: Configure Security Group , set Assign a security group to Select an existing security group. Choose the default security group to make sure that it can access your EFS file system. With your instance selected, select Actions > Networking > Change Security Groups . Select the checkbox for the default VPC security group and click Assign Security Groups . NFS client \u00b6 After connecting to your instances, you can install the NFS client either by the following commands or by copying these commands in the User data section to run them during the instances boot. To do so, Stop your instances and right cick on instance and in the Instance Setting list, choose Edit user data , and write the commands there. Linux AMI Instance \u00b6 Install the NFS client by: $> sudo yum install -y nfs-utils Make a directory called efs to mount on it: $> sudo mkdir efs Go back to the EFS Attach window (in the EFS console ). Mount your file system as your new directory. $> sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-38920a40.efs.us-east-2.amazonaws.com:/ efs Verify your file system has been successfully mounted by running the following command: $> df -h Create a test file in your new file system by running a simple dd command to generate a 1GiB file in your new directory. Run the following dd command in your SSH window: $> sudo dd if=/dev/zero of=~/efs/1GiB bs=1M count=1024 status=progress Ubuntu AMI Instance \u00b6 Install the NFS client by: S> sudo apt-get update $> sudo apt-get install nfs-common Make a directory called efs to mount on it: $> sudo mkdir efs Go back to the EFS Attach window (in the EFS console ). Mount your file system as your new directory. $> sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-38920a40.efs.us-east-2.amazonaws.com:/ efs Verify your file system has been successfully mounted by running the following command: $> df -h Create a test file in your new file system by running a simple dd command to generate a 1GiB file in your new directory. Run the following dd command in your SSH window: $> sudo dd if=/dev/zero of=~/efs/1GiB bs=1M count=1024 status=progress EFS mount helper \u00b6 After connecting to your instances, you can install the aws-mount-helper either by the following commands or by copying these commands in the User data section to run them during the instances boot. To do so, Stop your instances and right cick on instance and in the Instance Setting list, choose Edit user data , and write the commands there. If you don\u2019t want to get the amazon-efs-utils package from Amazon Linux or Amazon Linux 2 AMIs, the amazon-efs-utils package is also available on GitHub. After you clone the package, you can build and install amazon-efs-utils using one of the following methods, depending on the package type supported by your Linux distribution: RPM \u2013 This package type is supported by Amazon Linux, Red Hat Linux, CentOS, and similar. DEB \u2013 This package type is supported by Ubuntu, Debian, and similar. Installing the amazon-efs-utils Package on Other Linux Distributions : If you haven\u2019t done so already, install git with the following commands. $> sudo yum -y install git From the terminal, clone the amazon-efs-utils tool from GitHub to a directory of your choice, with the following command. $> git clone https://github.com/aws/efs-utils Because you need the bash command make, you can install it with the following command if your operating system doesn\u2019t already have it. $> sudo yum -y install make Linux AMI Instance \u00b6 Build and install amazon-efs-utils as an RPM package a. Open a terminal on your client and navigate to the directory that has the cloned amazon-efs-utils package from GitHub (for example \u201c/home/centos/efs-utils\u201d). b. If you haven\u2019t done so already, install the rpm-builder package with the following command. $> sudo yum -y install rpm-build c. Build the package with the following command. $> sudo make rpm d. Install the amazon-efs-utils package with the following command. $> sudo yum -y install ./build/amazon-efs-utils*rpm Make a directory called efs to mount on it: $> sudo mkdir efs Go back to the EFS Attach window (in the EFS console ). Mount your file system as your new directory. $> sudo mount -t efs -o tls fs-38920a40:/ efs Verify your file system has been successfully mounted by running the following command: $> df -h Create a test file in your new file system by running a simple dd command to generate a 1GiB file in your new directory. Run the following dd command in your SSH window: $> sudo dd if=/dev/zero of=~/efs/1GiB bs=1M count=1024 status=progress Ubuntu AMI Instance \u00b6 Build and install amazon-efs-utils as a DEB package a. Open a terminal on your client and navigate to the directory that has the cloned amazon-efs-utils package from GitHub. b. Install the binutils package, a dependency for building DEB packages. $> sudo apt-get -y install binutils c. Build the package with the following command. $> ./build-deb.sh d. Install the package with the following command. $> sudo apt-get -y install ./build/amazon-efs-utils*deb Make a directory called efs to mount on it: $> sudo mkdir efs Go back to the EFS Attach window (in the EFS console ). Mount your file system as your new directory. $> sudo mount -t efs -o tls fs-38920a40:/ efs Verify your file system has been successfully mounted by running the following command: $> df -h Create a test file in your new file system by running a simple dd command to generate a 1GiB file in your new directory. Run the following dd command in your SSH window: $> sudo dd if=/dev/zero of=~/efs/1GiB bs=1M count=1024 status=progress","title":"Mounting EFS File System in Linux and Ubuntu"},{"location":"EFSmount/#mounting-efs-file-system-in-linux-and-ubuntu","text":"To mount the EFS system you need to check the recommendation before starting the installations in your instances. Then you can mount your EFS volume either by: NFS client {Network-based mounting} EFS mount helper {Local-based mounting}","title":"Mounting EFS File System in Linux and Ubuntu"},{"location":"EFSmount/#recommendations","text":"There are some recommendations to have your EFS volume mounted successfully, such as: VPC Security groups","title":"Recommendations"},{"location":"EFSmount/#vpc","text":"You should keep all the components of your environment in one VPC, either the default VPC or your created VPC, see Create Your Amazon EFS File System .","title":"VPC"},{"location":"EFSmount/#security-groups","text":"In case you need to mount your EFS by using the nfs (Netwrk File System), you need to have two security groupes in each of your instances: the default security group to make sure that it can access your EFS file system. , and your security group for accessing the instance. To do that, see Create Your EC2 Resources : In Step 6: Configure Security Group , set Assign a security group to Select an existing security group. Choose the default security group to make sure that it can access your EFS file system. With your instance selected, select Actions > Networking > Change Security Groups . Select the checkbox for the default VPC security group and click Assign Security Groups .","title":"Security groups"},{"location":"EFSmount/#nfs-client","text":"After connecting to your instances, you can install the NFS client either by the following commands or by copying these commands in the User data section to run them during the instances boot. To do so, Stop your instances and right cick on instance and in the Instance Setting list, choose Edit user data , and write the commands there.","title":"NFS client"},{"location":"EFSmount/#linux-ami-instance","text":"Install the NFS client by: $> sudo yum install -y nfs-utils Make a directory called efs to mount on it: $> sudo mkdir efs Go back to the EFS Attach window (in the EFS console ). Mount your file system as your new directory. $> sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-38920a40.efs.us-east-2.amazonaws.com:/ efs Verify your file system has been successfully mounted by running the following command: $> df -h Create a test file in your new file system by running a simple dd command to generate a 1GiB file in your new directory. Run the following dd command in your SSH window: $> sudo dd if=/dev/zero of=~/efs/1GiB bs=1M count=1024 status=progress","title":"Linux AMI Instance"},{"location":"EFSmount/#ubuntu-ami-instance","text":"Install the NFS client by: S> sudo apt-get update $> sudo apt-get install nfs-common Make a directory called efs to mount on it: $> sudo mkdir efs Go back to the EFS Attach window (in the EFS console ). Mount your file system as your new directory. $> sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-38920a40.efs.us-east-2.amazonaws.com:/ efs Verify your file system has been successfully mounted by running the following command: $> df -h Create a test file in your new file system by running a simple dd command to generate a 1GiB file in your new directory. Run the following dd command in your SSH window: $> sudo dd if=/dev/zero of=~/efs/1GiB bs=1M count=1024 status=progress","title":"Ubuntu AMI Instance"},{"location":"EFSmount/#efs-mount-helper","text":"After connecting to your instances, you can install the aws-mount-helper either by the following commands or by copying these commands in the User data section to run them during the instances boot. To do so, Stop your instances and right cick on instance and in the Instance Setting list, choose Edit user data , and write the commands there. If you don\u2019t want to get the amazon-efs-utils package from Amazon Linux or Amazon Linux 2 AMIs, the amazon-efs-utils package is also available on GitHub. After you clone the package, you can build and install amazon-efs-utils using one of the following methods, depending on the package type supported by your Linux distribution: RPM \u2013 This package type is supported by Amazon Linux, Red Hat Linux, CentOS, and similar. DEB \u2013 This package type is supported by Ubuntu, Debian, and similar. Installing the amazon-efs-utils Package on Other Linux Distributions : If you haven\u2019t done so already, install git with the following commands. $> sudo yum -y install git From the terminal, clone the amazon-efs-utils tool from GitHub to a directory of your choice, with the following command. $> git clone https://github.com/aws/efs-utils Because you need the bash command make, you can install it with the following command if your operating system doesn\u2019t already have it. $> sudo yum -y install make","title":"EFS mount helper"},{"location":"EFSmount/#linux-ami-instance_1","text":"Build and install amazon-efs-utils as an RPM package a. Open a terminal on your client and navigate to the directory that has the cloned amazon-efs-utils package from GitHub (for example \u201c/home/centos/efs-utils\u201d). b. If you haven\u2019t done so already, install the rpm-builder package with the following command. $> sudo yum -y install rpm-build c. Build the package with the following command. $> sudo make rpm d. Install the amazon-efs-utils package with the following command. $> sudo yum -y install ./build/amazon-efs-utils*rpm Make a directory called efs to mount on it: $> sudo mkdir efs Go back to the EFS Attach window (in the EFS console ). Mount your file system as your new directory. $> sudo mount -t efs -o tls fs-38920a40:/ efs Verify your file system has been successfully mounted by running the following command: $> df -h Create a test file in your new file system by running a simple dd command to generate a 1GiB file in your new directory. Run the following dd command in your SSH window: $> sudo dd if=/dev/zero of=~/efs/1GiB bs=1M count=1024 status=progress","title":"Linux AMI Instance"},{"location":"EFSmount/#ubuntu-ami-instance_1","text":"Build and install amazon-efs-utils as a DEB package a. Open a terminal on your client and navigate to the directory that has the cloned amazon-efs-utils package from GitHub. b. Install the binutils package, a dependency for building DEB packages. $> sudo apt-get -y install binutils c. Build the package with the following command. $> ./build-deb.sh d. Install the package with the following command. $> sudo apt-get -y install ./build/amazon-efs-utils*deb Make a directory called efs to mount on it: $> sudo mkdir efs Go back to the EFS Attach window (in the EFS console ). Mount your file system as your new directory. $> sudo mount -t efs -o tls fs-38920a40:/ efs Verify your file system has been successfully mounted by running the following command: $> df -h Create a test file in your new file system by running a simple dd command to generate a 1GiB file in your new directory. Run the following dd command in your SSH window: $> sudo dd if=/dev/zero of=~/efs/1GiB bs=1M count=1024 status=progress","title":"Ubuntu AMI Instance"},{"location":"ELB/","text":"Load Balancing in AWS \u00b6 Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancing offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant. Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer 7), Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request. Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. Operating at the connection level (Layer 4), Network Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) and is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load Balancer is also optimized to handle sudden and volatile traffic patterns. Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network. For more information, see Comparison of Elastic Load Balancing products . ELB: Benefits and Use Cases \u00b6 ELB Benefits Highly available Secure Elastic Flexible Robust monitoring & auditing Hybrid load balancing ELB Use Cases Achieve better fault tolerance for your applications Automatically load balance your containerized applications Automatically scale your applications Using Elastic Load Balancing in your Amazon Virtual Private Cloud (Amazon VPC) Hybrid load balancing with Elastic Load Balancing Invoking Lambda functions over HTTP(S) See, ELB Features and Pricing before launching your first elastic load balancer on AWS. How Elastic Load Balancing works \u00b6 A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2 instances) in one or more Availability Zones. The load balancer also monitors the health of its registered targets and ensures that it routes traffic only to healthy targets. When the load balancer detects an unhealthy target, it stops routing traffic to that target. It then resumes routing traffic to that target when it detects that the target is healthy again. You configure your load balancer to accept incoming traffic by specifying one or more listeners. A listener is a process that checks for connection requests. It is configured with a protocol and port number for connections from clients to the load balancer. Likewise, it is configured with a protocol and port number for connections from the load balancer to the targets. Elastic Load Balancing supports three types of load balancers: Application Load Balancers Network Load Balancers Classic Load Balancers There is a key difference in how the load balancer types are configured. With Application Load Balancers and Network Load Balancers, you register targets in target groups, and route traffic to the target groups. With Classic Load Balancers, you register instances with the load balancer. Availability Zones and load balancer nodes \u00b6 When you enable an Availability Zone for your load balancer, Elastic Load Balancing creates a load balancer node in the Availability Zone. If you register targets in an Availability Zone but do not enable the Availability Zone, these registered targets do not receive traffic. Your load balancer is most effective when you ensure that each enabled Availability Zone has at least one registered target. We recommend that you enable multiple Availability Zones. (With an Application Load Balancer, we require you to enable multiple Availability Zones.) This configuration helps ensure that the load balancer can continue to route traffic. If one Availability Zone becomes unavailable or has no healthy targets, the load balancer can route traffic to the healthy targets in another Availability Zone. After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer. However, even though they remain registered, the load balancer does not route traffic to them. Cross-zone load balancing \u00b6 The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. The following diagrams demonstrate the effect of cross-zone load balancing. There are two enabled Availability Zones, with two targets in Availability Zone A and eight targets in Availability Zone B. Clients send requests, and Amazon Route 53 responds to each request with the IP address of one of the load balancer nodes. This distributes traffic such that each load balancer node receives 50% of the traffic from the clients. Each load balancer node distributes its share of the traffic across the registered targets in its scope. If cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets. If cross-zone load balancing is disabled: Each of the two targets in Availability Zone A receives 25% of the traffic. Each of the eight targets in Availability Zone B receives 6.25% of the traffic. This is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone. With Application Load Balancers, cross-zone load balancing is always enabled. With Network Load Balancers, cross-zone load balancing is disabled by default. After you create a Network Load Balancer, you can enable or disable cross-zone load balancing at any time. For more information, see Cross-zone load balancing in the User Guide for Network Load Balancers. When you create a Classic Load Balancer, the default for cross-zone load balancing depends on how you create the load balancer. With the API or CLI, cross-zone load balancing is disabled by default. With the AWS Management Console, the option to enable cross-zone load balancing is selected by default. After you create a Classic Load Balancer, you can enable or disable cross-zone load balancing at any time. For more information, see Enable cross-zone load balancing in the User Guide for Classic Load Balancers. Getting started with Elastic Load Balancing \u00b6 Let\u2019s get started by creating a load balancer with the Elastic Load Balancing wizard in the AWS Management Console, a point-and-click web-based interface. Create an account and sign into the console Create a load balancer by selecting Application Load Balancer or Network Load Balancer Specify a unique name and a network Create listeners for your load balancer (HTTP or HTTPS for Application Load Balancer and TCP or UDP for Network Load Balancer) Configure health checks for your load balancer Do one of the following: Manually register EC2 instances to your Target Group and register the Target Group with your load balancer Associate your load balancer with an Auto Scaling group that is registered to a Target Group For demos of common load balancer configurations, see Elastic Load Balancing demos . Create an Application Load Balancer \u00b6 AWS Management Console \u00b6 To create an Application Load Balancer using the AWS Management Console, see Getting started with Application Load Balancers in the User Guide for Application Load Balancers. You need to do the following: Before you begin \u00b6 Decide which two Availability Zones you will use for your EC2 instances. Configure your virtual private cloud (VPC) with at least one public subnet in each of these Availability Zones. These public subnets are used to configure the load balancer. You can launch your EC2 instances in other subnets of these Availability Zones instead. Launch at least one EC2 instance in each Availability Zone. Be sure to install a web server, such as Apache or Internet Information Services (IIS), on each EC2 instance. Ensure that the security groups for these instances allow HTTP access on port 80. Step 1: Select a load balancer type \u00b6 Step 2: Configure your load balancer and listener \u00b6 Step 3: Configure a security group for your load balancer \u00b6 Step 4: Configure your target group \u00b6 Step 5: Register targets with your target group \u00b6 Step 6: Create and test your load balancer \u00b6 Step 7: Delete your load balancer (optional) \u00b6 AWS CLI \u00b6 To create an Application Load Balancer using the AWS CLI, see Create an Application Load Balancer using the AWS CLI in the User Guide for Application Load Balancers. Create a Network Load Balancer \u00b6 AWS Management Console \u00b6 To create a Network Load Balancer using the AWS Management Console, see Getting started with Network Load Balancers in the User Guide for Network Load Balancers. You need to do the following: Before you begin \u00b6 Decide which Availability Zones you will use for your EC2 instances. Configure your virtual private cloud (VPC) with at least one public subnet in each of these Availability Zones. These public subnets are used to configure the load balancer. You can launch your EC2 instances in other subnets of these Availability Zones instead. Launch at least one EC2 instance in each Availability Zone. Ensure that the security groups for these instances allow TCP access from clients on the listener port and health check requests from your VPC. For more information, see Target security groups . Step 1: Choose a load balancer type \u00b6 Step 2: Configure your load balancer and listener \u00b6 Step 3: Configure your target group \u00b6 Step 4: Register targets with your target group \u00b6 Step 5: Create and test your load balancer \u00b6 Step 6: Delete your load balancer (optional) \u00b6 AWS CLI \u00b6 To create a Network Load Balancer using the AWS CLI, see Create a Network Load Balancer using the AWS CLI in the User Guide for Network Load Balancers. Create a Classic Load Balancer \u00b6 To create a Classic Load Balancer using the AWS Management Console, see Create a Classic Load Balancer in the User Guide for Classic Load Balancers. More resources \u00b6 Elastic Load Balancing Documentation Load Balancers Application Load Balancers Network Load Balancers Listeners Application Load Balancers Network Load Balancers Target Groups Application Load Balancers Network Load Balancers Best Practices in Evaluating Elastic Load Balancing","title":"AWS Elastic Load Balancers (ELB)"},{"location":"ELB/#load-balancing-in-aws","text":"Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancing offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant. Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer 7), Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request. Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. Operating at the connection level (Layer 4), Network Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) and is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load Balancer is also optimized to handle sudden and volatile traffic patterns. Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network. For more information, see Comparison of Elastic Load Balancing products .","title":"Load Balancing in AWS"},{"location":"ELB/#elb-benefits-and-use-cases","text":"ELB Benefits Highly available Secure Elastic Flexible Robust monitoring & auditing Hybrid load balancing ELB Use Cases Achieve better fault tolerance for your applications Automatically load balance your containerized applications Automatically scale your applications Using Elastic Load Balancing in your Amazon Virtual Private Cloud (Amazon VPC) Hybrid load balancing with Elastic Load Balancing Invoking Lambda functions over HTTP(S) See, ELB Features and Pricing before launching your first elastic load balancer on AWS.","title":"ELB: Benefits and Use Cases"},{"location":"ELB/#how-elastic-load-balancing-works","text":"A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2 instances) in one or more Availability Zones. The load balancer also monitors the health of its registered targets and ensures that it routes traffic only to healthy targets. When the load balancer detects an unhealthy target, it stops routing traffic to that target. It then resumes routing traffic to that target when it detects that the target is healthy again. You configure your load balancer to accept incoming traffic by specifying one or more listeners. A listener is a process that checks for connection requests. It is configured with a protocol and port number for connections from clients to the load balancer. Likewise, it is configured with a protocol and port number for connections from the load balancer to the targets. Elastic Load Balancing supports three types of load balancers: Application Load Balancers Network Load Balancers Classic Load Balancers There is a key difference in how the load balancer types are configured. With Application Load Balancers and Network Load Balancers, you register targets in target groups, and route traffic to the target groups. With Classic Load Balancers, you register instances with the load balancer.","title":"How Elastic Load Balancing works"},{"location":"ELB/#availability-zones-and-load-balancer-nodes","text":"When you enable an Availability Zone for your load balancer, Elastic Load Balancing creates a load balancer node in the Availability Zone. If you register targets in an Availability Zone but do not enable the Availability Zone, these registered targets do not receive traffic. Your load balancer is most effective when you ensure that each enabled Availability Zone has at least one registered target. We recommend that you enable multiple Availability Zones. (With an Application Load Balancer, we require you to enable multiple Availability Zones.) This configuration helps ensure that the load balancer can continue to route traffic. If one Availability Zone becomes unavailable or has no healthy targets, the load balancer can route traffic to the healthy targets in another Availability Zone. After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer. However, even though they remain registered, the load balancer does not route traffic to them.","title":"Availability Zones and load balancer nodes"},{"location":"ELB/#cross-zone-load-balancing","text":"The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. The following diagrams demonstrate the effect of cross-zone load balancing. There are two enabled Availability Zones, with two targets in Availability Zone A and eight targets in Availability Zone B. Clients send requests, and Amazon Route 53 responds to each request with the IP address of one of the load balancer nodes. This distributes traffic such that each load balancer node receives 50% of the traffic from the clients. Each load balancer node distributes its share of the traffic across the registered targets in its scope. If cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets. If cross-zone load balancing is disabled: Each of the two targets in Availability Zone A receives 25% of the traffic. Each of the eight targets in Availability Zone B receives 6.25% of the traffic. This is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone. With Application Load Balancers, cross-zone load balancing is always enabled. With Network Load Balancers, cross-zone load balancing is disabled by default. After you create a Network Load Balancer, you can enable or disable cross-zone load balancing at any time. For more information, see Cross-zone load balancing in the User Guide for Network Load Balancers. When you create a Classic Load Balancer, the default for cross-zone load balancing depends on how you create the load balancer. With the API or CLI, cross-zone load balancing is disabled by default. With the AWS Management Console, the option to enable cross-zone load balancing is selected by default. After you create a Classic Load Balancer, you can enable or disable cross-zone load balancing at any time. For more information, see Enable cross-zone load balancing in the User Guide for Classic Load Balancers.","title":"Cross-zone load balancing"},{"location":"ELB/#getting-started-with-elastic-load-balancing","text":"Let\u2019s get started by creating a load balancer with the Elastic Load Balancing wizard in the AWS Management Console, a point-and-click web-based interface. Create an account and sign into the console Create a load balancer by selecting Application Load Balancer or Network Load Balancer Specify a unique name and a network Create listeners for your load balancer (HTTP or HTTPS for Application Load Balancer and TCP or UDP for Network Load Balancer) Configure health checks for your load balancer Do one of the following: Manually register EC2 instances to your Target Group and register the Target Group with your load balancer Associate your load balancer with an Auto Scaling group that is registered to a Target Group For demos of common load balancer configurations, see Elastic Load Balancing demos .","title":"Getting started with Elastic Load Balancing"},{"location":"ELB/#create-an-application-load-balancer","text":"","title":"Create an Application Load Balancer"},{"location":"ELB/#aws-management-console","text":"To create an Application Load Balancer using the AWS Management Console, see Getting started with Application Load Balancers in the User Guide for Application Load Balancers. You need to do the following:","title":"AWS Management Console"},{"location":"ELB/#before-you-begin","text":"Decide which two Availability Zones you will use for your EC2 instances. Configure your virtual private cloud (VPC) with at least one public subnet in each of these Availability Zones. These public subnets are used to configure the load balancer. You can launch your EC2 instances in other subnets of these Availability Zones instead. Launch at least one EC2 instance in each Availability Zone. Be sure to install a web server, such as Apache or Internet Information Services (IIS), on each EC2 instance. Ensure that the security groups for these instances allow HTTP access on port 80.","title":"Before you begin"},{"location":"ELB/#step-1-select-a-load-balancer-type","text":"","title":"Step 1: Select a load balancer type"},{"location":"ELB/#step-2-configure-your-load-balancer-and-listener","text":"","title":"Step 2: Configure your load balancer and listener"},{"location":"ELB/#step-3-configure-a-security-group-for-your-load-balancer","text":"","title":"Step 3: Configure a security group for your load balancer"},{"location":"ELB/#step-4-configure-your-target-group","text":"","title":"Step 4: Configure your target group"},{"location":"ELB/#step-5-register-targets-with-your-target-group","text":"","title":"Step 5: Register targets with your target group"},{"location":"ELB/#step-6-create-and-test-your-load-balancer","text":"","title":"Step 6: Create and test your load balancer"},{"location":"ELB/#step-7-delete-your-load-balancer-optional","text":"","title":"Step 7: Delete your load balancer (optional)"},{"location":"ELB/#aws-cli","text":"To create an Application Load Balancer using the AWS CLI, see Create an Application Load Balancer using the AWS CLI in the User Guide for Application Load Balancers.","title":"AWS CLI"},{"location":"ELB/#create-a-network-load-balancer","text":"","title":"Create a Network Load Balancer"},{"location":"ELB/#aws-management-console_1","text":"To create a Network Load Balancer using the AWS Management Console, see Getting started with Network Load Balancers in the User Guide for Network Load Balancers. You need to do the following:","title":"AWS Management Console"},{"location":"ELB/#before-you-begin_1","text":"Decide which Availability Zones you will use for your EC2 instances. Configure your virtual private cloud (VPC) with at least one public subnet in each of these Availability Zones. These public subnets are used to configure the load balancer. You can launch your EC2 instances in other subnets of these Availability Zones instead. Launch at least one EC2 instance in each Availability Zone. Ensure that the security groups for these instances allow TCP access from clients on the listener port and health check requests from your VPC. For more information, see Target security groups .","title":"Before you begin"},{"location":"ELB/#step-1-choose-a-load-balancer-type","text":"","title":"Step 1: Choose a load balancer type"},{"location":"ELB/#step-2-configure-your-load-balancer-and-listener_1","text":"","title":"Step 2: Configure your load balancer and listener"},{"location":"ELB/#step-3-configure-your-target-group","text":"","title":"Step 3: Configure your target group"},{"location":"ELB/#step-4-register-targets-with-your-target-group","text":"","title":"Step 4: Register targets with your target group"},{"location":"ELB/#step-5-create-and-test-your-load-balancer","text":"","title":"Step 5: Create and test your load balancer"},{"location":"ELB/#step-6-delete-your-load-balancer-optional","text":"","title":"Step 6: Delete your load balancer (optional)"},{"location":"ELB/#aws-cli_1","text":"To create a Network Load Balancer using the AWS CLI, see Create a Network Load Balancer using the AWS CLI in the User Guide for Network Load Balancers.","title":"AWS CLI"},{"location":"ELB/#create-a-classic-load-balancer","text":"To create a Classic Load Balancer using the AWS Management Console, see Create a Classic Load Balancer in the User Guide for Classic Load Balancers.","title":"Create a Classic Load Balancer"},{"location":"ELB/#more-resources","text":"Elastic Load Balancing Documentation Load Balancers Application Load Balancers Network Load Balancers Listeners Application Load Balancers Network Load Balancers Target Groups Application Load Balancers Network Load Balancers Best Practices in Evaluating Elastic Load Balancing","title":"More resources"},{"location":"Elasticache/","text":"In-memory databases on AWS: Amazon ElastiCache \u00b6 Amazon ElastiCache is fully managed in-memory data store, compatible with Redis or Memcached. Power real-time applications with sub-millisecond latency. The in-memory database defined \u00b6 An in-memory database is a type of purpose-built database that relies primarily on memory for data storage, in contrast to databases that store data on disk or SSDs. In-memory databases are designed to attain minimal response time by eliminating the need to access disks. Because all data is stored and managed exclusively in main memory, it is at risk of being lost upon a process or server failure. In-memory databases can persist data on disks by storing each operation in a log or by taking snapshots. In-memory databases are ideal for applications that require microsecond response times and can have large spikes in traffic coming at any time such as gaming leaderboards, session stores, and real-time analytics. In-memory databases on AWS \u00b6 Amazon Elasticache for Redis \u00b6 Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides submillisecond latency to power internet-scale, real-time applications. Developers can use ElastiCache for Redis as an in-memory nonrelational database. The ElastiCache for Redis cluster configuration supports up to 15 shards and enables customers to run Redis workloads with up to 6.1 TB of in-memory capacity in a single cluster. ElastiCache for Redis also provides the ability to add and remove shards from a running cluster. You can dynamically scale out and even scale in your Redis cluster workloads to adapt to changes in demand. Get started with Amazon ElastiCache for Redis. \u00b6 Amazon ElastiCache for Memcached \u00b6 Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. It delivers the performance, ease-of-use, and simplicity of Memcached. ElastiCache for Memcached is fully managed, scalable, and secure - making it an ideal candidate for use cases where frequently accessed data must be in-memory. It is a popular choice for use cases such as Web, Mobile Apps, Gaming, Ad-Tech, and E-Commerce. Get started with Amazon ElastiCache for Memcached. \u00b6 Use cases \u00b6 Real-time bidding Gaming leaderboards Caching","title":"Amazon Elasticache"},{"location":"Elasticache/#in-memory-databases-on-aws-amazon-elasticache","text":"Amazon ElastiCache is fully managed in-memory data store, compatible with Redis or Memcached. Power real-time applications with sub-millisecond latency.","title":"In-memory databases on AWS: Amazon ElastiCache"},{"location":"Elasticache/#the-in-memory-database-defined","text":"An in-memory database is a type of purpose-built database that relies primarily on memory for data storage, in contrast to databases that store data on disk or SSDs. In-memory databases are designed to attain minimal response time by eliminating the need to access disks. Because all data is stored and managed exclusively in main memory, it is at risk of being lost upon a process or server failure. In-memory databases can persist data on disks by storing each operation in a log or by taking snapshots. In-memory databases are ideal for applications that require microsecond response times and can have large spikes in traffic coming at any time such as gaming leaderboards, session stores, and real-time analytics.","title":"The in-memory database defined"},{"location":"Elasticache/#in-memory-databases-on-aws","text":"","title":"In-memory databases on AWS"},{"location":"Elasticache/#amazon-elasticache-for-redis","text":"Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides submillisecond latency to power internet-scale, real-time applications. Developers can use ElastiCache for Redis as an in-memory nonrelational database. The ElastiCache for Redis cluster configuration supports up to 15 shards and enables customers to run Redis workloads with up to 6.1 TB of in-memory capacity in a single cluster. ElastiCache for Redis also provides the ability to add and remove shards from a running cluster. You can dynamically scale out and even scale in your Redis cluster workloads to adapt to changes in demand.","title":"Amazon Elasticache for Redis"},{"location":"Elasticache/#get-started-with-amazon-elasticache-for-redis","text":"","title":"Get started with Amazon ElastiCache for Redis."},{"location":"Elasticache/#amazon-elasticache-for-memcached","text":"Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. It delivers the performance, ease-of-use, and simplicity of Memcached. ElastiCache for Memcached is fully managed, scalable, and secure - making it an ideal candidate for use cases where frequently accessed data must be in-memory. It is a popular choice for use cases such as Web, Mobile Apps, Gaming, Ad-Tech, and E-Commerce.","title":"Amazon ElastiCache for Memcached"},{"location":"Elasticache/#get-started-with-amazon-elasticache-for-memcached","text":"","title":"Get started with Amazon ElastiCache for Memcached."},{"location":"Elasticache/#use-cases","text":"Real-time bidding Gaming leaderboards Caching","title":"Use cases"},{"location":"LoadBalanc/","text":"Amazon EC2 Elastic Load Balancing \u00b6 Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Load Balancers in AWS \u00b6 ELB offers three types of load balancers that all feature the high availability, automatic scaling, and robust security that are necessary to make your applications fault-tolerant. An Application Load Balancer operates at the request level (Layer 7), routing traffic to targets\u2013such as EC2 instances, microservices and containers\u2013within Amazon VPC, based on the content of the request. It\u2019s ideal for the advanced load balancing of Hypertext Transfer Protocol (HTTP) and Secure HTTP (HTTPS) traffic. A Network Load Balancer operates at the connection level (Layer 4), routing connections to targets\u2013such as Amazon EC2 instances, microservices, and containers\u2013within Amazon VPC, based on IP protocol data. It\u2019s ideal for load-balancing Transmission Control Protocol (TCP) traffic. The Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances, and it operates at both the request level and the connection level. Load Balancing: Manage your application workload by ELB \u00b6 Check, ELB: Benefits and Use Cases , and How Elastic Load Balancing works . Then, Getting started with Elastic Load Balancing","title":"Load Balancing on AWS"},{"location":"LoadBalanc/#amazon-ec2-elastic-load-balancing","text":"Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.","title":"Amazon EC2 Elastic Load Balancing"},{"location":"LoadBalanc/#load-balancers-in-aws","text":"ELB offers three types of load balancers that all feature the high availability, automatic scaling, and robust security that are necessary to make your applications fault-tolerant. An Application Load Balancer operates at the request level (Layer 7), routing traffic to targets\u2013such as EC2 instances, microservices and containers\u2013within Amazon VPC, based on the content of the request. It\u2019s ideal for the advanced load balancing of Hypertext Transfer Protocol (HTTP) and Secure HTTP (HTTPS) traffic. A Network Load Balancer operates at the connection level (Layer 4), routing connections to targets\u2013such as Amazon EC2 instances, microservices, and containers\u2013within Amazon VPC, based on IP protocol data. It\u2019s ideal for load-balancing Transmission Control Protocol (TCP) traffic. The Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances, and it operates at both the request level and the connection level.","title":"Load Balancers in AWS"},{"location":"LoadBalanc/#load-balancing-manage-your-application-workload-by-elb","text":"Check, ELB: Benefits and Use Cases , and How Elastic Load Balancing works . Then, Getting started with Elastic Load Balancing","title":"Load Balancing: Manage your application workload by ELB"},{"location":"Networkdemos/","text":"AWS VPC and Load Balancing Demonistrations \u00b6 VPC is one of the main components in the AWS network, it helps you create a private network that contains different subnets. These subnets are managed by the routing tables that attached to each subnet and to the default router inside your VPC. In these demos, we aim at changing the default subnet, default VPC, and the default route table. Three diffrent tobologies are implemeted on AWS with three cumulative topology, as follow: Demo1: Creating a new VPC with 2 Subnets one public and one private in one availability zone Demo2: Creating a new VPC with 4 Subnets in two availability zones with a load balancer Demo3: Creating a new VPC with 4 Subnets in two availability zones with a private connection to the On-premises datacenter Demo 1: Creating a new VPC with 2 Subnets in one availability zone \u00b6 In this demo, just creating a new VPC with one public subnet and one private subnet. Then adding one Internet gateway to allow the public trafic to the internet. Network Topology \u00b6 Step 1: create the VPC and the two supnets \u00b6 Follow, Creating the new VPC with two subnets , to create the VPC, by the following parameters: The VPC address block 10.10.0.0/16 One public subnet 10.10.1.0/24 One private subnet 10.10.2.0/24 Step 2: create the Internet Gateway for the public access \u00b6 Follow, create and configure an interent gateway , to craete the IGW, and do the following: Do not forget to deattach the default IGW. Attach the IGW to the VPC Create your route table, see how to work with route tables Add the 0.0.0.0/0 the default route to allow public access to your VPC Do the subnets association to this public route table Step 3: launch the EC2 instance \u00b6 Follow, creating an EC2 inside a subnet Change the default VPC and default subnet (Availability zone) keep the security group or configure yours by adding the HTTP rule Step 4: assign a public IP for your EC2 instance \u00b6 Use the Elastic IP addresses to create a new public IP and assighn it to your EC2 instance, see: Allocating an Elastic IP address Associating an Elastic IP address Test your new public IP for the instance by: http://your_instance_public_ip/ Step 5: create your private subnet in the same VPC \u00b6 Follow, Creating a subnet in your VPC and try to keep it: Private by not associate it to the public route table Create your database inside this subnet Watch it here \u00b6 Demo 2: Creating a new VPC with 4 Subnets in two availability zones with a load balancer \u00b6 In this demo, we continue on the configuration topology of the first demo, by adding two new subnets one public and one private but in a different availability zone and in the same VPC, then we attach an ELB between the two availability zones to balancing the webservers loads. Network Topology \u00b6 Step 1: continue in the same VPC in the previous demo \u00b6 Follow, Creating a subnet in your VPC , to create two new subnets, by the following parameters: One public subnet 10.10.3.0/24 One private subnet 10.10.4.0/24 Step 2: use the same Internet Gateway for the public access \u00b6 Associate the public subnet to the route table keep the private subnet away from this route table Step 3: launch the EC2 instance \u00b6 Follow, creating an EC2 inside a subnet Change the default VPC and default subnet (Availability zone) keep the security group or configure yours by adding the HTTP rule Step 4: assign a public IP for your EC2 instance \u00b6 Use the Elastic IP addresses to create a new public IP and assighn it to your EC2 instance, see: Allocating an Elastic IP address Associating an Elastic IP address Test your new public IP for the instance by: http://your_instance_public_ip/ you can ignore this step by allowing the Auto-assign Public IP in the Step 3: Configure Instance Details while you are launching the EC2 instance Step 5: create your private subnet in the same VPC \u00b6 Follow, Creating a subnet in your VPC and try to keep it: Private by not associate it to the public route table Create your database inside this subnet Step 6: create the Elastic Load Balancer (ELB) \u00b6 Follow, Create an Application Load Balancer , then: Associate the ELB with the 2 EC2 instances Choose your configured VPC Select two availability zones Select the two public subnets Keep the default security configuration Choose a security groupe In routing targets register the targets, the two instances Review and create, then give it some time Check your ELB after getting provisioned , use the public DNS name of the ELB to access your servers Watch it here \u00b6 Demo 3: Creating a new VPC with 4 Subnets with a private connection to the On-premises datacenter \u00b6 The aim in this demo to connect our private subnets to your on-premises data center through a private gateway. To complete this you need to use the direct connect and VPN AWS services. Network Topology \u00b6 Step 1: continue in the same VPC in the previous demo \u00b6 keep the previous configuration one VPC 2 public subnets 2 private subnets one ELB Listner: two public subnets Traget: the two instances The Multi-AZ databases are hosted in the two private subnets Step 2: create the private gateway \u00b6 Follow, Creating a virtual private gateway , then: Attach it your VPC Step 3: create the private route table \u00b6 Create your route table, see how to work with route tables Add the 172.16.0.0/16 the route to your on-premises datacenter Do the private subnets association to this private route table Step 4: Establish the private connection \u00b6 Use Direct Connect to launch the private connection to your on-premises datacenter Step 5: Secure the private connection \u00b6 Use AWS VPN to secure the private connection to your on-premises datacenter Watch it here \u00b6","title":"AWS Networking Demonstrations"},{"location":"Networkdemos/#aws-vpc-and-load-balancing-demonistrations","text":"VPC is one of the main components in the AWS network, it helps you create a private network that contains different subnets. These subnets are managed by the routing tables that attached to each subnet and to the default router inside your VPC. In these demos, we aim at changing the default subnet, default VPC, and the default route table. Three diffrent tobologies are implemeted on AWS with three cumulative topology, as follow: Demo1: Creating a new VPC with 2 Subnets one public and one private in one availability zone Demo2: Creating a new VPC with 4 Subnets in two availability zones with a load balancer Demo3: Creating a new VPC with 4 Subnets in two availability zones with a private connection to the On-premises datacenter","title":"AWS VPC and Load Balancing Demonistrations"},{"location":"Networkdemos/#demo-1-creating-a-new-vpc-with-2-subnets-in-one-availability-zone","text":"In this demo, just creating a new VPC with one public subnet and one private subnet. Then adding one Internet gateway to allow the public trafic to the internet.","title":"Demo 1: Creating a new VPC with 2 Subnets in one availability zone"},{"location":"Networkdemos/#network-topology","text":"","title":"Network Topology"},{"location":"Networkdemos/#step-1-create-the-vpc-and-the-two-supnets","text":"Follow, Creating the new VPC with two subnets , to create the VPC, by the following parameters: The VPC address block 10.10.0.0/16 One public subnet 10.10.1.0/24 One private subnet 10.10.2.0/24","title":"Step 1: create the VPC and the two supnets"},{"location":"Networkdemos/#step-2-create-the-internet-gateway-for-the-public-access","text":"Follow, create and configure an interent gateway , to craete the IGW, and do the following: Do not forget to deattach the default IGW. Attach the IGW to the VPC Create your route table, see how to work with route tables Add the 0.0.0.0/0 the default route to allow public access to your VPC Do the subnets association to this public route table","title":"Step 2: create the Internet Gateway for the public access"},{"location":"Networkdemos/#step-3-launch-the-ec2-instance","text":"Follow, creating an EC2 inside a subnet Change the default VPC and default subnet (Availability zone) keep the security group or configure yours by adding the HTTP rule","title":"Step 3: launch the EC2 instance"},{"location":"Networkdemos/#step-4-assign-a-public-ip-for-your-ec2-instance","text":"Use the Elastic IP addresses to create a new public IP and assighn it to your EC2 instance, see: Allocating an Elastic IP address Associating an Elastic IP address Test your new public IP for the instance by: http://your_instance_public_ip/","title":"Step 4: assign a public IP for your EC2 instance"},{"location":"Networkdemos/#step-5-create-your-private-subnet-in-the-same-vpc","text":"Follow, Creating a subnet in your VPC and try to keep it: Private by not associate it to the public route table Create your database inside this subnet","title":"Step 5: create your private subnet in the same VPC"},{"location":"Networkdemos/#watch-it-here","text":"","title":"Watch it here"},{"location":"Networkdemos/#demo-2-creating-a-new-vpc-with-4-subnets-in-two-availability-zones-with-a-load-balancer","text":"In this demo, we continue on the configuration topology of the first demo, by adding two new subnets one public and one private but in a different availability zone and in the same VPC, then we attach an ELB between the two availability zones to balancing the webservers loads.","title":"Demo 2: Creating a new VPC with 4 Subnets in two availability zones with a load balancer"},{"location":"Networkdemos/#network-topology_1","text":"","title":"Network Topology"},{"location":"Networkdemos/#step-1-continue-in-the-same-vpc-in-the-previous-demo","text":"Follow, Creating a subnet in your VPC , to create two new subnets, by the following parameters: One public subnet 10.10.3.0/24 One private subnet 10.10.4.0/24","title":"Step 1: continue in the same VPC in the previous demo"},{"location":"Networkdemos/#step-2-use-the-same-internet-gateway-for-the-public-access","text":"Associate the public subnet to the route table keep the private subnet away from this route table","title":"Step 2: use the same Internet Gateway for the public access"},{"location":"Networkdemos/#step-3-launch-the-ec2-instance_1","text":"Follow, creating an EC2 inside a subnet Change the default VPC and default subnet (Availability zone) keep the security group or configure yours by adding the HTTP rule","title":"Step 3: launch the EC2 instance"},{"location":"Networkdemos/#step-4-assign-a-public-ip-for-your-ec2-instance_1","text":"Use the Elastic IP addresses to create a new public IP and assighn it to your EC2 instance, see: Allocating an Elastic IP address Associating an Elastic IP address Test your new public IP for the instance by: http://your_instance_public_ip/ you can ignore this step by allowing the Auto-assign Public IP in the Step 3: Configure Instance Details while you are launching the EC2 instance","title":"Step 4: assign a public IP for your EC2 instance"},{"location":"Networkdemos/#step-5-create-your-private-subnet-in-the-same-vpc_1","text":"Follow, Creating a subnet in your VPC and try to keep it: Private by not associate it to the public route table Create your database inside this subnet","title":"Step 5: create your private subnet in the same VPC"},{"location":"Networkdemos/#step-6-create-the-elastic-load-balancer-elb","text":"Follow, Create an Application Load Balancer , then: Associate the ELB with the 2 EC2 instances Choose your configured VPC Select two availability zones Select the two public subnets Keep the default security configuration Choose a security groupe In routing targets register the targets, the two instances Review and create, then give it some time Check your ELB after getting provisioned , use the public DNS name of the ELB to access your servers","title":"Step 6: create the Elastic Load Balancer (ELB)"},{"location":"Networkdemos/#watch-it-here_1","text":"","title":"Watch it here"},{"location":"Networkdemos/#demo-3-creating-a-new-vpc-with-4-subnets-with-a-private-connection-to-the-on-premises-datacenter","text":"The aim in this demo to connect our private subnets to your on-premises data center through a private gateway. To complete this you need to use the direct connect and VPN AWS services.","title":"Demo 3: Creating a new VPC with 4 Subnets with a private connection to the On-premises datacenter"},{"location":"Networkdemos/#network-topology_2","text":"","title":"Network Topology"},{"location":"Networkdemos/#step-1-continue-in-the-same-vpc-in-the-previous-demo_1","text":"keep the previous configuration one VPC 2 public subnets 2 private subnets one ELB Listner: two public subnets Traget: the two instances The Multi-AZ databases are hosted in the two private subnets","title":"Step 1: continue in the same VPC in the previous demo"},{"location":"Networkdemos/#step-2-create-the-private-gateway","text":"Follow, Creating a virtual private gateway , then: Attach it your VPC","title":"Step 2: create the private gateway"},{"location":"Networkdemos/#step-3-create-the-private-route-table","text":"Create your route table, see how to work with route tables Add the 172.16.0.0/16 the route to your on-premises datacenter Do the private subnets association to this private route table","title":"Step 3: create the private route table"},{"location":"Networkdemos/#step-4-establish-the-private-connection","text":"Use Direct Connect to launch the private connection to your on-premises datacenter","title":"Step 4: Establish the private connection"},{"location":"Networkdemos/#step-5-secure-the-private-connection","text":"Use AWS VPN to secure the private connection to your on-premises datacenter","title":"Step 5: Secure the private connection"},{"location":"Networkdemos/#watch-it-here_2","text":"","title":"Watch it here"},{"location":"RDS/","text":"Databases in AWS \u00b6 Now, in this section,we\u2019re going to discuss how to run relational and NoSQL databases on AWS. We will first look at simply running your own database on top of EC2, much like you would on-premise. Then, we will introduce you to the Amazon Relational Database Service, or RDS. RDS is a managed database service where you can pick amongst popular database engines and let AWS manage your database for you. Next, we will demonstrate Amazon DynamoDB. DynamoDB is a managed NoSQL database that, depending upon your application, may be a better solution than a traditional SQL-based database. Amazon Relational Database (RDS) \u00b6 Amazon Relational Database Service (Amazon RDS) makes it straightforward to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as provisioning hardware, setting up the database, patching, and making backups. Amazon RDS currently supports six database engines: Amazon Aurora: https://aws.amazon.com/rds/aurora/ PostgreSQL: https://aws.amazon.com/rds/postgresql/ MySQL: https://aws.amazon.com/rds/mysql/ MariaDB: https://aws.amazon.com/rds/mariadb/ Oracle: https://aws.amazon.com/rds/oracle/ Microsoft SQL Server: https://aws.amazon.com/rds/sqlserver/ Overview of Amazon RDS \u00b6 Why do you want a managed relational database service? Because Amazon RDS takes over many of the difficult or tedious management tasks of a relational database: When you buy a server, you get CPU, memory, storage, and IOPS, all bundled together. With Amazon RDS, these are split apart so that you can scale them independently. If you need more CPU, less IOPS, or more storage, you can easily allocate them. Amazon RDS manages backups, software patching, automatic failure detection, and recovery. To deliver a managed service experience, Amazon RDS doesn\u2019t provide shell access to DB instances. It also restricts access to certain system procedures and tables that require advanced privileges. You can have automated backups performed when you need them, or manually create your own backup snapshot. You can use these backups to restore a database. The Amazon RDS restore process works reliably and efficiently. You can use the database products you are already familiar with: MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server. You can get high availability with a primary instance and a synchronous secondary instance that you can fail over to when problems occur. You can also use MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL read replicas to increase read scaling. In addition to the security in your database package, you can help control who can access your RDS databases by using AWS Identity and Access Management (IAM) to define users and permissions. You can also help protect your databases by putting them in a virtual private cloud. Bring your own database (BYODB) vs Amazon RDS \u00b6 Why you should migrate your relational database to Amazon? Databases are one of the most important parts of any application. So, how were databases before AWS? As we can see on the following chart, there\u2019s a whole lot of different functions that exist when running your own database. So if you were in an on-premises world, ypu would be responsible for everything from power and heating and cooling, racking and stacking the servers, maintaining the servers, so on, all the way up to the application itself. One of the great things about AWS is by simply moving your database onto EC2, a large section of what you would have to do on-premises, you don\u2019t have to worry about in AWS. You don\u2019t have to worry about provisioning the hardware as far as racking and stacking it, maintaining the server. Even installing the operating system is taken care of by simply running your system on EC2. What if there was a way to make it even simpler? Introducing the Amazon Relational Database Service, or RDS. At RDS, you still run a relational database of the flavor of your choosing, whether you\u2019re talking about a MySQL database, an Oracle database, a PostgreSQL database, or other flavors. But the difference is instead of taking care of the patching and the installs and installing the database itself, AWS takes care of all of the rest of the heavy lifting that otherwise your DBA would be in charge of. So no longer does you have to worry about managing the high availability, or managing backups, or taking care of scaling. But all of these pieces now get taken care of because of the Relational Database Service. And the only thing that your DBA needs to worry about is taking care of the application itself se the following chart. So you still are in charge of getting the right schema, getting charge of the data that\u2019s in there, and she is 100% responsible for that. Whether it\u2019s encrypted, who has access to it, AWS has no access to the data .But all of the rest of the undifferentiated heavy lifting that goes into databases, AWS takes care of. Is RDS the perfect solution for everyone? You certainly may be in a case where you\u2019re using a legacy database. Say you\u2019re still running a Sybase shop, you\u2019ll still run that on EC2. But as you look for ways to eliminate tasks from your team and let them focus on what they really prepare to do, RDS can certainly get rid of large portions that they don\u2019t have to worry about anymore. Amazon RDS DB instances \u00b6 A DB instance is an isolated database environment running in the cloud. It is the basic building block of Amazon RDS. A DB instance can contain multiple user-created databases, and can be accessed using the same client tools and applications you might use to access a standalone database instance. DB instances are simple to create and modify with the Amazon AWS command line tools, Amazon RDS API operations, or the AWS Management Console. /!\\ Note - Amazon RDS supports access to databases using any standard SQL client application. - Amazon RDS does not allow direct host access. You can have up to 40 Amazon RDS DB instances, with the following limitations: 10 for each SQL Server edition (Enterprise, Standard, Web, and Express) under the \u201clicense-included\u201d model 10 for Oracle under the \u201clicense-included\u201d model 40 for MySQL, MariaDB, or PostgreSQL 40 for Oracle under the \u201cbring-your-own-license\u201d (BYOL) licensing model Read more about DB instances, see RDS DB instances . DB instance classes \u00b6 The DB instance class determines the computation and memory capacity of an Amazon RDS DB instance. The DB instance class you need depends on your processing power and memory requirements. Check the following: DB instance class types Supported DB engines for DB instance classes Changing your DB instance class Configuring the processor for a DB instance class Hardware specifications for DB instance classes DB instance storage \u00b6 DB instances for Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server use Amazon Elastic Block Store (Amazon EBS) volumes for database and log storage. Depending on the amount of storage requested, Amazon RDS automatically stripes across multiple Amazon EBS volumes to enhance performance. Amazon RDS storage types Amazon RDS provides three storage types: - General Purpose SSD (also known as gp2), - Provisioned IOPS SSD (also known as io1), and - magnetic (also known as standard). They differ in performance characteristics and price, which means that you can tailor your storage performance and cost to the needs of your database workload. You can create MySQL, MariaDB, Oracle, and PostgreSQL RDS DB instances with up to 64 tebibytes (TiB) of storage. You can create SQL Server RDS DB instances with up to 16 TiB of storage. For this amount of storage, use the Provisioned IOPS SSD and General Purpose SSD storage types. High availability (Multi-AZ) for Amazon RDS \u00b6 Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon\u2019s failover technology. SQL Server DB instances use SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs). In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption. For more information on Availability Zones, see Regions, Availability Zones, and Local Zones . Using the RDS console, you can create a Multi-AZ deployment by simply specifying Multi-AZ when creating a DB instance. You can use the console to convert existing DB instances to Multi-AZ deployments by modifying the DB instance and specifying the Multi-AZ option. Check the pricing and billing (per hour) of the RDS in AWS: DB instance billing for Amazon RDS Amazon RDS for MySQL Pricing Amazon RDS for PostgreSQL Pricing Amazon RDS Read Replicas \u00b6 Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines\u2019 native asynchronous replication to update the read replica whenever there is a change to the source DB instance. The read replica operates as a DB instance that allows only read-only connections; applications can connect to a read replica just as they would to any DB instance. Amazon RDS replicates all databases in the source DB instance. Amazon Aurora futher extends the benefits of read replicas by employing an SSD-backed virtualized storage layer purpose-built for database workloads. Amazon Aurora replicas share the same underlying storage as the source instance, lowering costs and avoiding the need to copy data to the replica nodes. For more information about replication with Amazon Aurora, see the online documentation . Benefits \u00b6 Enhanced performance Increased availability Designed for security Setup \u00b6 Using the AWS Management Console, you can easily add read replicas to existing DB Instances. Use the \u201cCreate Read Replica\u201d option corresponding to your DB Instance in the AWS Management Console. Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server allow you to add up to 5 read replicas to each DB Instance. Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle offer you two SSD-based choices for database storage: General Purpose and Provisioned IOPS. Read replicas for these engines need not use the same type of storage as their master DB Instances. You may be able to optimize your performance or your spending by selecting an alternate storage type for read replicas. For more information see read replicas documentation for Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora. Databases engines in AWS RDS \u00b6 In this section, you can review information specific, details, features, and configurations for each of the particular DB engines that is supported in AWS RDS, see the following links: MariaDB on Amazon RDS Microsoft SQL Server on Amazon RDS MySQL on Amazon RDS Oracle on Amazon RDS PostgreSQL on Amazon RDS Migrate your Database to AWS Database \u00b6 You can use the AWS Database Migration Service (AWS DMS) ( https://aws.amazon.com/dms ) to quickly and securely migrate your databases to AWS.","title":"RDS"},{"location":"RDS/#databases-in-aws","text":"Now, in this section,we\u2019re going to discuss how to run relational and NoSQL databases on AWS. We will first look at simply running your own database on top of EC2, much like you would on-premise. Then, we will introduce you to the Amazon Relational Database Service, or RDS. RDS is a managed database service where you can pick amongst popular database engines and let AWS manage your database for you. Next, we will demonstrate Amazon DynamoDB. DynamoDB is a managed NoSQL database that, depending upon your application, may be a better solution than a traditional SQL-based database.","title":"Databases in AWS"},{"location":"RDS/#amazon-relational-database-rds","text":"Amazon Relational Database Service (Amazon RDS) makes it straightforward to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as provisioning hardware, setting up the database, patching, and making backups. Amazon RDS currently supports six database engines: Amazon Aurora: https://aws.amazon.com/rds/aurora/ PostgreSQL: https://aws.amazon.com/rds/postgresql/ MySQL: https://aws.amazon.com/rds/mysql/ MariaDB: https://aws.amazon.com/rds/mariadb/ Oracle: https://aws.amazon.com/rds/oracle/ Microsoft SQL Server: https://aws.amazon.com/rds/sqlserver/","title":"Amazon Relational Database (RDS)"},{"location":"RDS/#overview-of-amazon-rds","text":"Why do you want a managed relational database service? Because Amazon RDS takes over many of the difficult or tedious management tasks of a relational database: When you buy a server, you get CPU, memory, storage, and IOPS, all bundled together. With Amazon RDS, these are split apart so that you can scale them independently. If you need more CPU, less IOPS, or more storage, you can easily allocate them. Amazon RDS manages backups, software patching, automatic failure detection, and recovery. To deliver a managed service experience, Amazon RDS doesn\u2019t provide shell access to DB instances. It also restricts access to certain system procedures and tables that require advanced privileges. You can have automated backups performed when you need them, or manually create your own backup snapshot. You can use these backups to restore a database. The Amazon RDS restore process works reliably and efficiently. You can use the database products you are already familiar with: MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server. You can get high availability with a primary instance and a synchronous secondary instance that you can fail over to when problems occur. You can also use MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL read replicas to increase read scaling. In addition to the security in your database package, you can help control who can access your RDS databases by using AWS Identity and Access Management (IAM) to define users and permissions. You can also help protect your databases by putting them in a virtual private cloud.","title":"Overview of Amazon RDS"},{"location":"RDS/#bring-your-own-database-byodb-vs-amazon-rds","text":"Why you should migrate your relational database to Amazon? Databases are one of the most important parts of any application. So, how were databases before AWS? As we can see on the following chart, there\u2019s a whole lot of different functions that exist when running your own database. So if you were in an on-premises world, ypu would be responsible for everything from power and heating and cooling, racking and stacking the servers, maintaining the servers, so on, all the way up to the application itself. One of the great things about AWS is by simply moving your database onto EC2, a large section of what you would have to do on-premises, you don\u2019t have to worry about in AWS. You don\u2019t have to worry about provisioning the hardware as far as racking and stacking it, maintaining the server. Even installing the operating system is taken care of by simply running your system on EC2. What if there was a way to make it even simpler? Introducing the Amazon Relational Database Service, or RDS. At RDS, you still run a relational database of the flavor of your choosing, whether you\u2019re talking about a MySQL database, an Oracle database, a PostgreSQL database, or other flavors. But the difference is instead of taking care of the patching and the installs and installing the database itself, AWS takes care of all of the rest of the heavy lifting that otherwise your DBA would be in charge of. So no longer does you have to worry about managing the high availability, or managing backups, or taking care of scaling. But all of these pieces now get taken care of because of the Relational Database Service. And the only thing that your DBA needs to worry about is taking care of the application itself se the following chart. So you still are in charge of getting the right schema, getting charge of the data that\u2019s in there, and she is 100% responsible for that. Whether it\u2019s encrypted, who has access to it, AWS has no access to the data .But all of the rest of the undifferentiated heavy lifting that goes into databases, AWS takes care of. Is RDS the perfect solution for everyone? You certainly may be in a case where you\u2019re using a legacy database. Say you\u2019re still running a Sybase shop, you\u2019ll still run that on EC2. But as you look for ways to eliminate tasks from your team and let them focus on what they really prepare to do, RDS can certainly get rid of large portions that they don\u2019t have to worry about anymore.","title":"Bring your own database (BYODB) vs Amazon RDS"},{"location":"RDS/#amazon-rds-db-instances","text":"A DB instance is an isolated database environment running in the cloud. It is the basic building block of Amazon RDS. A DB instance can contain multiple user-created databases, and can be accessed using the same client tools and applications you might use to access a standalone database instance. DB instances are simple to create and modify with the Amazon AWS command line tools, Amazon RDS API operations, or the AWS Management Console. /!\\ Note - Amazon RDS supports access to databases using any standard SQL client application. - Amazon RDS does not allow direct host access. You can have up to 40 Amazon RDS DB instances, with the following limitations: 10 for each SQL Server edition (Enterprise, Standard, Web, and Express) under the \u201clicense-included\u201d model 10 for Oracle under the \u201clicense-included\u201d model 40 for MySQL, MariaDB, or PostgreSQL 40 for Oracle under the \u201cbring-your-own-license\u201d (BYOL) licensing model Read more about DB instances, see RDS DB instances .","title":"Amazon RDS DB instances"},{"location":"RDS/#db-instance-classes","text":"The DB instance class determines the computation and memory capacity of an Amazon RDS DB instance. The DB instance class you need depends on your processing power and memory requirements. Check the following: DB instance class types Supported DB engines for DB instance classes Changing your DB instance class Configuring the processor for a DB instance class Hardware specifications for DB instance classes","title":"DB instance classes"},{"location":"RDS/#db-instance-storage","text":"DB instances for Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server use Amazon Elastic Block Store (Amazon EBS) volumes for database and log storage. Depending on the amount of storage requested, Amazon RDS automatically stripes across multiple Amazon EBS volumes to enhance performance. Amazon RDS storage types Amazon RDS provides three storage types: - General Purpose SSD (also known as gp2), - Provisioned IOPS SSD (also known as io1), and - magnetic (also known as standard). They differ in performance characteristics and price, which means that you can tailor your storage performance and cost to the needs of your database workload. You can create MySQL, MariaDB, Oracle, and PostgreSQL RDS DB instances with up to 64 tebibytes (TiB) of storage. You can create SQL Server RDS DB instances with up to 16 TiB of storage. For this amount of storage, use the Provisioned IOPS SSD and General Purpose SSD storage types.","title":"DB instance storage"},{"location":"RDS/#high-availability-multi-az-for-amazon-rds","text":"Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon\u2019s failover technology. SQL Server DB instances use SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs). In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption. For more information on Availability Zones, see Regions, Availability Zones, and Local Zones . Using the RDS console, you can create a Multi-AZ deployment by simply specifying Multi-AZ when creating a DB instance. You can use the console to convert existing DB instances to Multi-AZ deployments by modifying the DB instance and specifying the Multi-AZ option. Check the pricing and billing (per hour) of the RDS in AWS: DB instance billing for Amazon RDS Amazon RDS for MySQL Pricing Amazon RDS for PostgreSQL Pricing","title":"High availability (Multi-AZ) for Amazon RDS"},{"location":"RDS/#amazon-rds-read-replicas","text":"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines\u2019 native asynchronous replication to update the read replica whenever there is a change to the source DB instance. The read replica operates as a DB instance that allows only read-only connections; applications can connect to a read replica just as they would to any DB instance. Amazon RDS replicates all databases in the source DB instance. Amazon Aurora futher extends the benefits of read replicas by employing an SSD-backed virtualized storage layer purpose-built for database workloads. Amazon Aurora replicas share the same underlying storage as the source instance, lowering costs and avoiding the need to copy data to the replica nodes. For more information about replication with Amazon Aurora, see the online documentation .","title":"Amazon RDS Read Replicas"},{"location":"RDS/#benefits","text":"Enhanced performance Increased availability Designed for security","title":"Benefits"},{"location":"RDS/#setup","text":"Using the AWS Management Console, you can easily add read replicas to existing DB Instances. Use the \u201cCreate Read Replica\u201d option corresponding to your DB Instance in the AWS Management Console. Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server allow you to add up to 5 read replicas to each DB Instance. Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle offer you two SSD-based choices for database storage: General Purpose and Provisioned IOPS. Read replicas for these engines need not use the same type of storage as their master DB Instances. You may be able to optimize your performance or your spending by selecting an alternate storage type for read replicas. For more information see read replicas documentation for Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora.","title":"Setup"},{"location":"RDS/#databases-engines-in-aws-rds","text":"In this section, you can review information specific, details, features, and configurations for each of the particular DB engines that is supported in AWS RDS, see the following links: MariaDB on Amazon RDS Microsoft SQL Server on Amazon RDS MySQL on Amazon RDS Oracle on Amazon RDS PostgreSQL on Amazon RDS","title":"Databases engines in AWS RDS"},{"location":"RDS/#migrate-your-database-to-aws-database","text":"You can use the AWS Database Migration Service (AWS DMS) ( https://aws.amazon.com/dms ) to quickly and securely migrate your databases to AWS.","title":"Migrate your Database to AWS Database"},{"location":"RDSdemos/","text":"Getting started with Amazon RDS \u00b6 Setting up for Amazon RDS \u00b6 General configurations you need to do before starting the RDS databases, you should have an AWS account and you already have access to the AWS console, then chack the following: Create an IAM user Determine requirements Provide access to your DB instance in your VPC by creating a security group Managing Databases in AWS \u00b6 Before craeting a DB in AWS RDS, you need to decide on: AWS Region Availability Zone (Subnet), local Zone, or Multi AZ DB instance class or type that will run the DB instance Then you can start creating, configuring, backing up, restoring, and managing a database instance. Configuring an Amazon RDS DB instance \u00b6 This section shows how to set up your Amazon RDS DB instance. Before creating a DB instance, decide on the DB instance class that will run the DB instance. Also, decide where the DB instance will run by choosing an AWS Region. Next, create the DB instance. Creating an Amazon RDS DB instance Connecting to an Amazon RDS DB instance Working with option groups Working with DB parameter groups Managing an Amazon RDS DB instance \u00b6 Following, you can find instructions for managing and maintaining your Amazon RDS DB instance. Stopping an Amazon RDS DB instance temporarily Starting an Amazon RDS DB instance that was previously stopped Modifying an Amazon RDS DB instance Maintaining a DB instance Upgrading a DB instance engine version Renaming a DB instance Rebooting a DB instance Working with read replicas Tagging Amazon RDS resources Working with Amazon Resource Names (ARNs) in Amazon RDS Working with storage for Amazon RDS DB instances Deleting a DB instance Backing up and restoring an Amazon RDS DB instance \u00b6 This section shows how to back up and restore a DB instance. Working with backups Creating a DB snapshot Restoring from a DB snapshot Copying a snapshot Sharing a DB snapshot Exporting DB snapshot data to Amazon S3 Restoring a DB instance to a specified time Deleting a snapshot Tutorial: Restore a DB instance from a DB snapshot Monitoring an Amazon RDS DB instance \u00b6 This section shows you how to monitor Amazon RDS. Overview of monitoring Amazon RDS DB instance status Using Amazon RDS recommendations Enhanced Monitoring Using Amazon RDS Performance Insights Using Amazon RDS event notification Viewing Amazon RDS events Getting CloudWatch Events and Amazon EventBridge events for Amazon RDS Amazon RDS database log files Start Working with AWS RDS and DB engines \u00b6 In this Section, you can strat trying the RDS in AWS with many DB engines, by trying the following: Demos Tutorials Best Practices Demos \u00b6 In the following examples, you can find how to create and connect to a DB instance using Amazon Relational Database Service (Amazon RDS). You can create a DB instance that uses MariaDB, MySQL, Microsoft SQL Server, Oracle, or PostgreSQL. Creating a DB instance and connecting to a database on a DB instance is slightly different for each of the DB engines. Choose one of the following DB engines that you want to use for detailed information on creating and connecting to the DB instance. After you have created and connected to your DB instance, there are instructions to help you delete the DB instance. Creating a MariaDB DB instance and connecting to a database on a MariaDB DB instance Creating a Microsoft SQL Server DB instance and connecting to a DB instance Creating a MySQL DB instance and connecting to a database on a MySQL DB instance Create and Connect to a MySQL Database How to Install MySQL on Ubuntu 18.04 How to Manage & Create MySQL Users, Databases & Tables Creating an Oracle DB instance and connecting to a database on an Oracle DB instance Creating a PostgreSQL DB instance and connecting to a database on a PostgreSQL DB instance Create and Connect to a PostgreSQL Database How To Install and Use PostgreSQL on Ubuntu 18.04 The PostgreSQL Describe Table Statement How to fix \u201cERROR: column c.relhasoids does not exist\u201d in Postgres? Tutorial: Create a web server and an Amazon RDS DB instance Troubleshooting for Amazon RDS \u00b6 Troubleshooting for Amazon RDS Tutorials \u00b6 The following tutorials show you how to perform common tasks that use Amazon RDS: Tutorial: Create an Amazon VPC for use with a DB instance Tutorial: Create a web server and an Amazon RDS DB instance Tutorial: Restore a DB instance from a DB snapshot Best practices for Amazon RDS \u00b6 Learn best practices for working with Amazon RDS. As new best practices are identified, we will keep this section up to date. Amazon RDS basic operational guidelines DB instance RAM recommendations Using Enhanced Monitoring to identify operating system issues Using metrics to identify performance issues Best practices for working with MySQL storage engines Best practices for working with MariaDB storage engines Best practices for working with Oracle Best practices for working with PostgreSQL Best practices for working with SQL Server Working with DB parameter groups Amazon RDS new features and best practices presentation video","title":"Database Engines in AWS"},{"location":"RDSdemos/#getting-started-with-amazon-rds","text":"","title":"Getting started with Amazon RDS"},{"location":"RDSdemos/#setting-up-for-amazon-rds","text":"General configurations you need to do before starting the RDS databases, you should have an AWS account and you already have access to the AWS console, then chack the following: Create an IAM user Determine requirements Provide access to your DB instance in your VPC by creating a security group","title":"Setting up for Amazon RDS"},{"location":"RDSdemos/#managing-databases-in-aws","text":"Before craeting a DB in AWS RDS, you need to decide on: AWS Region Availability Zone (Subnet), local Zone, or Multi AZ DB instance class or type that will run the DB instance Then you can start creating, configuring, backing up, restoring, and managing a database instance.","title":"Managing Databases in AWS"},{"location":"RDSdemos/#configuring-an-amazon-rds-db-instance","text":"This section shows how to set up your Amazon RDS DB instance. Before creating a DB instance, decide on the DB instance class that will run the DB instance. Also, decide where the DB instance will run by choosing an AWS Region. Next, create the DB instance. Creating an Amazon RDS DB instance Connecting to an Amazon RDS DB instance Working with option groups Working with DB parameter groups","title":"Configuring an Amazon RDS DB instance"},{"location":"RDSdemos/#managing-an-amazon-rds-db-instance","text":"Following, you can find instructions for managing and maintaining your Amazon RDS DB instance. Stopping an Amazon RDS DB instance temporarily Starting an Amazon RDS DB instance that was previously stopped Modifying an Amazon RDS DB instance Maintaining a DB instance Upgrading a DB instance engine version Renaming a DB instance Rebooting a DB instance Working with read replicas Tagging Amazon RDS resources Working with Amazon Resource Names (ARNs) in Amazon RDS Working with storage for Amazon RDS DB instances Deleting a DB instance","title":"Managing an Amazon RDS DB instance"},{"location":"RDSdemos/#backing-up-and-restoring-an-amazon-rds-db-instance","text":"This section shows how to back up and restore a DB instance. Working with backups Creating a DB snapshot Restoring from a DB snapshot Copying a snapshot Sharing a DB snapshot Exporting DB snapshot data to Amazon S3 Restoring a DB instance to a specified time Deleting a snapshot Tutorial: Restore a DB instance from a DB snapshot","title":"Backing up and restoring an Amazon RDS DB instance"},{"location":"RDSdemos/#monitoring-an-amazon-rds-db-instance","text":"This section shows you how to monitor Amazon RDS. Overview of monitoring Amazon RDS DB instance status Using Amazon RDS recommendations Enhanced Monitoring Using Amazon RDS Performance Insights Using Amazon RDS event notification Viewing Amazon RDS events Getting CloudWatch Events and Amazon EventBridge events for Amazon RDS Amazon RDS database log files","title":"Monitoring an Amazon RDS DB instance"},{"location":"RDSdemos/#start-working-with-aws-rds-and-db-engines","text":"In this Section, you can strat trying the RDS in AWS with many DB engines, by trying the following: Demos Tutorials Best Practices","title":"Start Working with AWS RDS and DB engines"},{"location":"RDSdemos/#demos","text":"In the following examples, you can find how to create and connect to a DB instance using Amazon Relational Database Service (Amazon RDS). You can create a DB instance that uses MariaDB, MySQL, Microsoft SQL Server, Oracle, or PostgreSQL. Creating a DB instance and connecting to a database on a DB instance is slightly different for each of the DB engines. Choose one of the following DB engines that you want to use for detailed information on creating and connecting to the DB instance. After you have created and connected to your DB instance, there are instructions to help you delete the DB instance. Creating a MariaDB DB instance and connecting to a database on a MariaDB DB instance Creating a Microsoft SQL Server DB instance and connecting to a DB instance Creating a MySQL DB instance and connecting to a database on a MySQL DB instance Create and Connect to a MySQL Database How to Install MySQL on Ubuntu 18.04 How to Manage & Create MySQL Users, Databases & Tables Creating an Oracle DB instance and connecting to a database on an Oracle DB instance Creating a PostgreSQL DB instance and connecting to a database on a PostgreSQL DB instance Create and Connect to a PostgreSQL Database How To Install and Use PostgreSQL on Ubuntu 18.04 The PostgreSQL Describe Table Statement How to fix \u201cERROR: column c.relhasoids does not exist\u201d in Postgres? Tutorial: Create a web server and an Amazon RDS DB instance","title":"Demos"},{"location":"RDSdemos/#troubleshooting-for-amazon-rds","text":"Troubleshooting for Amazon RDS","title":"Troubleshooting for Amazon RDS"},{"location":"RDSdemos/#tutorials","text":"The following tutorials show you how to perform common tasks that use Amazon RDS: Tutorial: Create an Amazon VPC for use with a DB instance Tutorial: Create a web server and an Amazon RDS DB instance Tutorial: Restore a DB instance from a DB snapshot","title":"Tutorials"},{"location":"RDSdemos/#best-practices-for-amazon-rds","text":"Learn best practices for working with Amazon RDS. As new best practices are identified, we will keep this section up to date. Amazon RDS basic operational guidelines DB instance RAM recommendations Using Enhanced Monitoring to identify operating system issues Using metrics to identify performance issues Best practices for working with MySQL storage engines Best practices for working with MariaDB storage engines Best practices for working with Oracle Best practices for working with PostgreSQL Best practices for working with SQL Server Working with DB parameter groups Amazon RDS new features and best practices presentation video","title":"Best practices for Amazon RDS"},{"location":"Redshift/","text":"The most popular and fastest cloud data warehouse: Amazon Redshift \u00b6 Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers. Check, Amazon Redshift pricing before starting the warehouse development on AWS Redshift. Redshift Use Cases \u00b6 Business intelligence Redshift makes it simple and cost effective to run high performance queries on petabytes of structured data so that you can build powerful reports and dashboards using your existing business intelligence tools. Operational analytics on business events Bring together structured data from your data warehouse and semi-structured data such as application logs from your S3 data lake to get real-time operational insights on your applications and systems. Cluster management \u00b6 An Amazon Redshift cluster is a set of nodes, which consists of a leader node and one or more compute nodes. The type and number of compute nodes that you need depends on the size of your data, the number of queries you will execute, and the query execution performance that you need. Creating and managing clusters \u00b6 Depending on your data warehousing needs, you can start with a small, single-node cluster and easily scale up to a larger, multi-node cluster as your requirements change. You can add or remove compute nodes to the cluster without any interruption to the service. For more information, see Amazon Redshift clusters . Getting started with Amazon Redshift \u00b6 This is a tutorial designed to walk you through the process of creating a sample Amazon Redshift cluster. You can use this sample cluster to evaluate the Amazon Redshift service. In this tutorial, you perform the following steps: Step 1: Set up prerequisites Step 2: Create an IAM role Step 3: Create a sample Amazon Redshift cluster Step 4: Authorize access to the cluster Step 5: Connect to the sample cluster and run queries Step 6: Load sample data from Amazon S3 Step 7: Find additional resources and reset your environment Check, Amazon Redshift Documentation for all the details. \u00b6","title":"Amazon Redshift"},{"location":"Redshift/#the-most-popular-and-fastest-cloud-data-warehouse-amazon-redshift","text":"Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers. Check, Amazon Redshift pricing before starting the warehouse development on AWS Redshift.","title":"The most popular and fastest cloud data warehouse: Amazon Redshift"},{"location":"Redshift/#redshift-use-cases","text":"Business intelligence Redshift makes it simple and cost effective to run high performance queries on petabytes of structured data so that you can build powerful reports and dashboards using your existing business intelligence tools. Operational analytics on business events Bring together structured data from your data warehouse and semi-structured data such as application logs from your S3 data lake to get real-time operational insights on your applications and systems.","title":"Redshift Use Cases"},{"location":"Redshift/#cluster-management","text":"An Amazon Redshift cluster is a set of nodes, which consists of a leader node and one or more compute nodes. The type and number of compute nodes that you need depends on the size of your data, the number of queries you will execute, and the query execution performance that you need.","title":"Cluster management"},{"location":"Redshift/#creating-and-managing-clusters","text":"Depending on your data warehousing needs, you can start with a small, single-node cluster and easily scale up to a larger, multi-node cluster as your requirements change. You can add or remove compute nodes to the cluster without any interruption to the service. For more information, see Amazon Redshift clusters .","title":"Creating and managing clusters"},{"location":"Redshift/#getting-started-with-amazon-redshift","text":"This is a tutorial designed to walk you through the process of creating a sample Amazon Redshift cluster. You can use this sample cluster to evaluate the Amazon Redshift service. In this tutorial, you perform the following steps: Step 1: Set up prerequisites Step 2: Create an IAM role Step 3: Create a sample Amazon Redshift cluster Step 4: Authorize access to the cluster Step 5: Connect to the sample cluster and run queries Step 6: Load sample data from Amazon S3 Step 7: Find additional resources and reset your environment","title":"Getting started with Amazon Redshift"},{"location":"Redshift/#check-amazon-redshift-documentation-for-all-the-details","text":"","title":"Check, Amazon Redshift Documentation for all the details."},{"location":"RunPython/","text":"Running Python Scripts on EC2 Instances \u00b6 In order to run a python script on an EC2 instance, yuo need to: Login to the AWS webservices Console Launcing an EC2 Instance Connecting to the EC2 Instance Transfering files to the EC2 Instance Running Python Scripts on the EC2 Instance Logging into the AWS Console \u00b6 Connectig to console by browsing https://console.aws.amazon.com/ , then choose the Sign Into the Console : You need to sign as a Root User . Enter you user name and password . After a sucessful login, you will see the AWS Management Console in your screen. In the Find Services bar, search for EC2 Once founded, press on EC2 Check more details on how to login and create an AWS user, see Setting Up an AWS Account . You need to think about your Availability Zone , before start launching an instance, see AWS Infrastructure . Launching an EC2 Instance \u00b6 To launch an EC2 instance, you need to press the Launch Instance button in your EC2 Dashboard . Then follow the wizard: Step1: Choose an Amazone Machine Image (AMI), check all the AMIs , We recommend a Free tier eligible AMIs for training, the name of the free AMIs, such as Ubuntu Server 16.04 LTS Step2: Choose an Instance Type, Check all the Instances Types and Capabilities , We recommend a Free tier eligible Instances for training, The AWS Free Tier allotment for Linux and Microsoft Windows instances is counted separately. You can run 750 hours of a Linux t3.micro , t2.micro , or t1.micro instance plus 750 hours of a Windows t3.micro , t2.micro , or t1.micro instance each month for the first 12 months . Steps 3 to 6: Configure Isnstance, Add Storage, Add Tags, and Configure Security Group You can skip these three steps by just pressing Review and Launch These three steps are related to the instance networking, storage, tags, and security accordingly. By default, the instance is configured well and has a small storage EBS. In case you need to configure those elements and for example add storage, you should not skip these steps. Step7: Review Instance Launch You need just press Launch . You have to use either an existing key pair or generating a new one, see, Create a key pair . Finally, press Launch Instances Press View Instances to see the status of your new instance and give it a Name . Wait until you find your instance status is running . Check more details on how to launch an AWS EC2 instance, see Launch an EC2 instance . Connecting to the EC2 Instance \u00b6 Connecting to the EC2 instances depends on many parameters and there are many ways to establish the connection and to exchange files between your local machine and the instance machine: Connecting to a Linux instance through Local Linux Machine , this can be accomplished by: SSH Connection Connecting to a Linux instance through Local Windows Machine , this can be accomplished by: Putty or MobaXterm Connecting to a Windows instance through Local Windows | Linux Machine Check more details on how to connect to an AWS EC2 instance, see Connect to an EC2 instance . Transfering files to the EC2 Instance \u00b6 Transfering files to the EC2 instances depends on many parameters and there are many ways to exchange files between your local machine and the instance machine: Transferring to | from a Linux instance through Local Linux Machine , this can be accomplished by: SCP FilZilla Transferring to | from a Linux instance through Local Windows Machine , this can be accomplished by: PSCP WinSCP FilZilla Transferring to | from a Windows instance through Local Windows | Linux Machine Check more details on how to transfer [to|from] an AWS EC2 instance, see transfer to|from an EC2 instance . Running Python Scripts on the EC2 Instance \u00b6 After connecting and traferrin files to the Ec2 instance. You can check the availability of the Python on the machine you have. What you need to start running python: Install Python3 It is already installed in the instance machine, and you can check on this by writing: ubuntu@ip-172-31-38-59:~$ python3 Python 3.5.2 (default, Jul 17 2020, 14:04:10) [GCC 5.4.0 20160609] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> exit() ubuntu@ip-172-31-38-59:~$ Install pip3 , will be needed to install more libraries and packages in python: ubuntu@ip-172-31-38-59:~$ sudo apt-get install python3-pip Install new packages by using pip3 , for example install numpy ubuntu@ip-172-31-38-59:~$ pip3 install numpy Prepare a python script to test it, such as import numpy as np a = np.sum([10,20]) print('hello from Hydro!\\n the result is= {}'.format(str(a))) Save this script in a file, named for example test.py , then run it by ubuntu@ip-172-31-38-59:~/testfolder$ python3 test.py hello from Hydro! the result is= 30 Prepare your virtual environment in Python3 \u00b6 Installing Virtualenv using pip3 \u00b6 Install Virtualenv : ubuntu@ip-172-31-38-59:~$ pip3 install virtualenv Collecting virtualenv Downloading virtualenv-15.1.0-py2.py3-none-any.whl (1.8MB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.8MB 367kB/s Installing collected packages: virtualenv Successfully installed virtualenv-15.1.0 Where it is located: ubuntu@ip-172-31-38-59:~$ which virtualenv /home/ubuntu/.local/bin/virtualenv Where is python3: ubuntu@ip-172-31-38-59:~$ which python3 /usr/bin/python3 Create a new Virtualenv \u00b6 Initiate a new virtual environment, and name it hydrosat : ubuntu@ip-172-31-38-59:~$ virtualenv -p /usr/bin/python3 hydrosat created virtual environment CPython3.5.2.final.0-64 in 130ms creator CPython3Posix(dest=/home/ubuntu/hydrosat, clear=False, global=False) seeder FromAppData(download=False, setuptools=bundle, pip=bundle, wheel=bundle, via=copy, app_data_dir=/home/ubuntu/.local/share/virtualenv) added seed packages: pip==20.1.1, setuptools==49.2.0, wheel==0.34.2 activators FishActivator,PowerShellActivator,BashActivator,CShellActivator,PythonActivator,XonshActivator activate the new virtual environment: ubuntu@ip-172-31-38-59:~$ source hydrosat/bin/activate (hydrosat) ubuntu@ip-172-31-38-59:~$ python -V Python 3.5.2 deactivate the virtual environment: (hydrosat) ubuntu@ip-172-31-38-59:~$ deactivate ubuntu@ip-172-31-38-59:~$ Delete the virtual environment \u00b6 To delete a virtual environment, simply delete the project folder. Using the previous example, run the following command: ubuntu@ip-172-31-38-59:~$ rm -rf hydrosat ubuntu@ip-172-31-38-59:~$ ls testfolder TextFile.txt ubuntu@ip-172-31-38-59:~$ For mor information, see Installing and using virtualenv with Python 3 and other techniques are here for Virtualenv . Watch it all here \u00b6 Run python on Linux instance, connected from Linux machine \u00b6 Run python on Linux instance, connected from Windows machine \u00b6","title":"Running Python on AWS EC2 Instance"},{"location":"RunPython/#running-python-scripts-on-ec2-instances","text":"In order to run a python script on an EC2 instance, yuo need to: Login to the AWS webservices Console Launcing an EC2 Instance Connecting to the EC2 Instance Transfering files to the EC2 Instance Running Python Scripts on the EC2 Instance","title":"Running Python Scripts on EC2 Instances"},{"location":"RunPython/#logging-into-the-aws-console","text":"Connectig to console by browsing https://console.aws.amazon.com/ , then choose the Sign Into the Console : You need to sign as a Root User . Enter you user name and password . After a sucessful login, you will see the AWS Management Console in your screen. In the Find Services bar, search for EC2 Once founded, press on EC2 Check more details on how to login and create an AWS user, see Setting Up an AWS Account . You need to think about your Availability Zone , before start launching an instance, see AWS Infrastructure .","title":"Logging into the AWS Console"},{"location":"RunPython/#launching-an-ec2-instance","text":"To launch an EC2 instance, you need to press the Launch Instance button in your EC2 Dashboard . Then follow the wizard: Step1: Choose an Amazone Machine Image (AMI), check all the AMIs , We recommend a Free tier eligible AMIs for training, the name of the free AMIs, such as Ubuntu Server 16.04 LTS Step2: Choose an Instance Type, Check all the Instances Types and Capabilities , We recommend a Free tier eligible Instances for training, The AWS Free Tier allotment for Linux and Microsoft Windows instances is counted separately. You can run 750 hours of a Linux t3.micro , t2.micro , or t1.micro instance plus 750 hours of a Windows t3.micro , t2.micro , or t1.micro instance each month for the first 12 months . Steps 3 to 6: Configure Isnstance, Add Storage, Add Tags, and Configure Security Group You can skip these three steps by just pressing Review and Launch These three steps are related to the instance networking, storage, tags, and security accordingly. By default, the instance is configured well and has a small storage EBS. In case you need to configure those elements and for example add storage, you should not skip these steps. Step7: Review Instance Launch You need just press Launch . You have to use either an existing key pair or generating a new one, see, Create a key pair . Finally, press Launch Instances Press View Instances to see the status of your new instance and give it a Name . Wait until you find your instance status is running . Check more details on how to launch an AWS EC2 instance, see Launch an EC2 instance .","title":"Launching an EC2 Instance"},{"location":"RunPython/#connecting-to-the-ec2-instance","text":"Connecting to the EC2 instances depends on many parameters and there are many ways to establish the connection and to exchange files between your local machine and the instance machine: Connecting to a Linux instance through Local Linux Machine , this can be accomplished by: SSH Connection Connecting to a Linux instance through Local Windows Machine , this can be accomplished by: Putty or MobaXterm Connecting to a Windows instance through Local Windows | Linux Machine Check more details on how to connect to an AWS EC2 instance, see Connect to an EC2 instance .","title":"Connecting to the EC2 Instance"},{"location":"RunPython/#transfering-files-to-the-ec2-instance","text":"Transfering files to the EC2 instances depends on many parameters and there are many ways to exchange files between your local machine and the instance machine: Transferring to | from a Linux instance through Local Linux Machine , this can be accomplished by: SCP FilZilla Transferring to | from a Linux instance through Local Windows Machine , this can be accomplished by: PSCP WinSCP FilZilla Transferring to | from a Windows instance through Local Windows | Linux Machine Check more details on how to transfer [to|from] an AWS EC2 instance, see transfer to|from an EC2 instance .","title":"Transfering files to the EC2 Instance"},{"location":"RunPython/#running-python-scripts-on-the-ec2-instance","text":"After connecting and traferrin files to the Ec2 instance. You can check the availability of the Python on the machine you have. What you need to start running python: Install Python3 It is already installed in the instance machine, and you can check on this by writing: ubuntu@ip-172-31-38-59:~$ python3 Python 3.5.2 (default, Jul 17 2020, 14:04:10) [GCC 5.4.0 20160609] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> exit() ubuntu@ip-172-31-38-59:~$ Install pip3 , will be needed to install more libraries and packages in python: ubuntu@ip-172-31-38-59:~$ sudo apt-get install python3-pip Install new packages by using pip3 , for example install numpy ubuntu@ip-172-31-38-59:~$ pip3 install numpy Prepare a python script to test it, such as import numpy as np a = np.sum([10,20]) print('hello from Hydro!\\n the result is= {}'.format(str(a))) Save this script in a file, named for example test.py , then run it by ubuntu@ip-172-31-38-59:~/testfolder$ python3 test.py hello from Hydro! the result is= 30","title":"Running Python Scripts on the EC2 Instance"},{"location":"RunPython/#prepare-your-virtual-environment-in-python3","text":"","title":"Prepare your virtual environment in Python3"},{"location":"RunPython/#installing-virtualenv-using-pip3","text":"Install Virtualenv : ubuntu@ip-172-31-38-59:~$ pip3 install virtualenv Collecting virtualenv Downloading virtualenv-15.1.0-py2.py3-none-any.whl (1.8MB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.8MB 367kB/s Installing collected packages: virtualenv Successfully installed virtualenv-15.1.0 Where it is located: ubuntu@ip-172-31-38-59:~$ which virtualenv /home/ubuntu/.local/bin/virtualenv Where is python3: ubuntu@ip-172-31-38-59:~$ which python3 /usr/bin/python3","title":"Installing Virtualenv using pip3"},{"location":"RunPython/#create-a-new-virtualenv","text":"Initiate a new virtual environment, and name it hydrosat : ubuntu@ip-172-31-38-59:~$ virtualenv -p /usr/bin/python3 hydrosat created virtual environment CPython3.5.2.final.0-64 in 130ms creator CPython3Posix(dest=/home/ubuntu/hydrosat, clear=False, global=False) seeder FromAppData(download=False, setuptools=bundle, pip=bundle, wheel=bundle, via=copy, app_data_dir=/home/ubuntu/.local/share/virtualenv) added seed packages: pip==20.1.1, setuptools==49.2.0, wheel==0.34.2 activators FishActivator,PowerShellActivator,BashActivator,CShellActivator,PythonActivator,XonshActivator activate the new virtual environment: ubuntu@ip-172-31-38-59:~$ source hydrosat/bin/activate (hydrosat) ubuntu@ip-172-31-38-59:~$ python -V Python 3.5.2 deactivate the virtual environment: (hydrosat) ubuntu@ip-172-31-38-59:~$ deactivate ubuntu@ip-172-31-38-59:~$","title":"Create a new Virtualenv"},{"location":"RunPython/#delete-the-virtual-environment","text":"To delete a virtual environment, simply delete the project folder. Using the previous example, run the following command: ubuntu@ip-172-31-38-59:~$ rm -rf hydrosat ubuntu@ip-172-31-38-59:~$ ls testfolder TextFile.txt ubuntu@ip-172-31-38-59:~$ For mor information, see Installing and using virtualenv with Python 3 and other techniques are here for Virtualenv .","title":"Delete the virtual environment"},{"location":"RunPython/#watch-it-all-here","text":"","title":"Watch it all here"},{"location":"RunPython/#run-python-on-linux-instance-connected-from-linux-machine","text":"","title":"Run python on Linux instance, connected from Linux machine"},{"location":"RunPython/#run-python-on-linux-instance-connected-from-windows-machine","text":"","title":"Run python on Linux instance, connected from Windows machine"},{"location":"S3/","text":"Amazon S3 Object Storage \u00b6 Amazon Simple Storage Service (Amazon S3) stores data as objects within resources that are called buckets. You can store as many objects as you want within a bucket, and you can write, read, and delete objects in your bucket. Objects can be up to 5 TB in size. You can control access to both the bucket and the objects (who can create, delete, and retrieve objects in the bucket for example), and view access logs for the bucket and its objects. You can also choose the AWS Region where a bucket is stored to optimize for latency, minimize costs, or address regulatory requirements. With Amazon S3, you pay only for what you use. There is no minimum fee. Estimate your monthly bill by using the AWS Simple Monthly Calculator. We charge less where our costs are less, and prices are based on the location of your Amazon S3 bucket. Full details on Amazon S3 can be found here: https://aws.amazon.com/s3 . S3 Benefits \u00b6 Industry-leading performance, scalability, availability, and durability Wide range of cost-effective storage classes Unmatched security, compliance, and audit capabilities Easily manage data and access controls Query-in-place services for analytics Most supported cloud storage service Block-storage(EBS, EFS) vs. S3 Storage \u00b6 Finally, Amazon S3 is an object store good at storing vast numbers of backups or user files. Unlike EBS or EFS, S3 is not limited to EC2. Files stored within an S3 bucket can be accessed programmatically or directly from services such as AWS CloudFront. This is why many websites use it to hold their content and media files, which may be served efficiently from AWS CloudFront. Amazon S3 Use Cases \u00b6 Data lake and big data analytics Backup and restoration Reliable disaster recovery Methodical archiving Amazon S3 Demonstration \u00b6 Let\u2019s go ahead and create a bucket, and actually upload an object into that bucket. See Working with Amazon S3 Buckets , and do: Naviage to the aws console, Look for the S3 services, Select Create Bucket , a. give it a name b. select an aws region c. leave the rest as default Add data (files, images, or videos) to the created bucket a. Upload an object b. Set object properties c. Set object permissions Delete your data, by checking the bucket and click delete","title":"Amazon S3 Storage"},{"location":"S3/#amazon-s3-object-storage","text":"Amazon Simple Storage Service (Amazon S3) stores data as objects within resources that are called buckets. You can store as many objects as you want within a bucket, and you can write, read, and delete objects in your bucket. Objects can be up to 5 TB in size. You can control access to both the bucket and the objects (who can create, delete, and retrieve objects in the bucket for example), and view access logs for the bucket and its objects. You can also choose the AWS Region where a bucket is stored to optimize for latency, minimize costs, or address regulatory requirements. With Amazon S3, you pay only for what you use. There is no minimum fee. Estimate your monthly bill by using the AWS Simple Monthly Calculator. We charge less where our costs are less, and prices are based on the location of your Amazon S3 bucket. Full details on Amazon S3 can be found here: https://aws.amazon.com/s3 .","title":"Amazon S3 Object Storage"},{"location":"S3/#s3-benefits","text":"Industry-leading performance, scalability, availability, and durability Wide range of cost-effective storage classes Unmatched security, compliance, and audit capabilities Easily manage data and access controls Query-in-place services for analytics Most supported cloud storage service","title":"S3 Benefits"},{"location":"S3/#block-storageebs-efs-vs-s3-storage","text":"Finally, Amazon S3 is an object store good at storing vast numbers of backups or user files. Unlike EBS or EFS, S3 is not limited to EC2. Files stored within an S3 bucket can be accessed programmatically or directly from services such as AWS CloudFront. This is why many websites use it to hold their content and media files, which may be served efficiently from AWS CloudFront.","title":"Block-storage(EBS, EFS) vs. S3 Storage"},{"location":"S3/#amazon-s3-use-cases","text":"Data lake and big data analytics Backup and restoration Reliable disaster recovery Methodical archiving","title":"Amazon S3 Use Cases"},{"location":"S3/#amazon-s3-demonstration","text":"Let\u2019s go ahead and create a bucket, and actually upload an object into that bucket. See Working with Amazon S3 Buckets , and do: Naviage to the aws console, Look for the S3 services, Select Create Bucket , a. give it a name b. select an aws region c. leave the rest as default Add data (files, images, or videos) to the created bucket a. Upload an object b. Set object properties c. Set object permissions Delete your data, by checking the bucket and click delete","title":"Amazon S3 Demonstration"},{"location":"SNS/","text":"Amazon Simple Notification Service (Amazon SNS) \u00b6 Amazon Simple Notification Service (Amazon SNS) is a managed service that provides message delivery from publishers to subscribers (also known as producers and consumers). Publishers communicate asynchronously with subscribers by sending messages to a topic, which is a logical access point and communication channel. Clients can subscribe to the SNS topic and receive published messages using a supported protocol, such as Amazon SQS, AWS Lambda, HTTP, email, mobile push notifications, and mobile text messages (SMS). Features and capabilities \u00b6 Application-to-application messaging Application-to-person notifications Standard and FIFO topics Message delivery retry Dead-letter queues Message attributes Message filtering Message security Message durability Getting started with Amazon SNS \u00b6 This section helps you become more familiar with Amazon SNS by showing you how to manage topics, subscriptions, and messages using the Amazon SNS console. Prerequisites Step 1: Create a topic Step 2: Create a subscription to the topic Step 3: Publish a message to the topic Step 4: Delete the subscription and topic Next steps","title":"Amazon Simple Notification Service"},{"location":"SNS/#amazon-simple-notification-service-amazon-sns","text":"Amazon Simple Notification Service (Amazon SNS) is a managed service that provides message delivery from publishers to subscribers (also known as producers and consumers). Publishers communicate asynchronously with subscribers by sending messages to a topic, which is a logical access point and communication channel. Clients can subscribe to the SNS topic and receive published messages using a supported protocol, such as Amazon SQS, AWS Lambda, HTTP, email, mobile push notifications, and mobile text messages (SMS).","title":"Amazon Simple Notification Service (Amazon SNS)"},{"location":"SNS/#features-and-capabilities","text":"Application-to-application messaging Application-to-person notifications Standard and FIFO topics Message delivery retry Dead-letter queues Message attributes Message filtering Message security Message durability","title":"Features and capabilities"},{"location":"SNS/#getting-started-with-amazon-sns","text":"This section helps you become more familiar with Amazon SNS by showing you how to manage topics, subscriptions, and messages using the Amazon SNS console. Prerequisites Step 1: Create a topic Step 2: Create a subscription to the topic Step 3: Publish a message to the topic Step 4: Delete the subscription and topic Next steps","title":"Getting started with Amazon SNS"},{"location":"WebserversDemo/","text":"Demo: Web Server on AWS EC2 Instances \u00b6 Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure and resizable compute capacity in the cloud. It\u2019s designed to make web-scale cloud computing easier for developers. Amazon EC2 presents a true virtual computing environment, and it allows you to use web service interfaces to launch instances with a variety of operating systems, load them with your custom application environment, manage your network\u2019s access permissions, and run your image by using as many or few systems as you want. Details on the features and cost of Amazon EC2 are available at: https://aws.amazon.com/ec2/ Amazon EC2 provides a wide selection of instance types that are optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity. They give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, which allows you to scale your resources to the requirements of your target workload. Current details about available instance types are available at: https://aws.amazon.com/ec2/instance-types/ In this section, you find one of the mostly used servers nowadays (e.g. web server) built on the EC2 comput instances on two different AMI, the first on Linux machine , the second on ubuntu machine , third one on Windows machine . Creating a web server on EC2 instance: Linux Machine \u00b6 1. Launching the Instance with the proper setting \u00b6 To create the web server on Amazon linux machine, follow the steps in Launcing EC2 Instance : Choose Linux AMI, and decide on which kind of instance you need Configure instance details, up tp now leave it as is In case you need more storage, add in the next step more volumes Mange your resources by tags Very important is to manage your security groups, see Create a security group a. Do not forget to add the HTTP/HTTPs rule in order to access your web server b. Make this rule to be accessed from anywhere c. Keep the SSH rule as is, this will allow you to access the machin through SSh Review your instance by adding a public key to access it 2. Installing the webserver [OR User data and shell scripts ] \u00b6 Once instance is launched and running: Connect to it through SSH Transfer the files you may need to install them, through internet or from your local machin Install the web server by using the followin script: $> yum update -y $> amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2 $> yum install -y httpd mariadb-server $> systemctl start httpd $> systemctl enable httpd $> usermod -a -G apache ec2-user $> chown -R ec2-user:apache /var/www $> chmod 2775 /var/www $> find /var/www -type d -exec chmod 2775 {} \\; $> find /var/www -type f -exec chmod 0664 {} \\; $> echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php You can run the previous commands in the terminal or run it as a bash script, see all scripts , Check Apache: Webserver installation Linux bash script (Script_linux.sh) , it is as follow: #!/bin/bash yum update -y amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2 yum install -y httpd mariadb-server systemctl start httpd systemctl enable httpd usermod -a -G apache ec2-user chown -R ec2-user:apache /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} \\; find /var/www -type f -exec chmod 0664 {} \\; echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php In order to run this in the terminal directly, do the following: $> wget https://raw.githubusercontent.com/AbdallahCoptan/HandsOn/master/AWS/docs/Codes/script_linux.sh $> ./script_linux.sh 3. Test the installed webserver \u00b6 In order to test your webserver in the instance, in a web browser, enter the URL of the PHP test file the script created. This URL is the public DNS address of your instance followed by a forward slash and the file name. http://my.public.dns.amazonaws.com(or the public IP)/phpinfo.php Creating a web server on EC2 instance: Ubuntu Machine \u00b6 1. Launching the Instance with the proper setting \u00b6 To create the web server on Amazon linux machine, follow the steps in Launcing EC2 Instance : Choose Ubuntu server AMI, and decide on which kind of instance you need Configure instance details, up tp now leave it as is In case you need more storage, add in the next step more volumes Mange your resources by tags Very important is to manage your security groups, see Create a security group a. Do not forget to add the HTTP/HTTPs rule in order to access your web server b. Make this rule to be accessed from anywhere c. Keep the SSH rule as is, this will allow you to access the machin through SSh Review your instance by adding a public key to access it 2. Installing the webserver [OR User data and shell scripts ] \u00b6 A. Installing Apache Web Server \u00b6 Once instance is launched and running: Connect to it through SSH Transfer the files you may need to install them, through internet or from your local machin Install the web server by using the followin script: $> sudo apt-get update $> sudo add-apt-repository ppa:ondrej/php $> sudo apt update $> sudo apt install php7.2 $> sudo groupadd apache $> sudo usermod -a -G apache ubuntu $> sudo chown -R ubuntu:apache /var/www $> sudo chmod 2775 /var/www $> find /var/www -type d -exec chmod 2775 {} \\; $> find /var/www -type f -exec chmod 0664 {} \\; $> echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php $> cd /var/www/html/ $> wget https://raw.githubusercontent.com/AbdallahCoptan/HandsOn/master/AWS/docs/Codes/hydro.html You can run the previous commands in the terminal or run it as a bash script, see all scripts , Check Apache: Webserver installation Ubuntu bash script (Script_linux_1.sh) , it is as follow: #!/bin/bash echo \"Y\" | sudo apt-get update echo \"\" | sudo add-apt-repository ppa:ondrej/php echo \"Y\" | sudo apt update echo \"Y\" | sudo apt install php7.2 sudo groupadd apache sudo usermod -a -G apache ubuntu sudo chown -R ubuntu:apache /var/www sudo chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} \\; find /var/www -type f -exec chmod 0664 {} \\; echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php cd /var/www/html/ wget https://raw.githubusercontent.com/AbdallahCoptan/HandsOn/master/AWS/docs/Codes/hydro.html In order to run this in the terminal directly, do the following: $> wget https://raw.githubusercontent.com/AbdallahCoptan/HandsOn/master/AWS/docs/Codes/script_ubuntu_1.sh $> chmod +x script_ubuntu_1.sh $> ./script_ubuntu_1.sh B. Installing Flusk through Python \u00b6 Once instance is launched and running: Connect to it through SSH Transfer the files you may need to install them, through internet or from your local machin Install the web server by using the followin script: $> git clone https://github.com/WillKoehrsen/recurrent-neural-networks.git $> sudo apt-get update $> sudo apt-get install python3-pip $> cd recurrent-neural-networks $> pip3 install --user -r requirements.txt $> cd deployment $> sudo python3 run_keras_server.py You can run the previous commands in the terminal or run it as a bash script, see all scripts , Python web flax: Webserver installation Ubuntu bash script (test.sh) , it is as follow: #!/bin/bash git clone https://github.com/WillKoehrsen/recurrent-neural-networks.git echo \"Y\" | sudo apt-get update echo \"Y\" | sudo apt-get install python3-pip cd recurrent-neural-networks pip3 install --user -r requirements.txt cd deployment sudo python3 run_keras_server.py In order to run this in the terminal directly, do the following: $> wget https://raw.githubusercontent.com/AbdallahCoptan/HandsOn/master/AWS/docs/Codes/test.sh $> chmod +x test.sh $> ./test.sh 3. Test the installed webserver \u00b6 A. Test Apache Server \u00b6 In order to test your webserver in the instance, in a web browser, enter the URL of the PHP test file the script created. This URL is the public DNS address of your instance followed by a forward slash and the file name. http://my.public.dns.amazonaws.com(or the public IP)/phpinfo.php http://my.public.dns.amazonaws.com(or the public IP)/ http://my.public.dns.amazonaws.com(or the public IP)/hydro.html B. Test Flusk Server \u00b6 In order to test your webserver in the instance, in a web browser, enter the URL of the PHP test file the script created. This URL is the public DNS address of your instance followed by a forward slash and the file name. http://my.public.dns.amazonaws.com(or the public IP)/ Creating a web server on EC2 instance: Windows Machine \u00b6 1. Create instance with a Windwos AMI \u00b6 To creat an instance, please check Getting started with Amazon EC2 Windows instances . 2. Connect to the Windows instance \u00b6 To connect to a Windows instance, see Connecting to Your Windows Instance in the Amazon EC2 User Guide for Windows Instances. To transfer files to and from a Windows instance, see Transfer files to Windows instances in the Amazon EC2 User Guide for Windows Instances. Use remmina to connect to your instance and transfer files from ubuntu machine. Check How to create shared folder on Windows Remote Desktop from Ubuntu via Remmina . 3. Install the web server on Windows \u00b6 How to Install Apache on a Windows Server Running commands on your Linux instance at launch \u00b6 When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the command line tools), or as base64-encoded text (for API calls). Specify instance user data at launch \u00b6 You can specify instance user data when you launch the instance. If the root volume of the instance is an EBS volume, you can also stop the instance and update its user data. Follow the procedure for launching an instance at Launching an instance using the Launch Instance Wizard , but when you get to Step 3: Configure Instance Details in that procedure, copy your shell script in the User data field, and then complete the launch procedure. In the example script below, the script creates and configures our web server. #!/bin/bash yum update -y amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2 yum install -y httpd mariadb-server systemctl start httpd systemctl enable httpd usermod -a -G apache ec2-user chown -R ec2-user:apache /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} \\; find /var/www -type f -exec chmod 0664 {} \\; echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php Allow enough time for the instance to launch and execute the commands in your script, and then check to see that your script has completed the tasks that you intended. For our example, in a web browser, enter the URL of the PHP test file the script created. This URL is the public DNS address of your instance followed by a forward slash and the file name. http://my.public.dns.amazonaws.com/phpinfo.php You should see the PHP information page. If you are unable to see the PHP information page, check that the security group you are using contains a rule to allow HTTP (port 80) traffic. For more information, see Adding rules to a security group . (Optional) If your script did not accomplish the tasks you were expecting it to, or if you just want to verify that your script completed without errors, examine the cloud-init output log file at /var/log/cloud-init-output.log and look for error messages in the output. View and update the instance user data \u00b6 To modify instance user data Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the navigation pane, choose Instances . Select the instance and choose Actions, Instance State, Stop . /!\\ Warning When you stop an instance, the data on any instance store volumes is erased. To keep data from instance store volumes, be sure to back it up to persistent storage. When prompted for confirmation, choose Yes, Stop . It can take a few minutes for the instance to stop. With the instance still selected, choose Actions, Instance Settings, View/Change User Data . You can\u2019t change the user data if the instance is running, but you can view it. In the View/Change User Data dialog box, update the user data, and then choose Save. Restart the instance. The new user data is visible on your instance after you restart it; however, user data scripts are not executed. Running commands on your Windows instance at launch \u00b6 For information about running commands on your Windows instance at launch, see Running Commands on Your Windows Instance at Launch and Managing Windows Instance Configuration in the Amazon EC2 User Guide for Windows Instances. More complex automation scenarios during the launching of instances \u00b6 If you are interested in more complex automation scenarios, consider using AWS CloudFormation and AWS OpsWorks. For more information, see the AWS CloudFormation User Guide and the AWS OpsWorks User Guide .","title":"Demo--Web server on EC2 Linux and Ubuntu Machines"},{"location":"WebserversDemo/#demo-web-server-on-aws-ec2-instances","text":"Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure and resizable compute capacity in the cloud. It\u2019s designed to make web-scale cloud computing easier for developers. Amazon EC2 presents a true virtual computing environment, and it allows you to use web service interfaces to launch instances with a variety of operating systems, load them with your custom application environment, manage your network\u2019s access permissions, and run your image by using as many or few systems as you want. Details on the features and cost of Amazon EC2 are available at: https://aws.amazon.com/ec2/ Amazon EC2 provides a wide selection of instance types that are optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity. They give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, which allows you to scale your resources to the requirements of your target workload. Current details about available instance types are available at: https://aws.amazon.com/ec2/instance-types/ In this section, you find one of the mostly used servers nowadays (e.g. web server) built on the EC2 comput instances on two different AMI, the first on Linux machine , the second on ubuntu machine , third one on Windows machine .","title":"Demo: Web Server on AWS EC2 Instances"},{"location":"WebserversDemo/#creating-a-web-server-on-ec2-instance-linux-machine","text":"","title":"Creating a web server on EC2 instance: Linux Machine"},{"location":"WebserversDemo/#1-launching-the-instance-with-the-proper-setting","text":"To create the web server on Amazon linux machine, follow the steps in Launcing EC2 Instance : Choose Linux AMI, and decide on which kind of instance you need Configure instance details, up tp now leave it as is In case you need more storage, add in the next step more volumes Mange your resources by tags Very important is to manage your security groups, see Create a security group a. Do not forget to add the HTTP/HTTPs rule in order to access your web server b. Make this rule to be accessed from anywhere c. Keep the SSH rule as is, this will allow you to access the machin through SSh Review your instance by adding a public key to access it","title":"1. Launching the Instance with the proper setting"},{"location":"WebserversDemo/#2-installing-the-webserver-or-user-data-and-shell-scripts","text":"Once instance is launched and running: Connect to it through SSH Transfer the files you may need to install them, through internet or from your local machin Install the web server by using the followin script: $> yum update -y $> amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2 $> yum install -y httpd mariadb-server $> systemctl start httpd $> systemctl enable httpd $> usermod -a -G apache ec2-user $> chown -R ec2-user:apache /var/www $> chmod 2775 /var/www $> find /var/www -type d -exec chmod 2775 {} \\; $> find /var/www -type f -exec chmod 0664 {} \\; $> echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php You can run the previous commands in the terminal or run it as a bash script, see all scripts , Check Apache: Webserver installation Linux bash script (Script_linux.sh) , it is as follow: #!/bin/bash yum update -y amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2 yum install -y httpd mariadb-server systemctl start httpd systemctl enable httpd usermod -a -G apache ec2-user chown -R ec2-user:apache /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} \\; find /var/www -type f -exec chmod 0664 {} \\; echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php In order to run this in the terminal directly, do the following: $> wget https://raw.githubusercontent.com/AbdallahCoptan/HandsOn/master/AWS/docs/Codes/script_linux.sh $> ./script_linux.sh","title":"2. Installing the webserver [OR User data and shell scripts]"},{"location":"WebserversDemo/#3-test-the-installed-webserver","text":"In order to test your webserver in the instance, in a web browser, enter the URL of the PHP test file the script created. This URL is the public DNS address of your instance followed by a forward slash and the file name. http://my.public.dns.amazonaws.com(or the public IP)/phpinfo.php","title":"3. Test the installed webserver"},{"location":"WebserversDemo/#creating-a-web-server-on-ec2-instance-ubuntu-machine","text":"","title":"Creating a web server on EC2 instance: Ubuntu Machine"},{"location":"WebserversDemo/#1-launching-the-instance-with-the-proper-setting_1","text":"To create the web server on Amazon linux machine, follow the steps in Launcing EC2 Instance : Choose Ubuntu server AMI, and decide on which kind of instance you need Configure instance details, up tp now leave it as is In case you need more storage, add in the next step more volumes Mange your resources by tags Very important is to manage your security groups, see Create a security group a. Do not forget to add the HTTP/HTTPs rule in order to access your web server b. Make this rule to be accessed from anywhere c. Keep the SSH rule as is, this will allow you to access the machin through SSh Review your instance by adding a public key to access it","title":"1. Launching the Instance with the proper setting"},{"location":"WebserversDemo/#2-installing-the-webserver-or-user-data-and-shell-scripts_1","text":"","title":"2. Installing the webserver [OR User data and shell scripts]"},{"location":"WebserversDemo/#a-installing-apache-web-server","text":"Once instance is launched and running: Connect to it through SSH Transfer the files you may need to install them, through internet or from your local machin Install the web server by using the followin script: $> sudo apt-get update $> sudo add-apt-repository ppa:ondrej/php $> sudo apt update $> sudo apt install php7.2 $> sudo groupadd apache $> sudo usermod -a -G apache ubuntu $> sudo chown -R ubuntu:apache /var/www $> sudo chmod 2775 /var/www $> find /var/www -type d -exec chmod 2775 {} \\; $> find /var/www -type f -exec chmod 0664 {} \\; $> echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php $> cd /var/www/html/ $> wget https://raw.githubusercontent.com/AbdallahCoptan/HandsOn/master/AWS/docs/Codes/hydro.html You can run the previous commands in the terminal or run it as a bash script, see all scripts , Check Apache: Webserver installation Ubuntu bash script (Script_linux_1.sh) , it is as follow: #!/bin/bash echo \"Y\" | sudo apt-get update echo \"\" | sudo add-apt-repository ppa:ondrej/php echo \"Y\" | sudo apt update echo \"Y\" | sudo apt install php7.2 sudo groupadd apache sudo usermod -a -G apache ubuntu sudo chown -R ubuntu:apache /var/www sudo chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} \\; find /var/www -type f -exec chmod 0664 {} \\; echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php cd /var/www/html/ wget https://raw.githubusercontent.com/AbdallahCoptan/HandsOn/master/AWS/docs/Codes/hydro.html In order to run this in the terminal directly, do the following: $> wget https://raw.githubusercontent.com/AbdallahCoptan/HandsOn/master/AWS/docs/Codes/script_ubuntu_1.sh $> chmod +x script_ubuntu_1.sh $> ./script_ubuntu_1.sh","title":"A. Installing Apache Web Server"},{"location":"WebserversDemo/#b-installing-flusk-through-python","text":"Once instance is launched and running: Connect to it through SSH Transfer the files you may need to install them, through internet or from your local machin Install the web server by using the followin script: $> git clone https://github.com/WillKoehrsen/recurrent-neural-networks.git $> sudo apt-get update $> sudo apt-get install python3-pip $> cd recurrent-neural-networks $> pip3 install --user -r requirements.txt $> cd deployment $> sudo python3 run_keras_server.py You can run the previous commands in the terminal or run it as a bash script, see all scripts , Python web flax: Webserver installation Ubuntu bash script (test.sh) , it is as follow: #!/bin/bash git clone https://github.com/WillKoehrsen/recurrent-neural-networks.git echo \"Y\" | sudo apt-get update echo \"Y\" | sudo apt-get install python3-pip cd recurrent-neural-networks pip3 install --user -r requirements.txt cd deployment sudo python3 run_keras_server.py In order to run this in the terminal directly, do the following: $> wget https://raw.githubusercontent.com/AbdallahCoptan/HandsOn/master/AWS/docs/Codes/test.sh $> chmod +x test.sh $> ./test.sh","title":"B. Installing Flusk through Python"},{"location":"WebserversDemo/#3-test-the-installed-webserver_1","text":"","title":"3. Test the installed webserver"},{"location":"WebserversDemo/#a-test-apache-server","text":"In order to test your webserver in the instance, in a web browser, enter the URL of the PHP test file the script created. This URL is the public DNS address of your instance followed by a forward slash and the file name. http://my.public.dns.amazonaws.com(or the public IP)/phpinfo.php http://my.public.dns.amazonaws.com(or the public IP)/ http://my.public.dns.amazonaws.com(or the public IP)/hydro.html","title":"A. Test Apache Server"},{"location":"WebserversDemo/#b-test-flusk-server","text":"In order to test your webserver in the instance, in a web browser, enter the URL of the PHP test file the script created. This URL is the public DNS address of your instance followed by a forward slash and the file name. http://my.public.dns.amazonaws.com(or the public IP)/","title":"B. Test Flusk Server"},{"location":"WebserversDemo/#creating-a-web-server-on-ec2-instance-windows-machine","text":"","title":"Creating a web server on EC2 instance: Windows Machine"},{"location":"WebserversDemo/#1-create-instance-with-a-windwos-ami","text":"To creat an instance, please check Getting started with Amazon EC2 Windows instances .","title":"1. Create instance with a Windwos AMI"},{"location":"WebserversDemo/#2-connect-to-the-windows-instance","text":"To connect to a Windows instance, see Connecting to Your Windows Instance in the Amazon EC2 User Guide for Windows Instances. To transfer files to and from a Windows instance, see Transfer files to Windows instances in the Amazon EC2 User Guide for Windows Instances. Use remmina to connect to your instance and transfer files from ubuntu machine. Check How to create shared folder on Windows Remote Desktop from Ubuntu via Remmina .","title":"2. Connect to the Windows instance"},{"location":"WebserversDemo/#3-install-the-web-server-on-windows","text":"How to Install Apache on a Windows Server","title":"3. Install the web server on Windows"},{"location":"WebserversDemo/#running-commands-on-your-linux-instance-at-launch","text":"When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the command line tools), or as base64-encoded text (for API calls).","title":"Running commands on your Linux instance at launch"},{"location":"WebserversDemo/#specify-instance-user-data-at-launch","text":"You can specify instance user data when you launch the instance. If the root volume of the instance is an EBS volume, you can also stop the instance and update its user data. Follow the procedure for launching an instance at Launching an instance using the Launch Instance Wizard , but when you get to Step 3: Configure Instance Details in that procedure, copy your shell script in the User data field, and then complete the launch procedure. In the example script below, the script creates and configures our web server. #!/bin/bash yum update -y amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2 yum install -y httpd mariadb-server systemctl start httpd systemctl enable httpd usermod -a -G apache ec2-user chown -R ec2-user:apache /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} \\; find /var/www -type f -exec chmod 0664 {} \\; echo \"<?php phpinfo(); ?>\" > /var/www/html/phpinfo.php Allow enough time for the instance to launch and execute the commands in your script, and then check to see that your script has completed the tasks that you intended. For our example, in a web browser, enter the URL of the PHP test file the script created. This URL is the public DNS address of your instance followed by a forward slash and the file name. http://my.public.dns.amazonaws.com/phpinfo.php You should see the PHP information page. If you are unable to see the PHP information page, check that the security group you are using contains a rule to allow HTTP (port 80) traffic. For more information, see Adding rules to a security group . (Optional) If your script did not accomplish the tasks you were expecting it to, or if you just want to verify that your script completed without errors, examine the cloud-init output log file at /var/log/cloud-init-output.log and look for error messages in the output.","title":"Specify instance user data at launch"},{"location":"WebserversDemo/#view-and-update-the-instance-user-data","text":"To modify instance user data Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the navigation pane, choose Instances . Select the instance and choose Actions, Instance State, Stop . /!\\ Warning When you stop an instance, the data on any instance store volumes is erased. To keep data from instance store volumes, be sure to back it up to persistent storage. When prompted for confirmation, choose Yes, Stop . It can take a few minutes for the instance to stop. With the instance still selected, choose Actions, Instance Settings, View/Change User Data . You can\u2019t change the user data if the instance is running, but you can view it. In the View/Change User Data dialog box, update the user data, and then choose Save. Restart the instance. The new user data is visible on your instance after you restart it; however, user data scripts are not executed.","title":"View and update the instance user data"},{"location":"WebserversDemo/#running-commands-on-your-windows-instance-at-launch","text":"For information about running commands on your Windows instance at launch, see Running Commands on Your Windows Instance at Launch and Managing Windows Instance Configuration in the Amazon EC2 User Guide for Windows Instances.","title":"Running commands on your Windows instance at launch"},{"location":"WebserversDemo/#more-complex-automation-scenarios-during-the-launching-of-instances","text":"If you are interested in more complex automation scenarios, consider using AWS CloudFormation and AWS OpsWorks. For more information, see the AWS CloudFormation User Guide and the AWS OpsWorks User Guide .","title":"More complex automation scenarios during the launching of instances"},{"location":"aroura/","text":"Amazon Aurora \u00b6 MySQL and PostgreSQL-compatible relational database built for the cloud. Performance and availability of commercial-grade databases at 1/10th the cost. Overview \u00b6 Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases. It provides the security, availability, and reliability of commercial databases at 1/10th the cost. Amazon Aurora is fully managed by Amazon Relational Database Service (RDS), which automates time-consuming administration tasks like hardware provisioning, database setup, patching, and backups. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs). Benefits \u00b6 High Performance and Scalability High Availability and Durability Highly Secure MySQL and PostgreSQL Compatible Fully Managed Migration Support Use cases \u00b6 Enterprise Applications Software as a Service (SaaS) Applications Web and Mobile Gaming Migrating Your Databases to Amazon Aurora \u00b6 If you\u2019re currently using Amazon RDS for MySQL or Amazon RDS for PostgreSQL\u2026 \u00b6 If you\u2019re currently using Amazon RDS for MySQL or Amazon RDS for PostgreSQL, migrating to Aurora is as simple as creating a snapshot and launching an Aurora instance from that snapshot. You can follow the simple, step by step instructions in the user guide to perform the migration. Since Amazon Aurora is fully MySQL- and PostgreSQL-compatible, your applications can easily be reconnected to the new instance without any changes. MySQL and PostgreSQL databases running on Amazon EC2 or on-premises can also be easily migrated. Create a snapshot backup of your existing database, upload it to Amazon S3, and use it to directly create an Amazon Aurora cluster. You can also import data stored in an Amazon S3 bucket into a table in an Amazon Aurora database. Standard MySQL import and export tools or MySQL binlog replication are also supported. Migrating to Amazon Aurora from supported databases running on Amazon EC2 or on-premises can also be done using AWS Database Migration Service. If you\u2019re currently using Oracle or Microsoft SQL Server\u2026 \u00b6 The AWS Schema Conversion Tool simplifies migration from Oracle and Microsoft SQL Server to Amazon Aurora by automatically converting the source database schema and a majority of the custom code - including views, stored procedures, and functions - to a format compatible with Amazon Aurora. Any code that cannot be automatically converted is clearly marked so that it can be manually converted. Learn more and download AWS Schema Conversion Tool \u00bb Migrating data from Oracle and Microsoft SQL Server databases to Amazon Aurora can be easily done using AWS Database Migration Service. You can begin a data migration with just a few clicks, and your source database remains fully operational during the migration, minimizing downtime to applications using that database. Learn more about AWS Database Migration Service \u00bb Getting started with Amazon Aurora \u00b6 This section shows you how to create and connect to an Aurora DB cluster using Amazon RDS. These procedures are tutorials that demonstrate the basics of getting started with Aurora. Subsequent sections introduce more advanced Aurora concepts and procedures, such as the different kinds of endpoints and how to scale Aurora clusters up and down. Start by Setting up your environment for Amazon Aurora Then, check: Creating a DB cluster and connecting to a database on an Aurora MySQL DB cluster Creating a DB cluster and connecting to a database on an Aurora PostgreSQL DB cluster Tutorial: Create a web server and an Amazon Aurora DB cluster","title":"Amazon Aurora"},{"location":"aroura/#amazon-aurora","text":"MySQL and PostgreSQL-compatible relational database built for the cloud. Performance and availability of commercial-grade databases at 1/10th the cost.","title":"Amazon Aurora"},{"location":"aroura/#overview","text":"Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases. It provides the security, availability, and reliability of commercial databases at 1/10th the cost. Amazon Aurora is fully managed by Amazon Relational Database Service (RDS), which automates time-consuming administration tasks like hardware provisioning, database setup, patching, and backups. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).","title":"Overview"},{"location":"aroura/#benefits","text":"High Performance and Scalability High Availability and Durability Highly Secure MySQL and PostgreSQL Compatible Fully Managed Migration Support","title":"Benefits"},{"location":"aroura/#use-cases","text":"Enterprise Applications Software as a Service (SaaS) Applications Web and Mobile Gaming","title":"Use cases"},{"location":"aroura/#migrating-your-databases-to-amazon-aurora","text":"","title":"Migrating Your Databases to Amazon Aurora"},{"location":"aroura/#if-youre-currently-using-amazon-rds-for-mysql-or-amazon-rds-for-postgresql","text":"If you\u2019re currently using Amazon RDS for MySQL or Amazon RDS for PostgreSQL, migrating to Aurora is as simple as creating a snapshot and launching an Aurora instance from that snapshot. You can follow the simple, step by step instructions in the user guide to perform the migration. Since Amazon Aurora is fully MySQL- and PostgreSQL-compatible, your applications can easily be reconnected to the new instance without any changes. MySQL and PostgreSQL databases running on Amazon EC2 or on-premises can also be easily migrated. Create a snapshot backup of your existing database, upload it to Amazon S3, and use it to directly create an Amazon Aurora cluster. You can also import data stored in an Amazon S3 bucket into a table in an Amazon Aurora database. Standard MySQL import and export tools or MySQL binlog replication are also supported. Migrating to Amazon Aurora from supported databases running on Amazon EC2 or on-premises can also be done using AWS Database Migration Service.","title":"If you're currently using Amazon RDS for MySQL or Amazon RDS for PostgreSQL..."},{"location":"aroura/#if-youre-currently-using-oracle-or-microsoft-sql-server","text":"The AWS Schema Conversion Tool simplifies migration from Oracle and Microsoft SQL Server to Amazon Aurora by automatically converting the source database schema and a majority of the custom code - including views, stored procedures, and functions - to a format compatible with Amazon Aurora. Any code that cannot be automatically converted is clearly marked so that it can be manually converted. Learn more and download AWS Schema Conversion Tool \u00bb Migrating data from Oracle and Microsoft SQL Server databases to Amazon Aurora can be easily done using AWS Database Migration Service. You can begin a data migration with just a few clicks, and your source database remains fully operational during the migration, minimizing downtime to applications using that database. Learn more about AWS Database Migration Service \u00bb","title":"If you're currently using Oracle or Microsoft SQL Server..."},{"location":"aroura/#getting-started-with-amazon-aurora","text":"This section shows you how to create and connect to an Aurora DB cluster using Amazon RDS. These procedures are tutorials that demonstrate the basics of getting started with Aurora. Subsequent sections introduce more advanced Aurora concepts and procedures, such as the different kinds of endpoints and how to scale Aurora clusters up and down. Start by Setting up your environment for Amazon Aurora Then, check: Creating a DB cluster and connecting to a database on an Aurora MySQL DB cluster Creating a DB cluster and connecting to a database on an Aurora PostgreSQL DB cluster Tutorial: Create a web server and an Amazon Aurora DB cluster","title":"Getting started with Amazon Aurora"},{"location":"awsStore/","text":"Storage on Amazon Web Services \u00b6 Storage is a large part of every enterprise architecture. Building and maintaining your own storage repository is complex, expensive, and time-consuming. Like computing, you don\u2019t want to underprovision or overprovision for your storage needs. And as your application grows, so does the amount of data that comes along with it. You want to ensure that you\u2019re prepared for this type of change in storage demands. AWS offers a complete range of cloud storage services to support both: application requirements, as well as archival and compliance requirements. AWS storage options enable customers to store and access their data over the internet in: a durable, reliable, and cost-effective manner. Storage Scenario \u00b6 When you have a corporate address book application which is hosted on a web page. This application has many types of data/information, such as static and dynamic content. There are images for each contact, as well as information about each contact, like their name and location. The images, files, and videos in such applications should be stored in a storage volume such as Amazon S3. And information for each contact, like their name and location, is coming from Amazon Relational Database Service, or Amazon RDS. In addition to this, we also need to store the static content that is the actual HTML document for the site. So why are we storing images in S3, but contact in RDS? To answer this question, we come up with a classification for the storage on AWS: object-level storage block-level storage S3 is what we call object-level storage, whereas RDS runs on block-level storage. [Object vs. block]-level Storage \u00b6 Object-level strorage works like this, If you have an image, like what we have in the previous scenario, and you want to update that image, you have to update the entire file . So the whole file\u2019s going to change. In contrast, databases like Amazon RDS run on top of block-level storage. How this works is if we wanted to change the location for a contact, we could just change the corresponding blocks. We do not need to update the entire data file for every single change. For object storage, we use Amazon S3. This provides highly durable and scalable stores for items like images, videos, text files, and more. Storage for databases and EC2 instances use block-level storage ,like Amazon Elastic Block Storage, or EBS. For file storage and shared file systems , Amazon offers Amazon Elastic File System, or Amazon EFS. Storage Object-level Block-level Formate Images, files, and videos Distributed files, and Databases Service(s) Amazon S3 EC2 instance store, Amazon EBS, and Amazon EFS","title":"Storage on AWS"},{"location":"awsStore/#storage-on-amazon-web-services","text":"Storage is a large part of every enterprise architecture. Building and maintaining your own storage repository is complex, expensive, and time-consuming. Like computing, you don\u2019t want to underprovision or overprovision for your storage needs. And as your application grows, so does the amount of data that comes along with it. You want to ensure that you\u2019re prepared for this type of change in storage demands. AWS offers a complete range of cloud storage services to support both: application requirements, as well as archival and compliance requirements. AWS storage options enable customers to store and access their data over the internet in: a durable, reliable, and cost-effective manner.","title":"Storage on Amazon Web Services"},{"location":"awsStore/#storage-scenario","text":"When you have a corporate address book application which is hosted on a web page. This application has many types of data/information, such as static and dynamic content. There are images for each contact, as well as information about each contact, like their name and location. The images, files, and videos in such applications should be stored in a storage volume such as Amazon S3. And information for each contact, like their name and location, is coming from Amazon Relational Database Service, or Amazon RDS. In addition to this, we also need to store the static content that is the actual HTML document for the site. So why are we storing images in S3, but contact in RDS? To answer this question, we come up with a classification for the storage on AWS: object-level storage block-level storage S3 is what we call object-level storage, whereas RDS runs on block-level storage.","title":"Storage Scenario"},{"location":"awsStore/#object-vs-block-level-storage","text":"Object-level strorage works like this, If you have an image, like what we have in the previous scenario, and you want to update that image, you have to update the entire file . So the whole file\u2019s going to change. In contrast, databases like Amazon RDS run on top of block-level storage. How this works is if we wanted to change the location for a contact, we could just change the corresponding blocks. We do not need to update the entire data file for every single change. For object storage, we use Amazon S3. This provides highly durable and scalable stores for items like images, videos, text files, and more. Storage for databases and EC2 instances use block-level storage ,like Amazon Elastic Block Storage, or EBS. For file storage and shared file systems , Amazon offers Amazon Elastic File System, or Amazon EFS. Storage Object-level Block-level Formate Images, files, and videos Distributed files, and Databases Service(s) Amazon S3 EC2 instance store, Amazon EBS, and Amazon EFS","title":"[Object vs. block]-level Storage"},{"location":"contact/","text":"","title":"Contact"},{"location":"demoAutoScaling/","text":"Demo: Auto-scaling, Load-Balancing, and CloudWatch Alarms \u00b6 This demo shows how to deal with auto-scaling, balancing your application load, and how to work with the CloadWatch. In CloudWatch, the demo will show how to monitor your instances and make alerts and alarms. Network Topology \u00b6 Use the following topology and start playing with Auto-scaling EC2 instances and CloudWatch monitoring: Step 1: Create the VPC \u00b6 By using the VPC wizard, create a VPC, by one public subnets use the 10.10.0.0/16 block of addresses Create one more subnets: 1 public give each a different ip address with /24 prefix Configure the Internet gateway associate the public subnets Create a VPC security group for a public web server attach it to the created VPC add inbound rules to the security group Type: SSH Port: 20 Source: \u2018custom\u2019 or everywhere . add inbound rules to the security group Type: HTTP Port: 80 Source: everywhere . Step 2: Create the EC2 instances and install the web servers \u00b6 Launch two EC2 instances AMI: Ubuntu 16 Add the VPC put each instnace in one of the public subnet Enable the auto assigning of IPs Configure Security Group Use the public security groupe for each of the instance Review and launch use your favorit key Wait until Instance Status for your instance reads as Running before continuing. Install an Apache web server with PHP Use this script to install the web server. Step 3: Create the application Elastic Load Balancer (ELB) \u00b6 Associate the ELB with the 2 EC2 instances Choose your configured VPC Select the two availability zones Select the two public subnets Keep the default security configuration Choose the public security groupe In routing targets register the targets, the three instances Review and create, then give it some time Check your ELB after getting provisioned , use the public DNS name of the ELB to access your servers Step 4: Create and configure the Auto-Scaling \u00b6 Create the Auto-scaling group Create launch configuration AMI: ami-0c6d456c3548469dc Instance Type: t2.micro Choose your Security Group Choose your created launch configuration Configure the Auto-Scaling Groupe Group Size : 2 Change VPC and Subnets Select Load balancing Select Traget groups Configure Scaling Policies Min and Max number of instances (Instances adjustment) Average CPU Metric Target value: 60% Instances Need : 300 seconds to warm up after scaling Step 5: Stress the web servers and application \u00b6 To check the scaling we need to stress the application by: $> stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 600s This should be issued in one of the instances or through the Load.php script. Step 6: Use CloudWatch to monitor the instance Adjustment \u00b6 Go to CloudWatch, and create a board for your instances CPU utilization Monitor their average Once Utilization cross the 60% the new two instances will be launched automaticaly Once load minimzed The new instances will be idel Create more alarms for other metrics and instancces Check your target group in the load balancing Watch it here \u00b6","title":"CloudWatch and Auto-scaling Demo"},{"location":"demoAutoScaling/#demo-auto-scaling-load-balancing-and-cloudwatch-alarms","text":"This demo shows how to deal with auto-scaling, balancing your application load, and how to work with the CloadWatch. In CloudWatch, the demo will show how to monitor your instances and make alerts and alarms.","title":"Demo: Auto-scaling, Load-Balancing, and CloudWatch Alarms"},{"location":"demoAutoScaling/#network-topology","text":"Use the following topology and start playing with Auto-scaling EC2 instances and CloudWatch monitoring:","title":"Network Topology"},{"location":"demoAutoScaling/#step-1-create-the-vpc","text":"By using the VPC wizard, create a VPC, by one public subnets use the 10.10.0.0/16 block of addresses Create one more subnets: 1 public give each a different ip address with /24 prefix Configure the Internet gateway associate the public subnets Create a VPC security group for a public web server attach it to the created VPC add inbound rules to the security group Type: SSH Port: 20 Source: \u2018custom\u2019 or everywhere . add inbound rules to the security group Type: HTTP Port: 80 Source: everywhere .","title":"Step 1: Create the VPC"},{"location":"demoAutoScaling/#step-2-create-the-ec2-instances-and-install-the-web-servers","text":"Launch two EC2 instances AMI: Ubuntu 16 Add the VPC put each instnace in one of the public subnet Enable the auto assigning of IPs Configure Security Group Use the public security groupe for each of the instance Review and launch use your favorit key Wait until Instance Status for your instance reads as Running before continuing. Install an Apache web server with PHP Use this script to install the web server.","title":"Step 2: Create the EC2 instances and install the web servers"},{"location":"demoAutoScaling/#step-3-create-the-application-elastic-load-balancer-elb","text":"Associate the ELB with the 2 EC2 instances Choose your configured VPC Select the two availability zones Select the two public subnets Keep the default security configuration Choose the public security groupe In routing targets register the targets, the three instances Review and create, then give it some time Check your ELB after getting provisioned , use the public DNS name of the ELB to access your servers","title":"Step 3: Create the application Elastic Load Balancer (ELB)"},{"location":"demoAutoScaling/#step-4-create-and-configure-the-auto-scaling","text":"Create the Auto-scaling group Create launch configuration AMI: ami-0c6d456c3548469dc Instance Type: t2.micro Choose your Security Group Choose your created launch configuration Configure the Auto-Scaling Groupe Group Size : 2 Change VPC and Subnets Select Load balancing Select Traget groups Configure Scaling Policies Min and Max number of instances (Instances adjustment) Average CPU Metric Target value: 60% Instances Need : 300 seconds to warm up after scaling","title":"Step 4: Create and configure the Auto-Scaling"},{"location":"demoAutoScaling/#step-5-stress-the-web-servers-and-application","text":"To check the scaling we need to stress the application by: $> stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 600s This should be issued in one of the instances or through the Load.php script.","title":"Step 5: Stress the web servers and application"},{"location":"demoAutoScaling/#step-6-use-cloudwatch-to-monitor-the-instance-adjustment","text":"Go to CloudWatch, and create a board for your instances CPU utilization Monitor their average Once Utilization cross the 60% the new two instances will be launched automaticaly Once load minimzed The new instances will be idel Create more alarms for other metrics and instancces Check your target group in the load balancing","title":"Step 6: Use CloudWatch to monitor the instance Adjustment"},{"location":"demoAutoScaling/#watch-it-here","text":"","title":"Watch it here"},{"location":"ebsDemo/","text":"Amazon EBS Demonstrations \u00b6 Amazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated inside an Availability Zone to protect you from component failure, which offers high availability and durability. Amazon EBS volumes offer the consistent and low-latency performance that you need to run your workloads. Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: Solid State Drive (SSD)-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS) Hard Disk Drive (HDD)-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s). Note: EBS types and services quotas \u00b6 Before using the Amazon EBS block storage, you need to check the types of the EBS volumes very carfuly, by checking Amazon EBS volume types . As your account is limited on the number of EBS volumes that you can use, For more information about these limits, see Amazon EC2 Service Quotas . How to request an increase in your limits, see Requesting a quota increase . The Elastic Volume feature of Amazon EBS allows you to: dynamically increase capacity, tune performance, and change the type of live volumes with no downtime or performance impact. This allows you to easily right-size your deployment and adapt to performance changes. Pricing for Amazon EBS is based on the amount (volume) and type of Amazon EBS volume that you provision. For pricing information, see: https://aws.amazon.com/ebs/pricing/ . Confirm that you are looking at cost in the correct Region. Full details on Amazon EBS are available here: https://aws.amazon.com/ebs Important feature in Amazone EBS \u00b6 EBS provides the most common block storage you will use at AWS. When you launch your EC2 instance, you\u2019re going to need some kind of block storage to go with it. It\u2019s part of the boot volume or maybe it\u2019s a separate data volume. AWS has racks of unused storage that you can provision to sizes as large as you need up to many terabytes in size. When you launch the EC2 instance, the boot volume can attach directly to your EC2 instance, as well as the data volume. These volumes live independent of the EC2 instance themselves . In fact, they may already exist before your EC2 instance launches. When it launches, it simply finds the volume and attaches it the same way you might have an old drive from a laptop. As in the following image, The EC2 instance, when it connects to the EBS volumes, now has a direct communication to these volumes. Nobody else can talk directly to them. It\u2019s how AWS maintains secure communications at all times. The EBS volumes have a lifecycle independent of EC2. What does this mean? Let\u2019s say that this EC2 instance is part of a developer machine that over the weekend, nobody is using because your developers go home over the weekend. So, during those 48 hours, 72 hours, there\u2019s no reason to be paying for EC2 because nobody is using it. All you have to do is simply stop the instance. When you stop EC2, the EBS volumes survive. They just simply are no longer connected to the EC2 instance. See the following image. Then, Monday morning comes around, your developer starts up an EC2 instance, and a brand new instance is created. It reattaches those same EBS volumes the same way you would simply shut down your laptop over the weekend and start it again on Monday. But over the weekend, you didn\u2019t have to pay for EC2. But there\u2019s more we can take advantage of the idea that EBS lives outside of EC2 because what if I want a more powerful machine? Just like I could take your hard drives out of your existing laptop and put it in a stronger laptop, at AWS, you can simply provision a newer, bigger EC2 instance, stop the old EC2 instance, and then just attach the volumes to your brand new EC2. So, now, I\u2019ve got the same boot volume, the same applications, the same data only running newer, bigger, stronger. It\u2019s one of the many advantages EBS brings to you as part of AWS Difference between EBS and Instance Store \u00b6 EC2 instances support two types for block level storage: - EC2 Instances can be launched using either Elastic Block Store (EBS) or - Instance Store volume as root volumes and additional volumes. EBS volume is network attached drive which results in slow performance but data is persistent meaning even if you reboot the instance data will be there. Instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Note \u00b6 EC2 instances can be launched by choosing between AMIs backed by EC2 instance store and AMIs backed by EBS. However, AWS recommends use of EBS backed AMIs, because they launch faster and use persistent storage, please see the AMIs Categories . For more information, please check AWS \u2014 Difference between EBS and Instance Store . Demonestrations: Creating an Amazon EBS volume \u00b6 You can create an Amazon EBS volumes, by the following three ways: The root storage through launching instance wizard Create a new EBS volume and attached it to a new/old instance Create from a saved snapshot. Demo: EBS as a root storage for EC2 instances \u00b6 This this demo shows how to use the EBS storage as a root storage while launching your EC2 instance. Please follow the following steps: Start launching your EC2 instance, by following the Launch Instance Wizard In Step 4: Add Storage , you can change your volume size, encrypte its data, and uncheck the deletion after instance termination. If you terminate the instance, you will find this voulume exists and avialble for attachment. To attach this old volume, see Attaching an Amazon EBS volume to an instance . To mount this old volume, see Mount an attached volume . /!\\ Note Mounting an attached volume, is not the same for different types of AMIs, so Windows, not the same like Linux. To demount and de-attaching, and deleting the volume, see Releasing an Amazon EBS volume . Demo: Create an empty EBS volume and attach it to a running instance \u00b6 This this demo shows how to craete an empty volume and attach it to an EC2 instance. Please follow the following steps: Start launching your EC2 instance, by following the Launch Instance Wizard In Step 2: Choose an Instance Type , choose zour favorite availability zone , it is not agood idea to keep it randome choise at this moment. Start creating an empty EBS volume, see create empty EBS volume through the console . /!\\ Note The New empty volume and the EC2 instance have to be in the same availability zone Attaching this new volume, see Attaching an Amazon EBS volume to an instance . Mounting this new volume, see Mount an attached volume . /!\\ Note Mounting an attached volume, is not the same for different types of AMIs, so Windows, not the same like Linux. To demount and de-attaching, and deleting the volume, see Releasing an Amazon EBS volume . Demo: Create an EBS volume from a previously created snapshot and attach it to a running instance \u00b6 This this demo shows how to create an EBS volume from a snapshot and attach it to an EC2 instance, see Creating Amazon EBS snapshots . Creating a snapshot for an EBS volume \u00b6 To create a snapshot for an EBS volume, you should have already an EC2 instance running with a an EBS root volume or an exist volume, to do so see this demo , then follow the folllowing steps: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . Choose Snapshots under Elastic Block Store in the navigation pane. Choose Create Snapshot. For Select resource type , choose Volume . For Volume , select the volume . (Optional) Enter a description for the snapshot. (Optional) Choose Add Tag to add tags to your snapshot. For each tag, provide a tag key and a tag value. Choose Create Snapshot . To create a snapshot using the command line You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . create-snapshot (AWS CLI) New-EC2Snapshot (AWS Tools for Windows PowerShell) Creating a multi-volume snapshot \u00b6 Use the following procedure to create a snapshot from the volumes of an instance. To create a snapshot for multi-EBS volumes, you should have already an EC2 instance running with multiple EBS volumes, to do so see this demo , To create multi-volume snapshots using the console: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . Choose Snapshots under Elastic Block Store in the navigation pane. Choose Create Snapshot. For Select resource type , choose Instance . Select the instance ID for which you want to create simultaneous backups for all of the attached EBS volumes. Multi-volume snapshots support up to 40 EBS volumes per instance. (Optional) Set Exclude root volume . (Optional) Set Copy tags from volume flag to automatically copy tags from the source volume to the corresponding snapshots. This sets snapshot metadata\u2014such as access policies, attachment information, and cost allocation\u2014to match the source volume. (Optional) Choose Add Tag to add tags to your snapshot. For each tag, provide a tag key and a tag value. Choose Create Snapshot . During snapshot creation, the snapshots are managed together. If one of the snapshots in the volume set fails, the other snapshots are moved to error status for the volume set. You can monitor the progress of your snapshots using CloudWatch Events. After the snapshot creation process completes, CloudWatch generates an event that contains the status and all of the relevant snapshots details for the affected instance. To create multi-volume snapshots using the command line You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . create-snapshot (AWS CLI) New-EC2SnapshotBatch (AWS Tools for Windows PowerShell) Other operations for the volumes Snapshots \u00b6 You can copy snapshots, share snapshots, and create volumes from snapshots. For more information, see the following: Copying an Amazon EBS snapshot Sharing an Amazon EBS snapshot Creating a volume from a snapshot Creating a volume from a snapshot \u00b6 Please follow the following steps: Start launching your EC2 instance, by following the Launch Instance Wizard In Step 2: Choose an Instance Type , choose your favorite availability zone , it is not agood idea to keep it randome choise at this moment. Start creating an EBS volume from a snapshot, see Create an EBS Volume from a Snapshot . Attaching this new volume, see Attaching an Amazon EBS volume to an instance . Mounting this new volume, see Mount an attached volume . /!\\ Note Mounting an attached volume, is not the same for different types of AMIs, so Windows, not the same like Linux. To demount and de-attaching, and deleting the volume, see Releasing an Amazon EBS volume . Best practice while mounting a volume which has been created through a snapshot \u00b6 During the mount of an EBS volume that has been created from a snapshot, you may face a problem called: XFS: Filesystem has duplicate UUID \u2013 can\u2019t mount To solve this problem, you need to repair the xfs and then generate a new UUID , as follow: $> sudo xfs_repair -L /dev/xvdh $> sudo xfs_admin -U generate /dev/xvdh Please check the following, How To Solve \u201cXFS: Filesystem has duplicate UUID \u2013 can\u2019t mount\u201d , and xfs_repair(8) - Linux man page for more informtion. Demo: Attach an EBS volume to a Windows AMI instance \u00b6 Please follow the following steps: Start launching your EC2 instance, by following the Launch Instance Wizard In Step 2: Choose an Instance Type , choose your favorite availability zone , it is not agood idea to keep it randome choise at this moment. Start creating an empty EBS volume, see create empty EBS volume through the console . /!\\ Note The New empty volume and the EC2 instance have to be in the same availability zone Attaching this new volume, see Attaching an Amazon EBS volume to an instance . Mounting this new volume, see Making an Amazon EBS volume available for use on Windows . /!\\ Note Mounting an attached volume, is not the same for different types of AMIs, so Windows, not the same like Linux. To demount and de-attaching, and deleting the volume, see Releasing an Amazon EBS volume .","title":"Amazon EBS Demo"},{"location":"ebsDemo/#amazon-ebs-demonstrations","text":"Amazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated inside an Availability Zone to protect you from component failure, which offers high availability and durability. Amazon EBS volumes offer the consistent and low-latency performance that you need to run your workloads. Amazon EBS provides a range of options that allow you to optimize storage performance and cost for your workload. These options are divided into two major categories: Solid State Drive (SSD)-backed storage for transactional workloads, such as databases and boot volumes (performance depends primarily on IOPS) Hard Disk Drive (HDD)-backed storage for throughput-intensive workloads, such as MapReduce and log processing (performance depends primarily on MB/s).","title":"Amazon EBS Demonstrations"},{"location":"ebsDemo/#note-ebs-types-and-services-quotas","text":"Before using the Amazon EBS block storage, you need to check the types of the EBS volumes very carfuly, by checking Amazon EBS volume types . As your account is limited on the number of EBS volumes that you can use, For more information about these limits, see Amazon EC2 Service Quotas . How to request an increase in your limits, see Requesting a quota increase . The Elastic Volume feature of Amazon EBS allows you to: dynamically increase capacity, tune performance, and change the type of live volumes with no downtime or performance impact. This allows you to easily right-size your deployment and adapt to performance changes. Pricing for Amazon EBS is based on the amount (volume) and type of Amazon EBS volume that you provision. For pricing information, see: https://aws.amazon.com/ebs/pricing/ . Confirm that you are looking at cost in the correct Region. Full details on Amazon EBS are available here: https://aws.amazon.com/ebs","title":"Note: EBS types and services quotas"},{"location":"ebsDemo/#important-feature-in-amazone-ebs","text":"EBS provides the most common block storage you will use at AWS. When you launch your EC2 instance, you\u2019re going to need some kind of block storage to go with it. It\u2019s part of the boot volume or maybe it\u2019s a separate data volume. AWS has racks of unused storage that you can provision to sizes as large as you need up to many terabytes in size. When you launch the EC2 instance, the boot volume can attach directly to your EC2 instance, as well as the data volume. These volumes live independent of the EC2 instance themselves . In fact, they may already exist before your EC2 instance launches. When it launches, it simply finds the volume and attaches it the same way you might have an old drive from a laptop. As in the following image, The EC2 instance, when it connects to the EBS volumes, now has a direct communication to these volumes. Nobody else can talk directly to them. It\u2019s how AWS maintains secure communications at all times. The EBS volumes have a lifecycle independent of EC2. What does this mean? Let\u2019s say that this EC2 instance is part of a developer machine that over the weekend, nobody is using because your developers go home over the weekend. So, during those 48 hours, 72 hours, there\u2019s no reason to be paying for EC2 because nobody is using it. All you have to do is simply stop the instance. When you stop EC2, the EBS volumes survive. They just simply are no longer connected to the EC2 instance. See the following image. Then, Monday morning comes around, your developer starts up an EC2 instance, and a brand new instance is created. It reattaches those same EBS volumes the same way you would simply shut down your laptop over the weekend and start it again on Monday. But over the weekend, you didn\u2019t have to pay for EC2. But there\u2019s more we can take advantage of the idea that EBS lives outside of EC2 because what if I want a more powerful machine? Just like I could take your hard drives out of your existing laptop and put it in a stronger laptop, at AWS, you can simply provision a newer, bigger EC2 instance, stop the old EC2 instance, and then just attach the volumes to your brand new EC2. So, now, I\u2019ve got the same boot volume, the same applications, the same data only running newer, bigger, stronger. It\u2019s one of the many advantages EBS brings to you as part of AWS","title":"Important feature in Amazone EBS"},{"location":"ebsDemo/#difference-between-ebs-and-instance-store","text":"EC2 instances support two types for block level storage: - EC2 Instances can be launched using either Elastic Block Store (EBS) or - Instance Store volume as root volumes and additional volumes. EBS volume is network attached drive which results in slow performance but data is persistent meaning even if you reboot the instance data will be there. Instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer.","title":"Difference between EBS and Instance Store"},{"location":"ebsDemo/#note","text":"EC2 instances can be launched by choosing between AMIs backed by EC2 instance store and AMIs backed by EBS. However, AWS recommends use of EBS backed AMIs, because they launch faster and use persistent storage, please see the AMIs Categories . For more information, please check AWS \u2014 Difference between EBS and Instance Store .","title":"Note"},{"location":"ebsDemo/#demonestrations-creating-an-amazon-ebs-volume","text":"You can create an Amazon EBS volumes, by the following three ways: The root storage through launching instance wizard Create a new EBS volume and attached it to a new/old instance Create from a saved snapshot.","title":"Demonestrations: Creating an Amazon EBS volume"},{"location":"ebsDemo/#demo-ebs-as-a-root-storage-for-ec2-instances","text":"This this demo shows how to use the EBS storage as a root storage while launching your EC2 instance. Please follow the following steps: Start launching your EC2 instance, by following the Launch Instance Wizard In Step 4: Add Storage , you can change your volume size, encrypte its data, and uncheck the deletion after instance termination. If you terminate the instance, you will find this voulume exists and avialble for attachment. To attach this old volume, see Attaching an Amazon EBS volume to an instance . To mount this old volume, see Mount an attached volume . /!\\ Note Mounting an attached volume, is not the same for different types of AMIs, so Windows, not the same like Linux. To demount and de-attaching, and deleting the volume, see Releasing an Amazon EBS volume .","title":"Demo: EBS as a root storage for EC2 instances"},{"location":"ebsDemo/#demo-create-an-empty-ebs-volume-and-attach-it-to-a-running-instance","text":"This this demo shows how to craete an empty volume and attach it to an EC2 instance. Please follow the following steps: Start launching your EC2 instance, by following the Launch Instance Wizard In Step 2: Choose an Instance Type , choose zour favorite availability zone , it is not agood idea to keep it randome choise at this moment. Start creating an empty EBS volume, see create empty EBS volume through the console . /!\\ Note The New empty volume and the EC2 instance have to be in the same availability zone Attaching this new volume, see Attaching an Amazon EBS volume to an instance . Mounting this new volume, see Mount an attached volume . /!\\ Note Mounting an attached volume, is not the same for different types of AMIs, so Windows, not the same like Linux. To demount and de-attaching, and deleting the volume, see Releasing an Amazon EBS volume .","title":"Demo: Create an empty EBS volume and attach it to a running instance"},{"location":"ebsDemo/#demo-create-an-ebs-volume-from-a-previously-created-snapshot-and-attach-it-to-a-running-instance","text":"This this demo shows how to create an EBS volume from a snapshot and attach it to an EC2 instance, see Creating Amazon EBS snapshots .","title":"Demo: Create an EBS volume from a previously created snapshot and attach it to a running instance"},{"location":"ebsDemo/#creating-a-snapshot-for-an-ebs-volume","text":"To create a snapshot for an EBS volume, you should have already an EC2 instance running with a an EBS root volume or an exist volume, to do so see this demo , then follow the folllowing steps: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . Choose Snapshots under Elastic Block Store in the navigation pane. Choose Create Snapshot. For Select resource type , choose Volume . For Volume , select the volume . (Optional) Enter a description for the snapshot. (Optional) Choose Add Tag to add tags to your snapshot. For each tag, provide a tag key and a tag value. Choose Create Snapshot . To create a snapshot using the command line You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . create-snapshot (AWS CLI) New-EC2Snapshot (AWS Tools for Windows PowerShell)","title":"Creating a snapshot for an EBS volume"},{"location":"ebsDemo/#creating-a-multi-volume-snapshot","text":"Use the following procedure to create a snapshot from the volumes of an instance. To create a snapshot for multi-EBS volumes, you should have already an EC2 instance running with multiple EBS volumes, to do so see this demo , To create multi-volume snapshots using the console: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . Choose Snapshots under Elastic Block Store in the navigation pane. Choose Create Snapshot. For Select resource type , choose Instance . Select the instance ID for which you want to create simultaneous backups for all of the attached EBS volumes. Multi-volume snapshots support up to 40 EBS volumes per instance. (Optional) Set Exclude root volume . (Optional) Set Copy tags from volume flag to automatically copy tags from the source volume to the corresponding snapshots. This sets snapshot metadata\u2014such as access policies, attachment information, and cost allocation\u2014to match the source volume. (Optional) Choose Add Tag to add tags to your snapshot. For each tag, provide a tag key and a tag value. Choose Create Snapshot . During snapshot creation, the snapshots are managed together. If one of the snapshots in the volume set fails, the other snapshots are moved to error status for the volume set. You can monitor the progress of your snapshots using CloudWatch Events. After the snapshot creation process completes, CloudWatch generates an event that contains the status and all of the relevant snapshots details for the affected instance. To create multi-volume snapshots using the command line You can use one of the following commands. For more information about these command line interfaces, see Accessing Amazon EC2 . create-snapshot (AWS CLI) New-EC2SnapshotBatch (AWS Tools for Windows PowerShell)","title":"Creating a multi-volume snapshot"},{"location":"ebsDemo/#other-operations-for-the-volumes-snapshots","text":"You can copy snapshots, share snapshots, and create volumes from snapshots. For more information, see the following: Copying an Amazon EBS snapshot Sharing an Amazon EBS snapshot Creating a volume from a snapshot","title":"Other operations for the volumes Snapshots"},{"location":"ebsDemo/#creating-a-volume-from-a-snapshot","text":"Please follow the following steps: Start launching your EC2 instance, by following the Launch Instance Wizard In Step 2: Choose an Instance Type , choose your favorite availability zone , it is not agood idea to keep it randome choise at this moment. Start creating an EBS volume from a snapshot, see Create an EBS Volume from a Snapshot . Attaching this new volume, see Attaching an Amazon EBS volume to an instance . Mounting this new volume, see Mount an attached volume . /!\\ Note Mounting an attached volume, is not the same for different types of AMIs, so Windows, not the same like Linux. To demount and de-attaching, and deleting the volume, see Releasing an Amazon EBS volume .","title":"Creating a volume from a snapshot"},{"location":"ebsDemo/#best-practice-while-mounting-a-volume-which-has-been-created-through-a-snapshot","text":"During the mount of an EBS volume that has been created from a snapshot, you may face a problem called: XFS: Filesystem has duplicate UUID \u2013 can\u2019t mount To solve this problem, you need to repair the xfs and then generate a new UUID , as follow: $> sudo xfs_repair -L /dev/xvdh $> sudo xfs_admin -U generate /dev/xvdh Please check the following, How To Solve \u201cXFS: Filesystem has duplicate UUID \u2013 can\u2019t mount\u201d , and xfs_repair(8) - Linux man page for more informtion.","title":"Best practice while mounting a volume which has been created through a snapshot"},{"location":"ebsDemo/#demo-attach-an-ebs-volume-to-a-windows-ami-instance","text":"Please follow the following steps: Start launching your EC2 instance, by following the Launch Instance Wizard In Step 2: Choose an Instance Type , choose your favorite availability zone , it is not agood idea to keep it randome choise at this moment. Start creating an empty EBS volume, see create empty EBS volume through the console . /!\\ Note The New empty volume and the EC2 instance have to be in the same availability zone Attaching this new volume, see Attaching an Amazon EBS volume to an instance . Mounting this new volume, see Making an Amazon EBS volume available for use on Windows . /!\\ Note Mounting an attached volume, is not the same for different types of AMIs, so Windows, not the same like Linux. To demount and de-attaching, and deleting the volume, see Releasing an Amazon EBS volume .","title":"Demo: Attach an EBS volume to a Windows AMI instance"},{"location":"lightsail/","text":"Amazon Web Services Lightsail \u00b6 Amazon Lightsail is the easiest way to get started with AWS for developers, small businesses, students, and other users who need a simple virtual private server (VPS) solution. Lightsail provides developers compute, storage, and networking capacity, and it also provides capabilities to deploy and manage websites and web applications in the cloud. Lightsail includes everything you need to launch your project quickly\u2013a virtual machine, solid state drive (SSD)-based storage, data transfer, Domain Name System (DNS) management, and a static IP\u2013for a low, predictable monthly price. Lightsail Overview \u00b6 For more information about AWS lightsail , please check: Lightsail features Lightsail pricing Lightsail Operating system & application templates \u00b6 Lightsail offers a number of preconfigured, one-click-to-launch application or developer stacks, including WordPress, Plesk, LAMP, Node.js and more. Simple operating systems are also available, including Amazon Linux, Windows Server, Ubuntu, CentOS, and more. Operating systems \u00b6 Applications \u00b6 Stacks \u00b6 Lightsail Demonistration \u00b6 All you have to do is log into your account, and right here on the homepage, you\u2019re going to be able to go and build with Lightsail just by clicking on the Lightsail login. Now, the nice thing with Lightsail is designed to step you easily through the different pieces that you\u2019re going to need without any technical explanation. Please folllow: The first thing that it\u2019s going to ask you is what type of application do you want to run on Lightsail? Now, you could run a WordPress application or maybe get into one of the other more complicated elements. For example, let\u2019s say you want to run a LAMP stack or run a Drupal server. These become easy choices once you make them. Decide how big of an engine do you want to run? Do you want it to have a high volume? High velocity? Or just a nice simple starting one? Got to give it a name. Waita bit, it\u2019s already running and we can go ahead and click on that WordPress site. Use the public IP address, open up a new tab to see the hello world example from wordpress. Watch more detailed information about Lightsail here \u00b6","title":"AWS Lightsail Services"},{"location":"lightsail/#amazon-web-services-lightsail","text":"Amazon Lightsail is the easiest way to get started with AWS for developers, small businesses, students, and other users who need a simple virtual private server (VPS) solution. Lightsail provides developers compute, storage, and networking capacity, and it also provides capabilities to deploy and manage websites and web applications in the cloud. Lightsail includes everything you need to launch your project quickly\u2013a virtual machine, solid state drive (SSD)-based storage, data transfer, Domain Name System (DNS) management, and a static IP\u2013for a low, predictable monthly price.","title":"Amazon Web Services Lightsail"},{"location":"lightsail/#lightsail-overview","text":"For more information about AWS lightsail , please check: Lightsail features Lightsail pricing","title":"Lightsail Overview"},{"location":"lightsail/#lightsail-operating-system-application-templates","text":"Lightsail offers a number of preconfigured, one-click-to-launch application or developer stacks, including WordPress, Plesk, LAMP, Node.js and more. Simple operating systems are also available, including Amazon Linux, Windows Server, Ubuntu, CentOS, and more.","title":"Lightsail Operating system &amp; application templates"},{"location":"lightsail/#operating-systems","text":"","title":"Operating systems"},{"location":"lightsail/#applications","text":"","title":"Applications"},{"location":"lightsail/#stacks","text":"","title":"Stacks"},{"location":"lightsail/#lightsail-demonistration","text":"All you have to do is log into your account, and right here on the homepage, you\u2019re going to be able to go and build with Lightsail just by clicking on the Lightsail login. Now, the nice thing with Lightsail is designed to step you easily through the different pieces that you\u2019re going to need without any technical explanation. Please folllow: The first thing that it\u2019s going to ask you is what type of application do you want to run on Lightsail? Now, you could run a WordPress application or maybe get into one of the other more complicated elements. For example, let\u2019s say you want to run a LAMP stack or run a Drupal server. These become easy choices once you make them. Decide how big of an engine do you want to run? Do you want it to have a high volume? High velocity? Or just a nice simple starting one? Got to give it a name. Waita bit, it\u2019s already running and we can go ahead and click on that WordPress site. Use the public IP address, open up a new tab to see the hello world example from wordpress.","title":"Lightsail Demonistration"},{"location":"lightsail/#watch-more-detailed-information-about-lightsail-here","text":"","title":"Watch more detailed information about Lightsail here"},{"location":"monitor/","text":"Monitoring AWS services \u00b6 Modern applications are distributed in nature, and are often following a service-oriented or microservices architecture. Components of this system will generate data in the form of metrics, logs, and more , but all of these data sources are distributed throughout your environment. With any system, to support operations, you have to have insight into the current state of the infrastructure and applications. You need a way to collect these data points and analyze them to help operate daily, and improve your operations over time. Your network will generate data, like flow logs. The servers will be generating data, such as how much CPU is currently being used, and your database layer will generate data, such as number of simultaneous connections. How can you collect this data in one centralized location, so you can actively monitor and operate your system? With Amazon CloudWatch , you can monitor your cloud infrastructure intelligently. CloudWatch will collect data from your cloud-based infrastructure in one centralized location. With this data, you can create statistics, which drive operational procedures using features such as: CloudWatch Alarms CloudWatch Events CloudWatch Logs Metrics \u2013 Let\u2019s consider this situation. You get reports from your user base that they\u2019re noticing a latency in your services. Since you now have been made aware of an issue, you can start looking at your system to figure out what the cause of the issue is. Upon investigation, you realize that the CPU load is around 95 percent across all of your server instances. \u2013 On-premises, to fix this, you would page an admin, who then launches another server to help bring the load down across the servers. The problem with this approach, manually responding to a problem, is that your users experience interruption before you can mitigate the disruption to your services. Wouldn\u2019t it be nice if we could instead anticipate and mitigate the disruption well before any of our customers are impacted? With Amazon CloudWatch, you can configure metrics and alarms to do just that. - CloudWatch collects data points over time and provides you with statistics about your infrastructure. So, you can then create alarms and trigger automated events based on those statistics. \u2013 Consider our previous example, instead of waiting until hearing about latency from users, instead, you can create an alarm that says, if a group of EC2 instances has CPU over 60 percent for more than 5 minutes, trigger an auto scaling policy, automatically launch a new server into the pool, and alert an admin. \u2013 The visibility into the CPU across EC2 instances, is what allows you to anticipate that your users will soon be experiencing latency if you don\u2019t act. CloudWatch alarms can be configured for more than just CPU utilization. You can choose from hundreds of different metrics, or create your own custom metrics for alarms that matter to you. In addition to alarms, CloudWatch also allows you to visualize the statistics about your environment through dashboards. - You can use out-of-the-box dashboards to view built-in or custom metrics, and - you can build your own custom dashboards or partner with a wide range of consultants through the Amazon Partner Network. \u2013 When building custom metrics, consider the data being generated in your log files. Systems generate different types of log files that have a wealth of information about system performance and error rates. CloudWatch Logs is a place where; you can send your log files for viewing in the console, and to stream to other services, like Lambda or Amazon Elasticsearch for analytics. CloudWatch is integrated with over 70 AWS services. You can use CloudWatch to : set alarms, visualize logs and metrics side-by-side, take automated actions, troubleshoot operational issues, and discover insights to optimize your applications. Getting started with CloudWatch is easy and there\u2019s no up-front commitment or minimum fee. For more information, visit aws.amazon.com/cloudwatch . Amazon CloudWatch Components \u00b6 Amazon CloudWatch is a monitoring service for AWS Cloud resources and the applications that you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources. Amazon CloudWatch Events \u00b6 Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in AWS resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. CloudWatch Events becomes aware of operational changes as they occur. Amazon CloudWatch Logs Metrics \u00b6 You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon EC2 instances, AWS CloudTrail, Amazon Route 53, and other sources. You can then retrieve the associated log data from CloudWatch Logs. You can collect metrics from servers by installing the CloudWatch agent on the server. You can install the agent on both Amazon EC2 instances and on-premises servers, and on servers that run either Linux or Windows Server. Amazon CloudWatch Alarms \u00b6 You can use Amazon CloudWatch Alarms to create alarms which monitor your servcies and track the events when happen to triger the alarms. There are two alarms in CloudWatch: metric alarms, and composite alarms A etric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Auto Scaling action, or creating a Systems Manager OpsItem. A composite alarm includes a rule expression that takes into account the alarm states of other alarms that you have created. The composite alarm goes into ALARM state only if all conditions of the rule are met. The alarms specified in a composite alarm\u2019s rule expression can include metric alarms and other composite alarms. Using composite alarms can reduce alarm noise. You can create multiple metric alarms, and also create a composite alarm and set up alerts only for the composite alarm. For example, a composite might go into ALARM state only when all of the underlying metric alarms are in ALARM state. Composite alarms can send Amazon SNS notifications when they change state, and can create Systems Manager OpsItems when they go into ALARM state, but can\u2019t perform EC2 actions or Auto Scaling actions. \u2013 You can add alarms to CloudWatch dashboards and monitor them visually. When an alarm is on a dashboard, it turns red when it is in the ALARM state, making it easier for you to monitor its status proactively. \u2013 An alarm invokes actions only when the alarm changes state. The exception is for alarms with Auto Scaling actions. For Auto Scaling actions, the alarm continues to invoke the action once per minute that the alarm remains in the new state. /!\\ Note CloudWatch doesn't test or validate the actions that you specify, nor does it detect any Amazon EC2 Auto Scaling or Amazon SNS errors resulting from an attempt to invoke nonexistent actions. Make sure that your alarm actions exist. Amazon CloudWatch Foundation: Observability of your AWS resources and applications on AWS and on-premises \u00b6 Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications, and services that run on AWS and on-premises servers. You can use CloudWatch to detect anomalous behavior in your environments, set alarms, visualize logs and metrics side by side, take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly. CloudWatch Benefits \u00b6 Observability on a single platform across applications and infrastructure Easiest way to collect metrics in AWS and on-premises Improve operational performance and resource optimization Get operational visibility and insight Derive actionable insights from logs CloudWatch Use cases \u00b6 Infrastructure monitoring and troubleshooting Mean-time-to-resolution improvement Proactive resource optimization Application monitoring Log analytics How CloudWatch works? \u00b6 CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, and visualizes it using automated dashboards so you can get a unified view of your AWS resources, applications, and services that run in AWS and on-premises. You can correlate your metrics and logs to better understand the health and performance of your resources. You can also create alarms based on metric value thresholds you specify, or that can watch for anomalous metric behavior based on machine learning algorithms. To take action quickly, you can set up automated actions to notify you if an alarm is triggered and automatically start auto scaling, for example, to help reduce mean-time-to-resolution. You can also dive deep and analyze your metrics, logs, and traces, to better understand how to improve application performance. Amazon CloudWatch Features \u00b6 Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. With CloudWatch, you can collect and access all your performance and operational data in form of logs and metrics from a single platform. This allows you to overcome the challenge of monitoring individual systems and applications in silos (server, network, database, etc.). CloudWatch enables you to monitor your complete stack (applications, infrastructure, and services) and leverage alarms, logs, and events data to take automated actions and reduce Mean Time to Resolution (MTTR). This frees up important resources and allows you to focus on building applications and business value. CloudWatch gives you actionable insights that help you optimize application performance, manage resource utilization, and understand system-wide operational health. CloudWatch provides up to 1-second visibility of metrics and logs data, 15 months of data retention (metrics), and the ability to perform calculations on metrics. This allows you to perform historical analysis for cost optimization and derive real-time insights into optimizing applications and infrastructure resources. You can use CloudWatch Container Insights to monitor, troubleshoot, and alarm on your containerized applications and microservices. CloudWatch collects, aggregates, and summarizes compute utilization information like CPU, memory, disk, and network data, as well as diagnostic information like container restart failures, to help DevOps engineers isolate issues and resolve them quickly. Container Insights gives you insights from container management services such as Amazon ECS for Kubernetes (EKS), Amazon\u2019s Elastic Container Service (ECS), AWS Fargate, and standalone Kubernetes (k8s). Feature 1. Collect \u00b6 Easily collect and store logs Built-in metrics Custom Metrics Collect and aggregate container metrics and logs Collect and aggregate Lambda metrics and logs Feature 2. Monitor \u00b6 Unified operational view with dashboards High resolution alarms Logs and metrics correlation Application Insights for .NET and SQL Server applications Container monitoring insights Lambda monitoring insights Anomaly Detection ServiceLens Synthetics Feature 3. Act \u00b6 Auto Scaling Automate response to operational changes with CloudWatch Events Alarm and automate actions on EKS, ECS, and k8s clusters Feature 4. Analyze \u00b6 Granular data and extended retention Custom operations on metrics Log analytics Analyze container metrics, logs, and traces Analyze Lambda metrics, logs, and traces Contributor Insights Compliance and Security \u00b6 Amazon CloudWatch is integrated with AWS Identity and Access Management (IAM) so that you can control which users and resources have permission to access your data and how they can access it. Amazon CloudWatch Logs is also PCI and FedRamp compliant. Data is encrypted at rest and during transfer. You can also use AWS KMS encryption to encrypt your log groups for added compliance and security. Amazon CloudWatch pricing \u00b6 With Amazon CloudWatch, there is no up-front commitment or minimum fee; you simply pay for what you use. You will be charged at the end of the month for your usage. Free tier \u00b6 You can get started with Amazon CloudWatch for free. Most AWS Services (EC2, S3, Kinesis, etc.) vend metrics automatically for free to CloudWatch. Many applications should be able to operate within these free tier limits. You can learn more about AWS Free Tier here . CloudWatch AWS Free Tier Metrics Basic Monitoring Metrics (at 5-minute frequency) 10 Detailed Monitoring Metrics (at 1-minute frequency) 1 Million API requests (not applicable to GetMetricData and GetMetricWidgetImage) Dashboard 3 Dashboards for up to 50 metrics per month Alarms 10 Alarm metrics (not applicable to high-resolution alarms) Logs 5GB Data (ingestion, archive storage, and data scanned by Logs Insights queries) Events All events except custom events are included Contributor Insights 1 Contributor Insights rule per month The first one million log events that match the rule per month Synthetics 100 canary runs per month Paid tier \u00b6 There is no up-front commitment or minimum fee. You simply pay for what you use and will be charged at the end of the month for your usage. Check the AWS CloudWatch prices from here . Getting started with Amazon CloudWatch \u00b6 Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/ . The CloudWatch overview home page appears. See, Getting Started with Amazon CloudWatch Chech how to get started \u00b6 Getting started with Amazon CloudWatch Getting Started with Amazon CloudWatch Events Getting Started with CloudWatch Logs Check how to use CloudWach Components \u00b6 Using Amazon CloudWatch Dashboards Using Amazon CloudWatch Metrics Using Amazon CloudWatch Alarms Using synthetic monitoring Check Tutorials \u00b6 CloudWatch Events Tutorials Analyzing Log Data with CloudWatch Logs Insights CloudWatch Tutorials: Scenario: Monitor Your Estimated Charges Using CloudWatch Scenario: Publish Metrics to CloudWatch Check the services Monitoring \u00b6 Monitoring Amazon EC2 Amazon EBS volume performance on Linux instances Monitoring Amazon EFS Monitoring Amazon S3 Monitoring an Amazon RDS DB instance Using Container Insights Getting started with Lambda Insights","title":"Monitoring Services on AWS"},{"location":"monitor/#monitoring-aws-services","text":"Modern applications are distributed in nature, and are often following a service-oriented or microservices architecture. Components of this system will generate data in the form of metrics, logs, and more , but all of these data sources are distributed throughout your environment. With any system, to support operations, you have to have insight into the current state of the infrastructure and applications. You need a way to collect these data points and analyze them to help operate daily, and improve your operations over time. Your network will generate data, like flow logs. The servers will be generating data, such as how much CPU is currently being used, and your database layer will generate data, such as number of simultaneous connections. How can you collect this data in one centralized location, so you can actively monitor and operate your system? With Amazon CloudWatch , you can monitor your cloud infrastructure intelligently. CloudWatch will collect data from your cloud-based infrastructure in one centralized location. With this data, you can create statistics, which drive operational procedures using features such as: CloudWatch Alarms CloudWatch Events CloudWatch Logs Metrics \u2013 Let\u2019s consider this situation. You get reports from your user base that they\u2019re noticing a latency in your services. Since you now have been made aware of an issue, you can start looking at your system to figure out what the cause of the issue is. Upon investigation, you realize that the CPU load is around 95 percent across all of your server instances. \u2013 On-premises, to fix this, you would page an admin, who then launches another server to help bring the load down across the servers. The problem with this approach, manually responding to a problem, is that your users experience interruption before you can mitigate the disruption to your services. Wouldn\u2019t it be nice if we could instead anticipate and mitigate the disruption well before any of our customers are impacted? With Amazon CloudWatch, you can configure metrics and alarms to do just that. - CloudWatch collects data points over time and provides you with statistics about your infrastructure. So, you can then create alarms and trigger automated events based on those statistics. \u2013 Consider our previous example, instead of waiting until hearing about latency from users, instead, you can create an alarm that says, if a group of EC2 instances has CPU over 60 percent for more than 5 minutes, trigger an auto scaling policy, automatically launch a new server into the pool, and alert an admin. \u2013 The visibility into the CPU across EC2 instances, is what allows you to anticipate that your users will soon be experiencing latency if you don\u2019t act. CloudWatch alarms can be configured for more than just CPU utilization. You can choose from hundreds of different metrics, or create your own custom metrics for alarms that matter to you. In addition to alarms, CloudWatch also allows you to visualize the statistics about your environment through dashboards. - You can use out-of-the-box dashboards to view built-in or custom metrics, and - you can build your own custom dashboards or partner with a wide range of consultants through the Amazon Partner Network. \u2013 When building custom metrics, consider the data being generated in your log files. Systems generate different types of log files that have a wealth of information about system performance and error rates. CloudWatch Logs is a place where; you can send your log files for viewing in the console, and to stream to other services, like Lambda or Amazon Elasticsearch for analytics. CloudWatch is integrated with over 70 AWS services. You can use CloudWatch to : set alarms, visualize logs and metrics side-by-side, take automated actions, troubleshoot operational issues, and discover insights to optimize your applications. Getting started with CloudWatch is easy and there\u2019s no up-front commitment or minimum fee. For more information, visit aws.amazon.com/cloudwatch .","title":"Monitoring AWS services"},{"location":"monitor/#amazon-cloudwatch-components","text":"Amazon CloudWatch is a monitoring service for AWS Cloud resources and the applications that you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources.","title":"Amazon CloudWatch Components"},{"location":"monitor/#amazon-cloudwatch-events","text":"Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in AWS resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. CloudWatch Events becomes aware of operational changes as they occur.","title":"Amazon CloudWatch Events"},{"location":"monitor/#amazon-cloudwatch-logs-metrics","text":"You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon EC2 instances, AWS CloudTrail, Amazon Route 53, and other sources. You can then retrieve the associated log data from CloudWatch Logs. You can collect metrics from servers by installing the CloudWatch agent on the server. You can install the agent on both Amazon EC2 instances and on-premises servers, and on servers that run either Linux or Windows Server.","title":"Amazon CloudWatch Logs Metrics"},{"location":"monitor/#amazon-cloudwatch-alarms","text":"You can use Amazon CloudWatch Alarms to create alarms which monitor your servcies and track the events when happen to triger the alarms. There are two alarms in CloudWatch: metric alarms, and composite alarms A etric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Auto Scaling action, or creating a Systems Manager OpsItem. A composite alarm includes a rule expression that takes into account the alarm states of other alarms that you have created. The composite alarm goes into ALARM state only if all conditions of the rule are met. The alarms specified in a composite alarm\u2019s rule expression can include metric alarms and other composite alarms. Using composite alarms can reduce alarm noise. You can create multiple metric alarms, and also create a composite alarm and set up alerts only for the composite alarm. For example, a composite might go into ALARM state only when all of the underlying metric alarms are in ALARM state. Composite alarms can send Amazon SNS notifications when they change state, and can create Systems Manager OpsItems when they go into ALARM state, but can\u2019t perform EC2 actions or Auto Scaling actions. \u2013 You can add alarms to CloudWatch dashboards and monitor them visually. When an alarm is on a dashboard, it turns red when it is in the ALARM state, making it easier for you to monitor its status proactively. \u2013 An alarm invokes actions only when the alarm changes state. The exception is for alarms with Auto Scaling actions. For Auto Scaling actions, the alarm continues to invoke the action once per minute that the alarm remains in the new state. /!\\ Note CloudWatch doesn't test or validate the actions that you specify, nor does it detect any Amazon EC2 Auto Scaling or Amazon SNS errors resulting from an attempt to invoke nonexistent actions. Make sure that your alarm actions exist.","title":"Amazon CloudWatch Alarms"},{"location":"monitor/#amazon-cloudwatch-foundation-observability-of-your-aws-resources-and-applications-on-aws-and-on-premises","text":"Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications, and services that run on AWS and on-premises servers. You can use CloudWatch to detect anomalous behavior in your environments, set alarms, visualize logs and metrics side by side, take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly.","title":"Amazon CloudWatch Foundation:  Observability of your AWS resources and applications on AWS and on-premises"},{"location":"monitor/#cloudwatch-benefits","text":"Observability on a single platform across applications and infrastructure Easiest way to collect metrics in AWS and on-premises Improve operational performance and resource optimization Get operational visibility and insight Derive actionable insights from logs","title":"CloudWatch Benefits"},{"location":"monitor/#cloudwatch-use-cases","text":"Infrastructure monitoring and troubleshooting Mean-time-to-resolution improvement Proactive resource optimization Application monitoring Log analytics","title":"CloudWatch Use cases"},{"location":"monitor/#how-cloudwatch-works","text":"CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, and visualizes it using automated dashboards so you can get a unified view of your AWS resources, applications, and services that run in AWS and on-premises. You can correlate your metrics and logs to better understand the health and performance of your resources. You can also create alarms based on metric value thresholds you specify, or that can watch for anomalous metric behavior based on machine learning algorithms. To take action quickly, you can set up automated actions to notify you if an alarm is triggered and automatically start auto scaling, for example, to help reduce mean-time-to-resolution. You can also dive deep and analyze your metrics, logs, and traces, to better understand how to improve application performance.","title":"How CloudWatch works?"},{"location":"monitor/#amazon-cloudwatch-features","text":"Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. With CloudWatch, you can collect and access all your performance and operational data in form of logs and metrics from a single platform. This allows you to overcome the challenge of monitoring individual systems and applications in silos (server, network, database, etc.). CloudWatch enables you to monitor your complete stack (applications, infrastructure, and services) and leverage alarms, logs, and events data to take automated actions and reduce Mean Time to Resolution (MTTR). This frees up important resources and allows you to focus on building applications and business value. CloudWatch gives you actionable insights that help you optimize application performance, manage resource utilization, and understand system-wide operational health. CloudWatch provides up to 1-second visibility of metrics and logs data, 15 months of data retention (metrics), and the ability to perform calculations on metrics. This allows you to perform historical analysis for cost optimization and derive real-time insights into optimizing applications and infrastructure resources. You can use CloudWatch Container Insights to monitor, troubleshoot, and alarm on your containerized applications and microservices. CloudWatch collects, aggregates, and summarizes compute utilization information like CPU, memory, disk, and network data, as well as diagnostic information like container restart failures, to help DevOps engineers isolate issues and resolve them quickly. Container Insights gives you insights from container management services such as Amazon ECS for Kubernetes (EKS), Amazon\u2019s Elastic Container Service (ECS), AWS Fargate, and standalone Kubernetes (k8s).","title":"Amazon CloudWatch Features"},{"location":"monitor/#feature-1-collect","text":"Easily collect and store logs Built-in metrics Custom Metrics Collect and aggregate container metrics and logs Collect and aggregate Lambda metrics and logs","title":"Feature 1. Collect"},{"location":"monitor/#feature-2-monitor","text":"Unified operational view with dashboards High resolution alarms Logs and metrics correlation Application Insights for .NET and SQL Server applications Container monitoring insights Lambda monitoring insights Anomaly Detection ServiceLens Synthetics","title":"Feature 2. Monitor"},{"location":"monitor/#feature-3-act","text":"Auto Scaling Automate response to operational changes with CloudWatch Events Alarm and automate actions on EKS, ECS, and k8s clusters","title":"Feature 3. Act"},{"location":"monitor/#feature-4-analyze","text":"Granular data and extended retention Custom operations on metrics Log analytics Analyze container metrics, logs, and traces Analyze Lambda metrics, logs, and traces Contributor Insights","title":"Feature 4. Analyze"},{"location":"monitor/#compliance-and-security","text":"Amazon CloudWatch is integrated with AWS Identity and Access Management (IAM) so that you can control which users and resources have permission to access your data and how they can access it. Amazon CloudWatch Logs is also PCI and FedRamp compliant. Data is encrypted at rest and during transfer. You can also use AWS KMS encryption to encrypt your log groups for added compliance and security.","title":"Compliance and Security"},{"location":"monitor/#amazon-cloudwatch-pricing","text":"With Amazon CloudWatch, there is no up-front commitment or minimum fee; you simply pay for what you use. You will be charged at the end of the month for your usage.","title":"Amazon CloudWatch pricing"},{"location":"monitor/#free-tier","text":"You can get started with Amazon CloudWatch for free. Most AWS Services (EC2, S3, Kinesis, etc.) vend metrics automatically for free to CloudWatch. Many applications should be able to operate within these free tier limits. You can learn more about AWS Free Tier here . CloudWatch AWS Free Tier Metrics Basic Monitoring Metrics (at 5-minute frequency) 10 Detailed Monitoring Metrics (at 1-minute frequency) 1 Million API requests (not applicable to GetMetricData and GetMetricWidgetImage) Dashboard 3 Dashboards for up to 50 metrics per month Alarms 10 Alarm metrics (not applicable to high-resolution alarms) Logs 5GB Data (ingestion, archive storage, and data scanned by Logs Insights queries) Events All events except custom events are included Contributor Insights 1 Contributor Insights rule per month The first one million log events that match the rule per month Synthetics 100 canary runs per month","title":"Free tier"},{"location":"monitor/#paid-tier","text":"There is no up-front commitment or minimum fee. You simply pay for what you use and will be charged at the end of the month for your usage. Check the AWS CloudWatch prices from here .","title":"Paid tier"},{"location":"monitor/#getting-started-with-amazon-cloudwatch","text":"Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/ . The CloudWatch overview home page appears. See, Getting Started with Amazon CloudWatch","title":"Getting started with Amazon CloudWatch"},{"location":"monitor/#chech-how-to-get-started","text":"Getting started with Amazon CloudWatch Getting Started with Amazon CloudWatch Events Getting Started with CloudWatch Logs","title":"Chech how to get started"},{"location":"monitor/#check-how-to-use-cloudwach-components","text":"Using Amazon CloudWatch Dashboards Using Amazon CloudWatch Metrics Using Amazon CloudWatch Alarms Using synthetic monitoring","title":"Check how to use CloudWach Components"},{"location":"monitor/#check-tutorials","text":"CloudWatch Events Tutorials Analyzing Log Data with CloudWatch Logs Insights CloudWatch Tutorials: Scenario: Monitor Your Estimated Charges Using CloudWatch Scenario: Publish Metrics to CloudWatch","title":"Check Tutorials"},{"location":"monitor/#check-the-services-monitoring","text":"Monitoring Amazon EC2 Amazon EBS volume performance on Linux instances Monitoring Amazon EFS Monitoring Amazon S3 Monitoring an Amazon RDS DB instance Using Container Insights Getting started with Lambda Insights","title":"Check the services Monitoring"},{"location":"mysqldemo/","text":"Demo: Create a web server and an Amazon RDS DB instance \u00b6 In this demo, we create one VPC that has three public subnets and three private subnets. Each of the public subnets has an EC2 instance with a webserver. The first private subnet has the DB instance and the other two have its read replica and the Multi-AZ. All these configurations and traffic are guided through an elastic load balancer to the internet. see teh demo topology in the following figure. Network Topology \u00b6 Use the following topology and start playing with EC2 instances and RDS databases: Step 1: Create the VPC \u00b6 By using the VPC wizard, create a VPC, by one public and one private subnets use the 10.10.0.0/16 block of addresses Create 4 more subnets: 2 public 2 private give each a different ip address with /24 prefix Configure the Internet gateway associate the public subnets keep the private subnets in the internal interfaces Check, Create an Amazon VPC for use with a DB instance for more details. Step 2: Configure the VPC security group \u00b6 From the VPC, or EC2 Dashboards, access the security groups and, Create a VPC security group for a public web server attach it to the created VPC add inbound rules to the security group Type: SSH Port: 20 Source: \u2018custom\u2019 or everywhere . add inbound rules to the security group Type: HTTP Port: 80 Source: everywhere . Create a VPC security group for a private DB instance attach it to the created VPC add inbound rules to the security group Type: MySQL/Aurora Port: allow MySQL traffic on port 3306 Source: the identifier of the publicsecuritygroup security group that you created previously. Step 3: Create the Database Instance \u00b6 Create a DB subnet group In the RDS dashboard, choose Subnet groups , then Create DB subnet group, as follow: Attach the VPC created Add subnets absed on Availability Zones and Subnets AZ: a,b,c subnets: all the private subnets 10.10.0.0/16 , 10.10.2.0/24 , 10.10.4.0/24 , and 10.10.6.0/24 Check, Create an Amazon VPC for use with a DB instance for more details. Create a mysql DB instance From Amazon RDS console, create database Master user: admin Auto generate a password \u2013 Disable the option In the Connectivity section, open Additional connectivity configuration and set these values: Virtual private cloud (VPC) \u2013 Choose our VPC Subnet group \u2013 The DB subnet group for the VPC Public access \u2013 No Existing VPC security groups \u2013 Choose an existing VPC security group that is configured for private access, Database port \u2013 3306 Open the Additional configuration section, and enter \u2018DatabaseName\u2019 for Initial database name. Keep the default settings for the other options. Choose Create database to create your RDS MySQL DB instance. Wait for the Status of your new DB instance to show as Available. Then choose the DB instance name to show its details. Configure the Multi AZ and Read replica Once your DB instance become avaulable Modify it and allow the Multi AZ option Choose the immediate change to make changes directly From Actions, choose create a read replica Create it in the third private subnet 10.10.6.0/24 Check, Create a DB instance for more details. Step 4: Create the EC2 instances and install the web servers \u00b6 Launch three EC2 instances AMI: Ubuntu 16 Add the VPC put each instnace in one of the public subnet Enable the auto assigning of IPs Configure Security Group Use the public security groupe for each of the instance Review and launch use your favorit key Wait until Instance Status for your instance reads as Running before continuing. Install an Apache web server with PHP Use this script to install the web server. Connect the Apache web server to your DB instance edit the dbinfo for connection DB_SERVER: db_instance_endpoint DB_USERNAME: admin DB_PASSWORD: \u2018master password\u2019 DB_DATABASE: \u2018database name\u2019 Save and close the dbinfo.inc file. Use the php file to test the connection and database DB.php use the public IP of each instance to check the database Check, Create an EC2 instance: webserver for more details. Step 5: Create the application Elastic Load Balancer (ELB) \u00b6 Associate the ELB with the 3 EC2 instances Choose your configured VPC Select the three availability zones Select the three public subnets Keep the default security configuration Choose the public security groupe In routing targets register the targets, the three instances Review and create, then give it some time Check your ELB after getting provisioned , use the public DNS name of the ELB to access your servers Check, Getting started with Application Load Balancers for more details Watch it here \u00b6 References and Help \u00b6 Please follow: Using Amazon RDS with Amazon VPC Determining whether you are using the EC2-VPC or EC2-Classic platform Scenarios for accessing a DB instance in a VPC DB In a VPC An EC2 instance in the same VPC An EC2 instance in a different VPC An EC2 instance not in a VPC A client application through the internet Not in a VPC An EC2 instance in a VPC An EC2 instance not in a VPC A client application through the internet Working with a DB instance in a VPC Tutorial: Create an Amazon VPC for use with a DB instance Then check the following: Create a DB instance Create an EC2 instance and install a web server","title":"MySQL RDS DB Instance Demo"},{"location":"mysqldemo/#demo-create-a-web-server-and-an-amazon-rds-db-instance","text":"In this demo, we create one VPC that has three public subnets and three private subnets. Each of the public subnets has an EC2 instance with a webserver. The first private subnet has the DB instance and the other two have its read replica and the Multi-AZ. All these configurations and traffic are guided through an elastic load balancer to the internet. see teh demo topology in the following figure.","title":"Demo: Create a web server and an Amazon RDS DB instance"},{"location":"mysqldemo/#network-topology","text":"Use the following topology and start playing with EC2 instances and RDS databases:","title":"Network Topology"},{"location":"mysqldemo/#step-1-create-the-vpc","text":"By using the VPC wizard, create a VPC, by one public and one private subnets use the 10.10.0.0/16 block of addresses Create 4 more subnets: 2 public 2 private give each a different ip address with /24 prefix Configure the Internet gateway associate the public subnets keep the private subnets in the internal interfaces Check, Create an Amazon VPC for use with a DB instance for more details.","title":"Step 1: Create the VPC"},{"location":"mysqldemo/#step-2-configure-the-vpc-security-group","text":"From the VPC, or EC2 Dashboards, access the security groups and, Create a VPC security group for a public web server attach it to the created VPC add inbound rules to the security group Type: SSH Port: 20 Source: \u2018custom\u2019 or everywhere . add inbound rules to the security group Type: HTTP Port: 80 Source: everywhere . Create a VPC security group for a private DB instance attach it to the created VPC add inbound rules to the security group Type: MySQL/Aurora Port: allow MySQL traffic on port 3306 Source: the identifier of the publicsecuritygroup security group that you created previously.","title":"Step 2: Configure the VPC security group"},{"location":"mysqldemo/#step-3-create-the-database-instance","text":"Create a DB subnet group In the RDS dashboard, choose Subnet groups , then Create DB subnet group, as follow: Attach the VPC created Add subnets absed on Availability Zones and Subnets AZ: a,b,c subnets: all the private subnets 10.10.0.0/16 , 10.10.2.0/24 , 10.10.4.0/24 , and 10.10.6.0/24 Check, Create an Amazon VPC for use with a DB instance for more details. Create a mysql DB instance From Amazon RDS console, create database Master user: admin Auto generate a password \u2013 Disable the option In the Connectivity section, open Additional connectivity configuration and set these values: Virtual private cloud (VPC) \u2013 Choose our VPC Subnet group \u2013 The DB subnet group for the VPC Public access \u2013 No Existing VPC security groups \u2013 Choose an existing VPC security group that is configured for private access, Database port \u2013 3306 Open the Additional configuration section, and enter \u2018DatabaseName\u2019 for Initial database name. Keep the default settings for the other options. Choose Create database to create your RDS MySQL DB instance. Wait for the Status of your new DB instance to show as Available. Then choose the DB instance name to show its details. Configure the Multi AZ and Read replica Once your DB instance become avaulable Modify it and allow the Multi AZ option Choose the immediate change to make changes directly From Actions, choose create a read replica Create it in the third private subnet 10.10.6.0/24 Check, Create a DB instance for more details.","title":"Step 3: Create the Database Instance"},{"location":"mysqldemo/#step-4-create-the-ec2-instances-and-install-the-web-servers","text":"Launch three EC2 instances AMI: Ubuntu 16 Add the VPC put each instnace in one of the public subnet Enable the auto assigning of IPs Configure Security Group Use the public security groupe for each of the instance Review and launch use your favorit key Wait until Instance Status for your instance reads as Running before continuing. Install an Apache web server with PHP Use this script to install the web server. Connect the Apache web server to your DB instance edit the dbinfo for connection DB_SERVER: db_instance_endpoint DB_USERNAME: admin DB_PASSWORD: \u2018master password\u2019 DB_DATABASE: \u2018database name\u2019 Save and close the dbinfo.inc file. Use the php file to test the connection and database DB.php use the public IP of each instance to check the database Check, Create an EC2 instance: webserver for more details.","title":"Step 4: Create the EC2 instances and install the web servers"},{"location":"mysqldemo/#step-5-create-the-application-elastic-load-balancer-elb","text":"Associate the ELB with the 3 EC2 instances Choose your configured VPC Select the three availability zones Select the three public subnets Keep the default security configuration Choose the public security groupe In routing targets register the targets, the three instances Review and create, then give it some time Check your ELB after getting provisioned , use the public DNS name of the ELB to access your servers Check, Getting started with Application Load Balancers for more details","title":"Step 5: Create the application Elastic Load Balancer (ELB)"},{"location":"mysqldemo/#watch-it-here","text":"","title":"Watch it here"},{"location":"mysqldemo/#references-and-help","text":"Please follow: Using Amazon RDS with Amazon VPC Determining whether you are using the EC2-VPC or EC2-Classic platform Scenarios for accessing a DB instance in a VPC DB In a VPC An EC2 instance in the same VPC An EC2 instance in a different VPC An EC2 instance not in a VPC A client application through the internet Not in a VPC An EC2 instance in a VPC An EC2 instance not in a VPC A client application through the internet Working with a DB instance in a VPC Tutorial: Create an Amazon VPC for use with a DB instance Then check the following: Create a DB instance Create an EC2 instance and install a web server","title":"References and Help"},{"location":"network/","text":"Network Addressing \u00b6 In this section, you need to test your experiance in computer networks and specialy the network addressing. Network addressing is an important part in managing your network topology and sub networks either inside AWS or in your on-premises data center. In order to test your knowledge, ask yourself if you know the following: Subnets, prefix, and network maskes Classfull / Classless network addressing Gateway and public IP Natting and Private IP \u2026etc Cisco: Addressing of a Network \u00b6 Check the following documentation to know the important terms and to see how to devide your IP range into multiple subnetworks. CIDR Notation \u00b6 An important concept that\u2019s used in networking on AWS is CIDR, or Classless Inter-Domain Routing. CIDR network addresses are allocated in a virtual private cloud (VPC) and in a subnet by using CIDR notation. A /16 block provides 65,536 IPv4 addresses. A /24 block provides 256 addresses. See this article for more information about CIDR. Please check the IP Addressing in your VPC .","title":"Network Addressing"},{"location":"network/#network-addressing","text":"In this section, you need to test your experiance in computer networks and specialy the network addressing. Network addressing is an important part in managing your network topology and sub networks either inside AWS or in your on-premises data center. In order to test your knowledge, ask yourself if you know the following: Subnets, prefix, and network maskes Classfull / Classless network addressing Gateway and public IP Natting and Private IP \u2026etc","title":"Network Addressing"},{"location":"network/#cisco-addressing-of-a-network","text":"Check the following documentation to know the important terms and to see how to devide your IP range into multiple subnetworks.","title":"Cisco: Addressing of a Network"},{"location":"network/#cidr-notation","text":"An important concept that\u2019s used in networking on AWS is CIDR, or Classless Inter-Domain Routing. CIDR network addresses are allocated in a virtual private cloud (VPC) and in a subnet by using CIDR notation. A /16 block provides 65,536 IPv4 addresses. A /24 block provides 256 addresses. See this article for more information about CIDR. Please check the IP Addressing in your VPC .","title":"CIDR Notation"},{"location":"network2/","text":"Networking and Content Delivery on AWS \u00b6 AWS provides the broadest and deepest set of networking services with the highest reliability , most security features , and highest performance in the world. This helps ensure you can run any kind of workload you have in the cloud. AWS Network Properities and Features \u00b6 Networking features in AWS: Most secure Highest network availability Consistent high performance Broadest global coverage The AWS network properties: 77 Availability Zones : multiple, physically separated, and isolated 100 Gbps network bandwidth : available from many EC2 instance types 24 AWS Regions : low latency, high throughput, and highly redundant 216 Points of Presence : providing global coverage for your users 2x More Regions : with multiple AZs than the next largest cloud provider AWS Networking and Content Delivery services \u00b6 There are many use cases in AWS networking, and for each use case there are some services are developed for handeling each of the use cses. Use cases are such as: Build a cloud network Scale your network design Secure your network traffic Build a hybrid IT network Content delivery networks Build a network for microservices architectures Would you like to see which services are used for each use case, check the table below and see more details about Networking and Content Delivery Services . Use Cases AWS service Functionality 1 a. Amazon VPC b. AWS Transit Gateway c. AWS PrivateLink d. Amazon Route 53 a. Define and provision a logically isolated network for your AWS resources b. Connect VPCs and on-premises networks through a central hub c. Provide private connectivity between VPCs, services, and on-premises applications d. Route users to Internet applications with a managed DNS service 2 a. Elastic Load Balancing b. AWS Global Accelerator a. Automatically distribute traffic across a pool of resources, such as instances, containers, IP addresses, and Lambda functions b. Direct traffic through the AWS Global network to improve global application performance 3 a. AWS Shield b. AWS WAF c. AWS Firewall Manager a. Safeguard applications running on AWS against DDoS attacks b. Protect your web applications from common web exploits c. Centrally configure and manage firewall rules 4 a. AWS Virtual Private Network (VPN) - Client b. AWS Virtual Private Network (VPN) - Site to Site c. AWS Direct Connect a. Connect your users to AWS or on-premises resources using a Virtual Private Network b. Create an encrypted connection between your network and your Amazon VPCs or AWS Transit Gateways c. Establish a private, dedicated connection between AWS and your datacenter, office, or colocation environment 5 a. Amazon CloudFront a. Securely deliver data, videos, applications, and APIs to customers globally with low latency, and high transfer speeds 6 a. AWS App Mesh b. Amazon API Gateway c. AWS Cloud Map a. Provide application-level networking for containers and microservices b. Create, maintain, and secure APIs at any scale c. Discover AWS services connected to your applications Amazon Virtual Private Cloud (VPC) \u00b6 Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including the selection of your own IP address range, the creation of subnets, and the configuration of route tables and network gateways. You can use both IPv4 and IPv6 in your VPC for secure and easy access to resources and applications. You could create up to five non-default VPCs per AWS account per Region. (See below for information about default VPCs.) Details on Amazon VPC can be found here: https://aws.amazon.com/vpc VPC Benefits and Use Cases \u00b6 Benefits: Secure Simple Customizable Use Cases: Host a simple, public-facing website Host multi-tier web applications Disaster recovery Extend your corporate network into the cloud Securely connect cloud applications to your datacenter Out-of-band and inline traffic inspection Please check the VPC Features and Pricing , before starting using it Amazon VPC concepts \u00b6 Amazon VPC is the networking layer for Amazon EC2. The following are the key concepts for VPCs: Virtual private cloud (VPC) \u2014 A virtual network dedicated to your AWS account. Subnet \u2014 A range of IP addresses in your VPC. Route table \u2014 A set of rules, called routes, that are used to determine where network traffic is directed. Internet gateway \u2014 A gateway that you attach to your VPC to enable communication between resources in your VPC and the internet. VPC endpoint \u2014 Enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. For more information, see VPC endpoints and VPC endpoint services (AWS PrivateLink) . Accessing Amazon VPC \u00b6 You can create, access, and manage your VPCs using any of the following interfaces: AWS Management Console \u2014 Provides a web interface that you can use to access your VPCs. AWS Command Line Interface (AWS CLI) \u2014 Provides commands for a broad set of AWS services, including Amazon VPC, and is supported on Windows, Mac, and Linux. For more information, see AWS Command Line Interface . AWS SDKs \u2014 Provides language-specific APIs and takes care of many of the connection details, such as calculating signatures, handling request retries, and error handling. For more information, see AWS SDKs . Query API \u2014 Provides low-level API actions that you call using HTTPS requests. Using the Query API is the most direct way to access Amazon VPC, but it requires that your application handle low-level details such as generating the hash to sign the request, and error handling. For more information, see the Amazon EC2 API Reference . VPCs and subnets \u00b6 A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. You can launch your AWS resources, such as Amazon EC2 instances, into your VPC. You can specify an IP address range for the VPC, add subnets, associate security groups, and configure route tables. A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet. Use a public subnet for resources that must be connected to the internet, and a private subnet for resources that won\u2019t be connected to the internet . To protect the AWS resources in each subnet, you can use multiple layers of security, including security groups and network access control lists (ACL). You can optionally associate an IPv6 CIDR block with your VPC, and assign IPv6 addresses to the instances in your VPC, see https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html#subnet-public-ip . VPC Subnets \u00b6 A VPC spans all the Availability Zones in the Region. After creating a VPC, you can add one or more subnets in each Availability Zone. When you create a subnet, you specify the CIDR block for the subnet, which is a subset of the VPC CIDR block. Each subnet must reside entirely: within one Availability Zone , and it can\u2019t span Availability Zones , see the VPC and subnet basics . The following diagram shows a new VPC with an IPv4 CIDR block. Security in a VPC is provided by using Security Groups and Network Access Control Groups. Internet Gateway \u00b6 An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. An internet gateway supports IPv4 and IPv6 traffic. It does not cause availability risks or bandwidth constraints on your network traffic. There\u2019s no additional charge for having an internet gateway in your account. The following diagram shows a VPC that has been configured with subnets in multiple Availability Zones. 1A, 2A, and 3A are instances in your VPC. An IPv6 CIDR block is associated with the VPC, and an IPv6 CIDR block is associated with subnet 1. An internet gateway enables communication over the internet, and a virtual private network (VPN) connection enables communication with your corporate network. Default VPC \u00b6 In each Region, AWS will provision a default VPC . This VPC has a /16 IPv4 CIDR address block of 172.31.0.0/16. This provides 65,536 private IPv4 addresses. In addition, there will be a /20 subnet that is created for each Availability Zone in the Region, which provides 4,096 addresses per subnet, with a few addresses reserved for AWS usage. The route table that is associated with the default VPC will have a public route, which in turn is associated with a provisioned internet gateway. You can modify or delete the default VPC if you want to do so. The most current details on the default VPC can be found here: https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html Default subnets \u00b6 By default, a default subnet is a public subnet, because the main route table sends the subnet\u2019s traffic that is destined for the internet to the internet gateway. You can make a default subnet into a private subnet by removing the route from the destination 0.0.0.0/0 to the internet gateway. However, if you do this, no EC2 instance running in that subnet can access the internet. Instances that you launch into a default subnet receive both a public IPv4 address and a private IPv4 address, and both public and private DNS hostnames. Instances that you launch into a nondefault subnet in a default VPC don\u2019t receive a public IPv4 address or a DNS hostname. You can change your subnet\u2019s default public IP addressing behavior. For more information, see Modifying the public IPv4 addressing attribute for your subnet . From time to time, AWS may add a new Availability Zone to a Region. In most cases, we automatically create a new default subnet in this Availability Zone for your default VPC within a few days. However, if you made any modifications to your default VPC, we do not add a new default subnet. If you want a default subnet for the new Availability Zone, you can create one yourself. For more information, see Creating a default subnet and Default VPC and default subnets . Route tables \u00b6 A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. Route table concepts \u00b6 The following are the key concepts for route tables. Main route table \u2014The route table that automatically comes with your VPC. It controls the routing for all subnets that are not explicitly associated with any other route table. Custom route table \u2014A route table that you create for your VPC. Edge association \u2014A route table that you use to route inbound VPC traffic to an appliance. You associate a route table with the internet gateway or virtual private gateway, and specify the network interface of your appliance as the target for VPC traffic. Route table association \u2014The association between a route table and a subnet, internet gateway, or virtual private gateway. Subnet route table \u2014A route table that\u2019s associated with a subnet. Gateway route table \u2014A route table that\u2019s associated with an internet gateway or virtual private gateway. Local gateway route table \u2014A route table that\u2019s associated with an Outposts local gateway. For information about local gateways, see Local Gateways in the AWS Outposts User Guide. Destination \u2014The range of IP addresses where you want traffic to go (destination CIDR). For example, an external corporate network with a 172.16.0.0/12 CIDR. Propagation \u2014Route propagation allows a virtual private gateway to automatically propagate routes to the route tables. This means that you don\u2019t need to manually enter VPN routes to your route tables. For more information about VPN routing options, see Site-to-Site VPN routing options in the Site-to-Site VPN User Guide. Target \u2014The gateway, network interface, or connection through which to send the destination traffic; for example, an internet gateway. Local route \u2014A default route for communication within the VPC. For example routing options, see Example routing options . How route tables work \u00b6 Your VPC has an implicit router, and you use route tables to control where network traffic is directed. Each subnet in your VPC must be associated with a route table, which controls the routing for the subnet (subnet route table). You can explicitly associate a subnet with a particular route table. Otherwise, the subnet is implicitly associated with the main route table. A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same subnet route table. You can optionally associate a route table with an internet gateway or a virtual private gateway (gateway route table). This enables you to specify routing rules for inbound traffic that enters your VPC through the gateway. For more information, see Gateway route tables . Routes \u00b6 Each route in a table specifies a destination and a target. For example, to enable your subnet to access the internet through an internet gateway, add the following route to your subnet route table. Destination Target 0.0.0.0/0 igw-12345678901234567 The destination for the route is 0.0.0.0/0, which represents all IPv4 addresses. The target is the internet gateway that\u2019s attached to your VPC. Main route table \u00b6 When you create a VPC, it automatically has a main route table. The main route table controls the routing for all subnets that are not explicitly associated with any other route table. On the Route Tables page in the Amazon VPC console, you can view the main route table for a VPC by looking for Yes in the Main column. By default, when you create a nondefault VPC, the main route table contains only a local route. When you use the VPC wizard in the console to create a nondefault VPC with a NAT gateway or virtual private gateway, the wizard automatically adds routes to the main route table for those gateways. You can add, remove, and modify routes in the main route table. You cannot create a more specific route than the local route. You cannot delete the main route table, but you can replace the main route table with a custom subnet route table that you\u2019ve created. You cannot set a gateway route table as the main route table. You can explicitly associate a subnet with the main route table, even if it\u2019s already implicitly associated. You might want to do that if you change which table is the main route table. When you change which table is the main route table, it also changes the default for additional new subnets, or for any subnets that are not explicitly associated with any other route table. For more information, see Replacing the main route table . Custom route tables \u00b6 By default, a custom route table is empty and you add routes as needed. When you use the VPC wizard in the console to create a VPC with an internet gateway, the wizard creates a custom route table and adds a route to the internet gateway. One way to protect your VPC is to leave the main route table in its original default state. Then, explicitly associate each new subnet that you create with one of the custom route tables you\u2019ve created. This ensures that you explicitly control how each subnet routes traffic. You can add, remove, and modify routes in a custom route table. You can delete a custom route table only if it has no associations. Route priority \u00b6 We use the most specific route in your route table that matches the traffic to determine how to route the traffic (longest prefix match). Routes to IPv4 and IPv6 addresses or CIDR blocks are independent of each other. We use the most specific route that matches either IPv4 traffic or IPv6 traffic to determine how to route the traffic. For example, the following subnet route table has a route for IPv4 internet traffic ( 0.0.0.0/0 ) that points to an internet gateway, and a route for 172.31.0.0/16 IPv4 traffic that points to a peering connection ( pcx-11223344556677889 ). Any traffic from the subnet that\u2019s destined for the 172.31.0.0/16 IP address range uses the peering connection, because this route is more specific than the route for internet gateway. Any traffic destined for a target within the VPC ( 10.0.0.0/16 ) is covered by the Local route, and therefore is routed within the VPC. All other traffic from the subnet uses the internet gateway. Destination Target 10.0.0.0/16 Local 172.31.0.0/16 pcx-11223344556677889 0.0.0.0/0 igw-12345678901234567 Still more and more details about routes and routing tables, see: Example routing options Working with route tables AWS Direct Connect \u00b6 AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment , which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs. Amazon Direct Connect Use Cases and Benefits \u00b6 When to use AWS Direct Connect With AWS Direct Connect, you can connect to all your AWS resources in an AWS Region, transfer your business critical data directly from your datacenter, office, or colocation environment into and from AWS, bypassing your Internet service provider and removing network congestion. Further, AWS Direct Connect\u2019s simple pay as-you-go pricing and no minimum commitment means you pay only for the network ports you use and the data you transfer out of the AWS Region over the AWS Direct Connect connection, which can greatly reduce your networking costs. There are several use cases for which AWS Direct Connect is desirable, including: Working with Large Data Sets Real-time Data Feeds Hybrid Environments AWS Direct Connect Benefits Reduces Your Bandwidth Costs Consistent Network Performance Compatible with all AWS Services Private Connectivity to your Amazon VPC Elastic Simple For more details, read AWS Direct Connect features and AWS Direct Connect pricing . Virtual private gateway \u00b6 You can use an AWS Direct Connect gateway to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in any account that are located in the same or different Regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC. Then, you create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway. Consider the following while using the VPG : There are limits for creating and using Direct Connect gateways. The VPCs to which you connect through a Direct Connect gateway cannot have overlapping CIDR blocks. If you add an IPv4 CIDR block to a VPC that\u2019s associated with a Direct Connect gateway, ensure that the CIDR block does not overlap with an existing CIDR block for any other associated VPC. For more information, see Adding IPv4 CIDR Blocks to a VPC in the Amazon VPC User Guide. You cannot create a public virtual interface to a Direct Connect gateway. A Direct Connect gateway supports communication between attached private virtual interfaces and associated virtual private gateways only. The following traffic flows are not supported: Direct communication between the VPCs that are associated with a single Direct Connect gateway. This includes traffic from one VPC to another by using a hairpin through an on-premises network through a single Direct Connect gateway. Direct communication between the virtual interfaces that are attached to a single Direct Connect gateway. Direct communication between the virtual interfaces that are attached to a single Direct Connect gateway and a VPN connection on a virtual private gateway that\u2019s associated with the same Direct Connect gateway. You cannot associate a virtual private gateway with more than one Direct Connect gateway and you cannot attach a private virtual interface to more than one Direct Connect gateway. A virtual private gateway that you associate with a Direct Connect gateway must be attached to a VPC. A virtual private gateway association proposal expires 7 days after it is created. An accepted virtual private gateway proposal, or a deleted virtual private gateway proposal remains visible for 3 days. To connect your AWS Direct Connect connection to a VPC in the same Region only, you can create a Direct Connect gateway. Or, you can create a private virtual interface and attach it to the virtual private gateway for the VPC. For more information, see Creating a private virtual interface and VPN CloudHub . To use your AWS Direct Connect connection with a VPC in another account, you can create a hosted private virtual interface for that account. When the owner of the other account accepts the hosted virtual interface, they can choose to attach it either to a virtual private gateway or to a Direct Connect gateway in their account. For more information, see AWS Direct Connect virtual interfaces . Please see, how to: Creating a virtual private gateway Associating and disassociating virtual private gateways Creating a private virtual interface to the Direct Connect gateway Associating a virtual private gateway across accounts AWS VPN \u00b6 AWS Virtual Private Network solutions establish secure connections between your on-premises networks, remote offices, client devices, and the AWS global network. AWS VPN is comprised of two services: AWS Site-to-Site VPN and AWS Client VPN. Together, they deliver a highly-available, managed, and elastic cloud VPN solution to protect your network traffic. AWS Site-to-Site VPN creates encrypted tunnels between your network and your Amazon Virtual Private Clouds or AWS Transit Gateways. For managing remote access, AWS Client VPN connects your users to AWS or on-premises resources using a VPN software client. Two flavors of the AWS VPN: AWS Client VPN AWS Site-to-Site VPN AWS Client VPN \u00b6 AWS Client VPN is a fully-managed, elastic VPN service that automatically scales up or down based on user demand. Because it is a cloud VPN solution, you don\u2019t need to install and manage hardware or software-based solutions, or try to estimate how many remote users to support at one time. Benefits Fully managed Advanced authentication Elastic Remote access Use cases Quickly scale remote access Easily deploy and remove VPN access for temporary workers Easily access applications in the cloud or on premises How it works AWS Site-to-Site VPN \u00b6 AWS Site-to-Site VPN creates a secure connection between your data center or branch office and your AWS cloud resources. For globally distributed applications, the Accelerated Site-to-Site VPN option provides even greater performance by working with AWS Global Accelerator. Benefits Highly available Secure Robust monitoring Accelerate Applicationss Use cases Extend your corporate network into the cloud Secure communication between remote locations How it works VPN CloudHub \u00b6 If you have multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub. This enables your remote sites to communicate with each other, and not just with the VPC. The VPN CloudHub operates on a simple hub-and-spoke model that you can use with or without a VPC. This design is suitable if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices. The sites must not have overlapping IP ranges. The following diagram shows the VPN CloudHub architecture, with blue dashed lines indicating network traffic between remote sites being routed over their Site-to-Site VPN connections. See, Providing secure communication between sites using VPN CloudHub for more details about how to configure the previous topology.","title":"Networking in AWS"},{"location":"network2/#networking-and-content-delivery-on-aws","text":"AWS provides the broadest and deepest set of networking services with the highest reliability , most security features , and highest performance in the world. This helps ensure you can run any kind of workload you have in the cloud.","title":"Networking and Content Delivery on AWS"},{"location":"network2/#aws-network-properities-and-features","text":"Networking features in AWS: Most secure Highest network availability Consistent high performance Broadest global coverage The AWS network properties: 77 Availability Zones : multiple, physically separated, and isolated 100 Gbps network bandwidth : available from many EC2 instance types 24 AWS Regions : low latency, high throughput, and highly redundant 216 Points of Presence : providing global coverage for your users 2x More Regions : with multiple AZs than the next largest cloud provider","title":"AWS Network Properities and Features"},{"location":"network2/#aws-networking-and-content-delivery-services","text":"There are many use cases in AWS networking, and for each use case there are some services are developed for handeling each of the use cses. Use cases are such as: Build a cloud network Scale your network design Secure your network traffic Build a hybrid IT network Content delivery networks Build a network for microservices architectures Would you like to see which services are used for each use case, check the table below and see more details about Networking and Content Delivery Services . Use Cases AWS service Functionality 1 a. Amazon VPC b. AWS Transit Gateway c. AWS PrivateLink d. Amazon Route 53 a. Define and provision a logically isolated network for your AWS resources b. Connect VPCs and on-premises networks through a central hub c. Provide private connectivity between VPCs, services, and on-premises applications d. Route users to Internet applications with a managed DNS service 2 a. Elastic Load Balancing b. AWS Global Accelerator a. Automatically distribute traffic across a pool of resources, such as instances, containers, IP addresses, and Lambda functions b. Direct traffic through the AWS Global network to improve global application performance 3 a. AWS Shield b. AWS WAF c. AWS Firewall Manager a. Safeguard applications running on AWS against DDoS attacks b. Protect your web applications from common web exploits c. Centrally configure and manage firewall rules 4 a. AWS Virtual Private Network (VPN) - Client b. AWS Virtual Private Network (VPN) - Site to Site c. AWS Direct Connect a. Connect your users to AWS or on-premises resources using a Virtual Private Network b. Create an encrypted connection between your network and your Amazon VPCs or AWS Transit Gateways c. Establish a private, dedicated connection between AWS and your datacenter, office, or colocation environment 5 a. Amazon CloudFront a. Securely deliver data, videos, applications, and APIs to customers globally with low latency, and high transfer speeds 6 a. AWS App Mesh b. Amazon API Gateway c. AWS Cloud Map a. Provide application-level networking for containers and microservices b. Create, maintain, and secure APIs at any scale c. Discover AWS services connected to your applications","title":"AWS Networking and Content Delivery services"},{"location":"network2/#amazon-virtual-private-cloud-vpc","text":"Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including the selection of your own IP address range, the creation of subnets, and the configuration of route tables and network gateways. You can use both IPv4 and IPv6 in your VPC for secure and easy access to resources and applications. You could create up to five non-default VPCs per AWS account per Region. (See below for information about default VPCs.) Details on Amazon VPC can be found here: https://aws.amazon.com/vpc","title":"Amazon Virtual Private Cloud (VPC)"},{"location":"network2/#vpc-benefits-and-use-cases","text":"Benefits: Secure Simple Customizable Use Cases: Host a simple, public-facing website Host multi-tier web applications Disaster recovery Extend your corporate network into the cloud Securely connect cloud applications to your datacenter Out-of-band and inline traffic inspection Please check the VPC Features and Pricing , before starting using it","title":"VPC Benefits and Use Cases"},{"location":"network2/#amazon-vpc-concepts","text":"Amazon VPC is the networking layer for Amazon EC2. The following are the key concepts for VPCs: Virtual private cloud (VPC) \u2014 A virtual network dedicated to your AWS account. Subnet \u2014 A range of IP addresses in your VPC. Route table \u2014 A set of rules, called routes, that are used to determine where network traffic is directed. Internet gateway \u2014 A gateway that you attach to your VPC to enable communication between resources in your VPC and the internet. VPC endpoint \u2014 Enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. For more information, see VPC endpoints and VPC endpoint services (AWS PrivateLink) .","title":"Amazon VPC concepts"},{"location":"network2/#accessing-amazon-vpc","text":"You can create, access, and manage your VPCs using any of the following interfaces: AWS Management Console \u2014 Provides a web interface that you can use to access your VPCs. AWS Command Line Interface (AWS CLI) \u2014 Provides commands for a broad set of AWS services, including Amazon VPC, and is supported on Windows, Mac, and Linux. For more information, see AWS Command Line Interface . AWS SDKs \u2014 Provides language-specific APIs and takes care of many of the connection details, such as calculating signatures, handling request retries, and error handling. For more information, see AWS SDKs . Query API \u2014 Provides low-level API actions that you call using HTTPS requests. Using the Query API is the most direct way to access Amazon VPC, but it requires that your application handle low-level details such as generating the hash to sign the request, and error handling. For more information, see the Amazon EC2 API Reference .","title":"Accessing Amazon VPC"},{"location":"network2/#vpcs-and-subnets","text":"A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. You can launch your AWS resources, such as Amazon EC2 instances, into your VPC. You can specify an IP address range for the VPC, add subnets, associate security groups, and configure route tables. A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet. Use a public subnet for resources that must be connected to the internet, and a private subnet for resources that won\u2019t be connected to the internet . To protect the AWS resources in each subnet, you can use multiple layers of security, including security groups and network access control lists (ACL). You can optionally associate an IPv6 CIDR block with your VPC, and assign IPv6 addresses to the instances in your VPC, see https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html#subnet-public-ip .","title":"VPCs and subnets"},{"location":"network2/#vpc-subnets","text":"A VPC spans all the Availability Zones in the Region. After creating a VPC, you can add one or more subnets in each Availability Zone. When you create a subnet, you specify the CIDR block for the subnet, which is a subset of the VPC CIDR block. Each subnet must reside entirely: within one Availability Zone , and it can\u2019t span Availability Zones , see the VPC and subnet basics . The following diagram shows a new VPC with an IPv4 CIDR block. Security in a VPC is provided by using Security Groups and Network Access Control Groups.","title":"VPC Subnets"},{"location":"network2/#internet-gateway","text":"An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. An internet gateway supports IPv4 and IPv6 traffic. It does not cause availability risks or bandwidth constraints on your network traffic. There\u2019s no additional charge for having an internet gateway in your account. The following diagram shows a VPC that has been configured with subnets in multiple Availability Zones. 1A, 2A, and 3A are instances in your VPC. An IPv6 CIDR block is associated with the VPC, and an IPv6 CIDR block is associated with subnet 1. An internet gateway enables communication over the internet, and a virtual private network (VPN) connection enables communication with your corporate network.","title":"Internet Gateway"},{"location":"network2/#default-vpc","text":"In each Region, AWS will provision a default VPC . This VPC has a /16 IPv4 CIDR address block of 172.31.0.0/16. This provides 65,536 private IPv4 addresses. In addition, there will be a /20 subnet that is created for each Availability Zone in the Region, which provides 4,096 addresses per subnet, with a few addresses reserved for AWS usage. The route table that is associated with the default VPC will have a public route, which in turn is associated with a provisioned internet gateway. You can modify or delete the default VPC if you want to do so. The most current details on the default VPC can be found here: https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html","title":"Default VPC"},{"location":"network2/#default-subnets","text":"By default, a default subnet is a public subnet, because the main route table sends the subnet\u2019s traffic that is destined for the internet to the internet gateway. You can make a default subnet into a private subnet by removing the route from the destination 0.0.0.0/0 to the internet gateway. However, if you do this, no EC2 instance running in that subnet can access the internet. Instances that you launch into a default subnet receive both a public IPv4 address and a private IPv4 address, and both public and private DNS hostnames. Instances that you launch into a nondefault subnet in a default VPC don\u2019t receive a public IPv4 address or a DNS hostname. You can change your subnet\u2019s default public IP addressing behavior. For more information, see Modifying the public IPv4 addressing attribute for your subnet . From time to time, AWS may add a new Availability Zone to a Region. In most cases, we automatically create a new default subnet in this Availability Zone for your default VPC within a few days. However, if you made any modifications to your default VPC, we do not add a new default subnet. If you want a default subnet for the new Availability Zone, you can create one yourself. For more information, see Creating a default subnet and Default VPC and default subnets .","title":"Default subnets"},{"location":"network2/#route-tables","text":"A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed.","title":"Route tables"},{"location":"network2/#route-table-concepts","text":"The following are the key concepts for route tables. Main route table \u2014The route table that automatically comes with your VPC. It controls the routing for all subnets that are not explicitly associated with any other route table. Custom route table \u2014A route table that you create for your VPC. Edge association \u2014A route table that you use to route inbound VPC traffic to an appliance. You associate a route table with the internet gateway or virtual private gateway, and specify the network interface of your appliance as the target for VPC traffic. Route table association \u2014The association between a route table and a subnet, internet gateway, or virtual private gateway. Subnet route table \u2014A route table that\u2019s associated with a subnet. Gateway route table \u2014A route table that\u2019s associated with an internet gateway or virtual private gateway. Local gateway route table \u2014A route table that\u2019s associated with an Outposts local gateway. For information about local gateways, see Local Gateways in the AWS Outposts User Guide. Destination \u2014The range of IP addresses where you want traffic to go (destination CIDR). For example, an external corporate network with a 172.16.0.0/12 CIDR. Propagation \u2014Route propagation allows a virtual private gateway to automatically propagate routes to the route tables. This means that you don\u2019t need to manually enter VPN routes to your route tables. For more information about VPN routing options, see Site-to-Site VPN routing options in the Site-to-Site VPN User Guide. Target \u2014The gateway, network interface, or connection through which to send the destination traffic; for example, an internet gateway. Local route \u2014A default route for communication within the VPC. For example routing options, see Example routing options .","title":"Route table concepts"},{"location":"network2/#how-route-tables-work","text":"Your VPC has an implicit router, and you use route tables to control where network traffic is directed. Each subnet in your VPC must be associated with a route table, which controls the routing for the subnet (subnet route table). You can explicitly associate a subnet with a particular route table. Otherwise, the subnet is implicitly associated with the main route table. A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same subnet route table. You can optionally associate a route table with an internet gateway or a virtual private gateway (gateway route table). This enables you to specify routing rules for inbound traffic that enters your VPC through the gateway. For more information, see Gateway route tables .","title":"How route tables work"},{"location":"network2/#routes","text":"Each route in a table specifies a destination and a target. For example, to enable your subnet to access the internet through an internet gateway, add the following route to your subnet route table. Destination Target 0.0.0.0/0 igw-12345678901234567 The destination for the route is 0.0.0.0/0, which represents all IPv4 addresses. The target is the internet gateway that\u2019s attached to your VPC.","title":"Routes"},{"location":"network2/#main-route-table","text":"When you create a VPC, it automatically has a main route table. The main route table controls the routing for all subnets that are not explicitly associated with any other route table. On the Route Tables page in the Amazon VPC console, you can view the main route table for a VPC by looking for Yes in the Main column. By default, when you create a nondefault VPC, the main route table contains only a local route. When you use the VPC wizard in the console to create a nondefault VPC with a NAT gateway or virtual private gateway, the wizard automatically adds routes to the main route table for those gateways. You can add, remove, and modify routes in the main route table. You cannot create a more specific route than the local route. You cannot delete the main route table, but you can replace the main route table with a custom subnet route table that you\u2019ve created. You cannot set a gateway route table as the main route table. You can explicitly associate a subnet with the main route table, even if it\u2019s already implicitly associated. You might want to do that if you change which table is the main route table. When you change which table is the main route table, it also changes the default for additional new subnets, or for any subnets that are not explicitly associated with any other route table. For more information, see Replacing the main route table .","title":"Main route table"},{"location":"network2/#custom-route-tables","text":"By default, a custom route table is empty and you add routes as needed. When you use the VPC wizard in the console to create a VPC with an internet gateway, the wizard creates a custom route table and adds a route to the internet gateway. One way to protect your VPC is to leave the main route table in its original default state. Then, explicitly associate each new subnet that you create with one of the custom route tables you\u2019ve created. This ensures that you explicitly control how each subnet routes traffic. You can add, remove, and modify routes in a custom route table. You can delete a custom route table only if it has no associations.","title":"Custom route tables"},{"location":"network2/#route-priority","text":"We use the most specific route in your route table that matches the traffic to determine how to route the traffic (longest prefix match). Routes to IPv4 and IPv6 addresses or CIDR blocks are independent of each other. We use the most specific route that matches either IPv4 traffic or IPv6 traffic to determine how to route the traffic. For example, the following subnet route table has a route for IPv4 internet traffic ( 0.0.0.0/0 ) that points to an internet gateway, and a route for 172.31.0.0/16 IPv4 traffic that points to a peering connection ( pcx-11223344556677889 ). Any traffic from the subnet that\u2019s destined for the 172.31.0.0/16 IP address range uses the peering connection, because this route is more specific than the route for internet gateway. Any traffic destined for a target within the VPC ( 10.0.0.0/16 ) is covered by the Local route, and therefore is routed within the VPC. All other traffic from the subnet uses the internet gateway. Destination Target 10.0.0.0/16 Local 172.31.0.0/16 pcx-11223344556677889 0.0.0.0/0 igw-12345678901234567 Still more and more details about routes and routing tables, see: Example routing options Working with route tables","title":"Route priority"},{"location":"network2/#aws-direct-connect","text":"AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment , which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. This allows you to use the same connection to access public resources such as objects stored in Amazon S3 using public IP address space, and private resources such as Amazon EC2 instances running within an Amazon Virtual Private Cloud (VPC) using private IP space, while maintaining network separation between the public and private environments. Virtual interfaces can be reconfigured at any time to meet your changing needs.","title":"AWS Direct Connect"},{"location":"network2/#amazon-direct-connect-use-cases-and-benefits","text":"When to use AWS Direct Connect With AWS Direct Connect, you can connect to all your AWS resources in an AWS Region, transfer your business critical data directly from your datacenter, office, or colocation environment into and from AWS, bypassing your Internet service provider and removing network congestion. Further, AWS Direct Connect\u2019s simple pay as-you-go pricing and no minimum commitment means you pay only for the network ports you use and the data you transfer out of the AWS Region over the AWS Direct Connect connection, which can greatly reduce your networking costs. There are several use cases for which AWS Direct Connect is desirable, including: Working with Large Data Sets Real-time Data Feeds Hybrid Environments AWS Direct Connect Benefits Reduces Your Bandwidth Costs Consistent Network Performance Compatible with all AWS Services Private Connectivity to your Amazon VPC Elastic Simple For more details, read AWS Direct Connect features and AWS Direct Connect pricing .","title":"Amazon Direct Connect Use Cases and Benefits"},{"location":"network2/#virtual-private-gateway","text":"You can use an AWS Direct Connect gateway to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in any account that are located in the same or different Regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC. Then, you create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway. Consider the following while using the VPG : There are limits for creating and using Direct Connect gateways. The VPCs to which you connect through a Direct Connect gateway cannot have overlapping CIDR blocks. If you add an IPv4 CIDR block to a VPC that\u2019s associated with a Direct Connect gateway, ensure that the CIDR block does not overlap with an existing CIDR block for any other associated VPC. For more information, see Adding IPv4 CIDR Blocks to a VPC in the Amazon VPC User Guide. You cannot create a public virtual interface to a Direct Connect gateway. A Direct Connect gateway supports communication between attached private virtual interfaces and associated virtual private gateways only. The following traffic flows are not supported: Direct communication between the VPCs that are associated with a single Direct Connect gateway. This includes traffic from one VPC to another by using a hairpin through an on-premises network through a single Direct Connect gateway. Direct communication between the virtual interfaces that are attached to a single Direct Connect gateway. Direct communication between the virtual interfaces that are attached to a single Direct Connect gateway and a VPN connection on a virtual private gateway that\u2019s associated with the same Direct Connect gateway. You cannot associate a virtual private gateway with more than one Direct Connect gateway and you cannot attach a private virtual interface to more than one Direct Connect gateway. A virtual private gateway that you associate with a Direct Connect gateway must be attached to a VPC. A virtual private gateway association proposal expires 7 days after it is created. An accepted virtual private gateway proposal, or a deleted virtual private gateway proposal remains visible for 3 days. To connect your AWS Direct Connect connection to a VPC in the same Region only, you can create a Direct Connect gateway. Or, you can create a private virtual interface and attach it to the virtual private gateway for the VPC. For more information, see Creating a private virtual interface and VPN CloudHub . To use your AWS Direct Connect connection with a VPC in another account, you can create a hosted private virtual interface for that account. When the owner of the other account accepts the hosted virtual interface, they can choose to attach it either to a virtual private gateway or to a Direct Connect gateway in their account. For more information, see AWS Direct Connect virtual interfaces . Please see, how to: Creating a virtual private gateway Associating and disassociating virtual private gateways Creating a private virtual interface to the Direct Connect gateway Associating a virtual private gateway across accounts","title":"Virtual private gateway"},{"location":"network2/#aws-vpn","text":"AWS Virtual Private Network solutions establish secure connections between your on-premises networks, remote offices, client devices, and the AWS global network. AWS VPN is comprised of two services: AWS Site-to-Site VPN and AWS Client VPN. Together, they deliver a highly-available, managed, and elastic cloud VPN solution to protect your network traffic. AWS Site-to-Site VPN creates encrypted tunnels between your network and your Amazon Virtual Private Clouds or AWS Transit Gateways. For managing remote access, AWS Client VPN connects your users to AWS or on-premises resources using a VPN software client. Two flavors of the AWS VPN: AWS Client VPN AWS Site-to-Site VPN","title":"AWS VPN"},{"location":"network2/#aws-client-vpn","text":"AWS Client VPN is a fully-managed, elastic VPN service that automatically scales up or down based on user demand. Because it is a cloud VPN solution, you don\u2019t need to install and manage hardware or software-based solutions, or try to estimate how many remote users to support at one time. Benefits Fully managed Advanced authentication Elastic Remote access Use cases Quickly scale remote access Easily deploy and remove VPN access for temporary workers Easily access applications in the cloud or on premises How it works","title":"AWS Client VPN"},{"location":"network2/#aws-site-to-site-vpn","text":"AWS Site-to-Site VPN creates a secure connection between your data center or branch office and your AWS cloud resources. For globally distributed applications, the Accelerated Site-to-Site VPN option provides even greater performance by working with AWS Global Accelerator. Benefits Highly available Secure Robust monitoring Accelerate Applicationss Use cases Extend your corporate network into the cloud Secure communication between remote locations How it works","title":"AWS Site-to-Site VPN"},{"location":"network2/#vpn-cloudhub","text":"If you have multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub. This enables your remote sites to communicate with each other, and not just with the VPC. The VPN CloudHub operates on a simple hub-and-spoke model that you can use with or without a VPC. This design is suitable if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices. The sites must not have overlapping IP ranges. The following diagram shows the VPN CloudHub architecture, with blue dashed lines indicating network traffic between remote sites being routed over their Site-to-Site VPN connections. See, Providing secure communication between sites using VPN CloudHub for more details about how to configure the previous topology.","title":"VPN CloudHub"},{"location":"nosql/","text":"NoSQL Databases in Amazon Web Services \u00b6 NoSQL Databases \u00b6 High-performance, nonrelational databases with flexible data models What are NoSQL databases? \u00b6 NoSQL databases are purpose built for specific data models and have flexible schemas for building modern applications. NoSQL databases are widely recognized for their ease of development, functionality, and performance at scale. How Does a NoSQL (nonrelational) Database Work? \u00b6 NoSQL databases use a variety of data models for accessing and managing data. These types of databases are: optimized specifically for applications that require large data volume, low latency, and flexible data models, which are achieved by relaxing some of the data consistency restrictions of other databases. Consider the example of modeling the schema for a simple book database: In a relational database, a book record is often dissembled (or \u201cnormalized\u201d) and stored in separate tables, and relationships are defined by primary and foreign key constraints. In this example, the Books table has columns for ISBN, Book Title , and Edition Number , the Authors table has columns for AuthorID and Author Name , and finally the Author-ISBN table has columns for AuthorID and ISBN . The relational model is designed to enable the database to enforce referential integrity between tables in the database, normalized to reduce the redundancy, and generally optimized for storage. In a NoSQL database, a book record is usually stored as a JSON document. For each book, the item, ISBN, Book Title, Edition Number, Author Name, and AuthorID are stored as attributes in a single document. In this model, data is optimized for intuitive development and horizontal scalability. Why should you use a NoSQL database? \u00b6 NoSQL databases are a great fit for many modern applications such as: mobile, web, and gaming that require: flexible, scalable, high-performance, and highly functional databases to provide great user experiences. NoSQL databases have the following features: Flexibility : NoSQL databases generally provide flexible schemas that enable faster and more iterative development. The flexible data model makes NoSQL databases ideal for semi-structured and unstructured data. Scalability : NoSQL databases are generally designed to scale out by using distributed clusters of hardware instead of scaling up by adding expensive and robust servers. Some cloud providers handle these operations behind-the-scenes as a fully managed service. High-performance : NoSQL database are optimized for specific data models and access patterns that enable higher performance than trying to accomplish similar functionality with relational databases. Highly functional : NoSQL databases provide highly functional APIs and data types that are purpose built for each of their respective data models. Types of NoSQL Databases \u00b6 Key-value : Key-value databases are highly partitionable and allow horizontal scaling at scales that other types of databases cannot achieve. Use cases such as gaming, ad tech, and IoT lend themselves particularly well to the key-value data model. Amazon DynamoDB is designed to provide consistent single-digit millisecond latency for any scale of workloads. This consistent performance is a big part of why the Snapchat Stories feature , which includes Snapchat\u2019s largest storage write workload, moved to DynamoDB. Document : In application code, data is represented often as an object or JSON-like document because it is an efficient and intuitive data model for developers. Document databases make it easier for developers to store and query data in a database by using the same document model format that they use in their application code. The flexible, semistructured, and hierarchical nature of documents and document databases allows them to evolve with applications\u2019 needs. The document model works well with catalogs, user profiles, and content management systems where each document is unique and evolves over time. Amazon DocumentDB (with MongoDB compatibility) and MongoDB are popular document databases that provide powerful and intuitive APIs for flexible and iterative development. Graph : A graph database\u2019s purpose is to make it easy to build and run applications that work with highly connected datasets. Typical use cases for a graph database include social networking, recommendation engines, fraud detection, and knowledge graphs. Amazon Neptune is a fully-managed graph database service. Neptune supports both the Property Graph model and the Resource Description Framework (RDF), providing the choice of two graph APIs: TinkerPop and RDF/SPARQL. Popular graph databases include Neo4j and Giraph. In-memory : Gaming and ad-tech applications have use cases such as leaderboards, session stores, and real-time analytics that require microsecond response times and can have large spikes in traffic coming at any time. Amazon ElastiCache offers Memcached and Redis, to serve low-latency, high-throughput workloads, such as McDonald\u2019s , that cannot be served with disk-based data stores. Amazon DynamoDB Accelerator (DAX) is another example of a purpose-built data store. DAX makes DynamoDB reads an order of magnitude faster. Search : Many applications output logs to help developers troubleshoot issues. Amazon Elasticsearch Service (Amazon ES) is purpose built for providing near-real-time visualizations and analytics of machine-generated data by indexing, aggregating, and searching semistructured logs and metrics. Amazon ES also is a powerful, high-performance search engine for full-text search use cases. Expedia is using more than 150 Amazon ES domains, 30 TB of data, and 30 billion documents for a variety of mission-critical use cases, ranging from operational monitoring and troubleshooting to distributed application stack tracing and pricing optimization. SQL (relational) vs. NoSQL (nonrelational) databases \u00b6 For decades, the predominant data model that was used for application development was the relational data model used by relational databases such as Oracle, DB2, SQL Server, MySQL, and PostgreSQL. It wasn\u2019t until the mid to late 2000s that other data models began to gain significant adoption and usage. To differentiate and categorize these new classes of databases and data models, the term \u201cNoSQL\u201d was coined. Often the term \u201cNoSQL\u201d is used interchangeably with \u201cnonrelational.\u201d Though there are many types of NoSQL databases with varying features, the following table shows some of the differences between SQL and NoSQL databases. Relational databases NoSQL databases Optimal workloads Relational databases are designed for transactional and strongly consistent online transaction processing (OLTP) applications and are good for online analytical processing (OLAP). NoSQL databases are designed for a number of data access patterns that include low-latency applications. NoSQL search databases are designed for analytics over semi-structured data. Data model The relational model normalizes data into tables that are composed of rows and columns . A schema strictly defines the tables, rows, columns, indexes, relationships between tables, and other database elements. The database enforces the referential integrity in relationships between tables. NoSQL databases provide a variety of data models such as key-value , document , and graph , which are optimized for performance and scale. Performance Performance is generally dependent on the disk subsystem. The optimization of queries, indexes, and table structure is often required to achieve peak performance. Performance is generally a function of the underlying hardware cluster size, network latency, and the calling application. Scale Relational databases typically scale up by increasing the compute capabilities of the hardware or scale-out by adding replicas for read-only workloads. NoSQL databases typically are partitionable because access patterns are able to scale out by using distributed architecture to increase throughput that provides consistent performance at near boundless scale. APIs Requests to store and retrieve data are communicated using queries that conform to a structured query language (SQL). These queries are parsed and executed by the relational database. Object-based APIs allow app developers to easily store and retrieve data structures. Partition keys let apps look up key-value pairs, column sets, or semistructured documents that contain serialized app objects and attributes. SQL vs. NoSQL Terminology \u00b6 The following table compares terminology used by select NoSQL databases with terminology used by SQL databases. SQL MongoDB DynamoDB Cassandra Couchbase Table Collection Table Table Data bucket Row Document Item Row Document Column Field Attribute Column Field Primary key ObjectId Primary key Primary key Document ID Index Index Secondary index Index Index View View Global secondary index Materialized view View Nested table or object Embedded document Map Map Map Array Array List List List NoSQL Database Engines in AWS \u00b6 NoSQL BD AWS DB Engine Key-value DynamoDB Document DocumentDB \u2013 MongoDB Graph Neptune In-memory ElastiCache \u2013 Redis DB \u2013 Memcached Search ElastiSearch","title":"NoSQL DB"},{"location":"nosql/#nosql-databases-in-amazon-web-services","text":"","title":"NoSQL Databases in Amazon Web Services"},{"location":"nosql/#nosql-databases","text":"High-performance, nonrelational databases with flexible data models","title":"NoSQL Databases"},{"location":"nosql/#what-are-nosql-databases","text":"NoSQL databases are purpose built for specific data models and have flexible schemas for building modern applications. NoSQL databases are widely recognized for their ease of development, functionality, and performance at scale.","title":"What are NoSQL databases?"},{"location":"nosql/#how-does-a-nosql-nonrelational-database-work","text":"NoSQL databases use a variety of data models for accessing and managing data. These types of databases are: optimized specifically for applications that require large data volume, low latency, and flexible data models, which are achieved by relaxing some of the data consistency restrictions of other databases. Consider the example of modeling the schema for a simple book database: In a relational database, a book record is often dissembled (or \u201cnormalized\u201d) and stored in separate tables, and relationships are defined by primary and foreign key constraints. In this example, the Books table has columns for ISBN, Book Title , and Edition Number , the Authors table has columns for AuthorID and Author Name , and finally the Author-ISBN table has columns for AuthorID and ISBN . The relational model is designed to enable the database to enforce referential integrity between tables in the database, normalized to reduce the redundancy, and generally optimized for storage. In a NoSQL database, a book record is usually stored as a JSON document. For each book, the item, ISBN, Book Title, Edition Number, Author Name, and AuthorID are stored as attributes in a single document. In this model, data is optimized for intuitive development and horizontal scalability.","title":"How Does a NoSQL (nonrelational) Database Work?"},{"location":"nosql/#why-should-you-use-a-nosql-database","text":"NoSQL databases are a great fit for many modern applications such as: mobile, web, and gaming that require: flexible, scalable, high-performance, and highly functional databases to provide great user experiences. NoSQL databases have the following features: Flexibility : NoSQL databases generally provide flexible schemas that enable faster and more iterative development. The flexible data model makes NoSQL databases ideal for semi-structured and unstructured data. Scalability : NoSQL databases are generally designed to scale out by using distributed clusters of hardware instead of scaling up by adding expensive and robust servers. Some cloud providers handle these operations behind-the-scenes as a fully managed service. High-performance : NoSQL database are optimized for specific data models and access patterns that enable higher performance than trying to accomplish similar functionality with relational databases. Highly functional : NoSQL databases provide highly functional APIs and data types that are purpose built for each of their respective data models.","title":"Why should you use a NoSQL database?"},{"location":"nosql/#types-of-nosql-databases","text":"Key-value : Key-value databases are highly partitionable and allow horizontal scaling at scales that other types of databases cannot achieve. Use cases such as gaming, ad tech, and IoT lend themselves particularly well to the key-value data model. Amazon DynamoDB is designed to provide consistent single-digit millisecond latency for any scale of workloads. This consistent performance is a big part of why the Snapchat Stories feature , which includes Snapchat\u2019s largest storage write workload, moved to DynamoDB. Document : In application code, data is represented often as an object or JSON-like document because it is an efficient and intuitive data model for developers. Document databases make it easier for developers to store and query data in a database by using the same document model format that they use in their application code. The flexible, semistructured, and hierarchical nature of documents and document databases allows them to evolve with applications\u2019 needs. The document model works well with catalogs, user profiles, and content management systems where each document is unique and evolves over time. Amazon DocumentDB (with MongoDB compatibility) and MongoDB are popular document databases that provide powerful and intuitive APIs for flexible and iterative development. Graph : A graph database\u2019s purpose is to make it easy to build and run applications that work with highly connected datasets. Typical use cases for a graph database include social networking, recommendation engines, fraud detection, and knowledge graphs. Amazon Neptune is a fully-managed graph database service. Neptune supports both the Property Graph model and the Resource Description Framework (RDF), providing the choice of two graph APIs: TinkerPop and RDF/SPARQL. Popular graph databases include Neo4j and Giraph. In-memory : Gaming and ad-tech applications have use cases such as leaderboards, session stores, and real-time analytics that require microsecond response times and can have large spikes in traffic coming at any time. Amazon ElastiCache offers Memcached and Redis, to serve low-latency, high-throughput workloads, such as McDonald\u2019s , that cannot be served with disk-based data stores. Amazon DynamoDB Accelerator (DAX) is another example of a purpose-built data store. DAX makes DynamoDB reads an order of magnitude faster. Search : Many applications output logs to help developers troubleshoot issues. Amazon Elasticsearch Service (Amazon ES) is purpose built for providing near-real-time visualizations and analytics of machine-generated data by indexing, aggregating, and searching semistructured logs and metrics. Amazon ES also is a powerful, high-performance search engine for full-text search use cases. Expedia is using more than 150 Amazon ES domains, 30 TB of data, and 30 billion documents for a variety of mission-critical use cases, ranging from operational monitoring and troubleshooting to distributed application stack tracing and pricing optimization.","title":"Types of NoSQL Databases"},{"location":"nosql/#sql-relational-vs-nosql-nonrelational-databases","text":"For decades, the predominant data model that was used for application development was the relational data model used by relational databases such as Oracle, DB2, SQL Server, MySQL, and PostgreSQL. It wasn\u2019t until the mid to late 2000s that other data models began to gain significant adoption and usage. To differentiate and categorize these new classes of databases and data models, the term \u201cNoSQL\u201d was coined. Often the term \u201cNoSQL\u201d is used interchangeably with \u201cnonrelational.\u201d Though there are many types of NoSQL databases with varying features, the following table shows some of the differences between SQL and NoSQL databases. Relational databases NoSQL databases Optimal workloads Relational databases are designed for transactional and strongly consistent online transaction processing (OLTP) applications and are good for online analytical processing (OLAP). NoSQL databases are designed for a number of data access patterns that include low-latency applications. NoSQL search databases are designed for analytics over semi-structured data. Data model The relational model normalizes data into tables that are composed of rows and columns . A schema strictly defines the tables, rows, columns, indexes, relationships between tables, and other database elements. The database enforces the referential integrity in relationships between tables. NoSQL databases provide a variety of data models such as key-value , document , and graph , which are optimized for performance and scale. Performance Performance is generally dependent on the disk subsystem. The optimization of queries, indexes, and table structure is often required to achieve peak performance. Performance is generally a function of the underlying hardware cluster size, network latency, and the calling application. Scale Relational databases typically scale up by increasing the compute capabilities of the hardware or scale-out by adding replicas for read-only workloads. NoSQL databases typically are partitionable because access patterns are able to scale out by using distributed architecture to increase throughput that provides consistent performance at near boundless scale. APIs Requests to store and retrieve data are communicated using queries that conform to a structured query language (SQL). These queries are parsed and executed by the relational database. Object-based APIs allow app developers to easily store and retrieve data structures. Partition keys let apps look up key-value pairs, column sets, or semistructured documents that contain serialized app objects and attributes.","title":"SQL (relational) vs. NoSQL (nonrelational) databases"},{"location":"nosql/#sql-vs-nosql-terminology","text":"The following table compares terminology used by select NoSQL databases with terminology used by SQL databases. SQL MongoDB DynamoDB Cassandra Couchbase Table Collection Table Table Data bucket Row Document Item Row Document Column Field Attribute Column Field Primary key ObjectId Primary key Primary key Document ID Index Index Secondary index Index Index View View Global secondary index Materialized view View Nested table or object Embedded document Map Map Map Array Array List List List","title":"SQL vs. NoSQL Terminology"},{"location":"nosql/#nosql-database-engines-in-aws","text":"NoSQL BD AWS DB Engine Key-value DynamoDB Document DocumentDB \u2013 MongoDB Graph Neptune In-memory ElastiCache \u2013 Redis DB \u2013 Memcached Search ElastiSearch","title":"NoSQL Database Engines in AWS"},{"location":"othercomputeservices/","text":"Other AWS Compute Services \u00b6 Building and running your application starts with compute, whether you are building enterprise, cloud-native, or mobile applications; or running massive clusters to sequence the human genome. AWS offers a comprehensive portfolio of compute services that allow you to develop, deploy, run, and scale your applications and workloads in the world\u2019s most powerful, secure, and innovative compute cloud. Details about the full range of AWS compute services can be found here . Two other styles of computing may Hyddrosat is interested on them : serverless computing container services AWS Lambda \u00b6 AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume\u2013there is no charge when your code isn\u2019t running. Additional information about Lambda can be found at: https://aws.amazon.com/lambda Watch it here \u00b6 AWS Container Services \u00b6 Please check the Containers on AWS , The most secure, reliable, and scalable way to run containers 1. AWS ECS \u00b6 Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers. It allows you to run and scale containerized applications on AWS. You can find more details at: https://aws.amazon.com/ecs/ Please check: Deploy Docker Containers on Amazon Elastic Container Service (Amazon ECS) Running Docker on AWS EC2 Deploying Docker containers on ECS 2. Amazon EKS \u00b6 Amazon Elastic Container Service for Kubernetes (Amazon EKS) makes it straightforward to deploy, manage, and scale containerized applications that use Kubernetes on AWS. Details can be found at: https://aws.amazon.com/eks/ Deploy a Kubernetes Application with Amazon Elastic Container Service for Kubernetes 3. AWS Fargate \u00b6 AWS Fargate is a compute engine for Amazon ECS and Amazon EKS that allows you to run containers without having to manage servers or clusters. You can find more information at: https://aws.amazon.com/fargate/ Run your containers on AWS Fargate","title":"Other AWS Compute Services"},{"location":"othercomputeservices/#other-aws-compute-services","text":"Building and running your application starts with compute, whether you are building enterprise, cloud-native, or mobile applications; or running massive clusters to sequence the human genome. AWS offers a comprehensive portfolio of compute services that allow you to develop, deploy, run, and scale your applications and workloads in the world\u2019s most powerful, secure, and innovative compute cloud. Details about the full range of AWS compute services can be found here . Two other styles of computing may Hyddrosat is interested on them : serverless computing container services","title":"Other AWS Compute Services"},{"location":"othercomputeservices/#aws-lambda","text":"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume\u2013there is no charge when your code isn\u2019t running. Additional information about Lambda can be found at: https://aws.amazon.com/lambda","title":"AWS Lambda"},{"location":"othercomputeservices/#watch-it-here","text":"","title":"Watch it here"},{"location":"othercomputeservices/#aws-container-services","text":"Please check the Containers on AWS , The most secure, reliable, and scalable way to run containers","title":"AWS Container Services"},{"location":"othercomputeservices/#1-aws-ecs","text":"Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers. It allows you to run and scale containerized applications on AWS. You can find more details at: https://aws.amazon.com/ecs/ Please check: Deploy Docker Containers on Amazon Elastic Container Service (Amazon ECS) Running Docker on AWS EC2 Deploying Docker containers on ECS","title":"1. AWS ECS"},{"location":"othercomputeservices/#2-amazon-eks","text":"Amazon Elastic Container Service for Kubernetes (Amazon EKS) makes it straightforward to deploy, manage, and scale containerized applications that use Kubernetes on AWS. Details can be found at: https://aws.amazon.com/eks/ Deploy a Kubernetes Application with Amazon Elastic Container Service for Kubernetes","title":"2. Amazon EKS"},{"location":"othercomputeservices/#3-aws-fargate","text":"AWS Fargate is a compute engine for Amazon ECS and Amazon EKS that allows you to run containers without having to manage servers or clusters. You can find more information at: https://aws.amazon.com/fargate/ Run your containers on AWS Fargate","title":"3. AWS Fargate"},{"location":"pricing/","text":"Pricing and Cost Management in AWS \u00b6 This section shows the pricing and cost management of the AWS cloud services. See the slides that support you while choosing the services and managing your costs: When you use AWS, you spin up resources only when you need them, and you pay only for what you use. Instead of having a fixed cost like you would have in a traditional on-premises environment, you instead incur a variable cost. This is fundamentally different than the way costs were managed on-premise, and therefore, has shifted the way that costs are understood, managed, and optimized. To optimize costs, you want to establish best practices for cost management, and use tools to monitor and analyze costs over time. AWS offers multiple cost management tools through the AWS Management Console. Such as: AWS Cost Explorer AWS Budgets AWS Trusted Advisor AWS Pricing Calculator AWS Cost Explorer \u00b6 AWS Cost Explorer lets you visualize, understand, and manage your AWS costs and usage over time. You can create custom reports (including charts and tabular data) that analyze cost and usage data, both at a high level (e.g., total costs and usage across all accounts) and for highly specific requests (e.g., m2.2xlarge costs within account Y that are tagged project: secretProject). User interface for exploring your AWS Costs It provides breakdowns incluing: by service by cost tag Provides predictions for the next three monthes of costs Gives recommendations for cost optimization Can be accsssed via API AWS Trusted Advisor \u00b6 AWS Trusted Advisor is an online resource to help you reduce costs, increase performance, and improve security by optimizing your AWS environment. Trusted Advisor provides real-time guidance to help you provision your resources by following our best practices. AWS Pricing Calculator \u00b6 Also called Total Ownership Calculator ( TOC ), AWS has announced a new pricing tool, the AWS Pricing Calculator . This new tool, which is currently in beta, can be used to calculate Amazon EC2 and Amazon EBS pricing. For further information about this tool, see this blog post. For other services, you can use the AWS Simple Monthly Calculator . AWS Budgets \u00b6 AWS Budgets allows you to create alerts when your costs or usage exceeds a threshold over a specific time period. You define the time period, whether it be a week, month, or year, and you also define your thresholds. When you exceed the threshold, a notification can be sent out. Refrences \u00b6 How does AWS pricing work? AWS Total Cost of Ownership (TCO) Calculator","title":"AWS Pricing"},{"location":"pricing/#pricing-and-cost-management-in-aws","text":"This section shows the pricing and cost management of the AWS cloud services. See the slides that support you while choosing the services and managing your costs: When you use AWS, you spin up resources only when you need them, and you pay only for what you use. Instead of having a fixed cost like you would have in a traditional on-premises environment, you instead incur a variable cost. This is fundamentally different than the way costs were managed on-premise, and therefore, has shifted the way that costs are understood, managed, and optimized. To optimize costs, you want to establish best practices for cost management, and use tools to monitor and analyze costs over time. AWS offers multiple cost management tools through the AWS Management Console. Such as: AWS Cost Explorer AWS Budgets AWS Trusted Advisor AWS Pricing Calculator","title":"Pricing and Cost Management in AWS"},{"location":"pricing/#aws-cost-explorer","text":"AWS Cost Explorer lets you visualize, understand, and manage your AWS costs and usage over time. You can create custom reports (including charts and tabular data) that analyze cost and usage data, both at a high level (e.g., total costs and usage across all accounts) and for highly specific requests (e.g., m2.2xlarge costs within account Y that are tagged project: secretProject). User interface for exploring your AWS Costs It provides breakdowns incluing: by service by cost tag Provides predictions for the next three monthes of costs Gives recommendations for cost optimization Can be accsssed via API","title":"AWS Cost Explorer"},{"location":"pricing/#aws-trusted-advisor","text":"AWS Trusted Advisor is an online resource to help you reduce costs, increase performance, and improve security by optimizing your AWS environment. Trusted Advisor provides real-time guidance to help you provision your resources by following our best practices.","title":"AWS Trusted Advisor"},{"location":"pricing/#aws-pricing-calculator","text":"Also called Total Ownership Calculator ( TOC ), AWS has announced a new pricing tool, the AWS Pricing Calculator . This new tool, which is currently in beta, can be used to calculate Amazon EC2 and Amazon EBS pricing. For further information about this tool, see this blog post. For other services, you can use the AWS Simple Monthly Calculator .","title":"AWS Pricing Calculator"},{"location":"pricing/#aws-budgets","text":"AWS Budgets allows you to create alerts when your costs or usage exceeds a threshold over a specific time period. You define the time period, whether it be a week, month, or year, and you also define your thresholds. When you exceed the threshold, a notification can be sent out.","title":"AWS Budgets"},{"location":"pricing/#refrences","text":"How does AWS pricing work? AWS Total Cost of Ownership (TCO) Calculator","title":"Refrences"},{"location":"ref/","text":"REFERENCES \u00b6 All Documentations All White Papers AWS Global Infrastructure AWS Free Tier Services AWS Solutions Library Find the hands-on tutorials for your AWS needs","title":"References"},{"location":"ref/#references","text":"All Documentations All White Papers AWS Global Infrastructure AWS Free Tier Services AWS Solutions Library Find the hands-on tutorials for your AWS needs","title":"REFERENCES"},{"location":"security/","text":"","title":"AWS Security"},{"location":"slides/","text":"The Hydrosat Training Sessions Resources \u00b6 The resources in this training is consisting of three sections, the Slides, Tutorials, and videos. Slides \u00b6 Understanding Cloud Concepts Reviewing AWS Core Services AWS Computing, Storage, and Databases AWS Networking and Load Balancing AWS Pricing and Support AWS Security and Access Management Cloud Pricing Appendix \u00b6 Understanding AWS Core Services List AWS Security & Architecture Services List Network Addressing Tutorials \u00b6 Start working on AWS by creating an account AWS Computing Run Python Scripts on EC2 Storage volumes in AWS Relational databases in AWS Docker Images on AWS Networking in AWS Load balancing and Scalling in AWS AWS Pricing and Support AWS Security and Access Management Videos \u00b6 Generating SSH key pairs for accessing EC2 instances Set up AWS budget alert after logged in through Console Running Python Script on EC2 Instance, connection established through Linux machine Create, attach, and mount a new EBS volume Detach, unmount, and delete an existing EBS volume Scripts \u00b6 Apache: Webserver installation Linux bash script (Script_linux.sh) Apache: Webserver installation Ubuntu bash script (Script_ubuntu_1.sh) Python web flax: Webserver installation Ubuntu bash script (test.sh) User data script for \u201cPython web flax: Webserver installation Ubuntu bash script\u201d (script_ubuntu.sh) Welcome Hydrosat web page (hydro.html) Script for VPC demos Script for RDS database testing and demo MySql database connection information file","title":"Training Resources"},{"location":"slides/#the-hydrosat-training-sessions-resources","text":"The resources in this training is consisting of three sections, the Slides, Tutorials, and videos.","title":"The Hydrosat Training Sessions Resources"},{"location":"slides/#slides","text":"Understanding Cloud Concepts Reviewing AWS Core Services AWS Computing, Storage, and Databases AWS Networking and Load Balancing AWS Pricing and Support AWS Security and Access Management Cloud Pricing","title":"Slides"},{"location":"slides/#appendix","text":"Understanding AWS Core Services List AWS Security & Architecture Services List Network Addressing","title":"Appendix"},{"location":"slides/#tutorials","text":"Start working on AWS by creating an account AWS Computing Run Python Scripts on EC2 Storage volumes in AWS Relational databases in AWS Docker Images on AWS Networking in AWS Load balancing and Scalling in AWS AWS Pricing and Support AWS Security and Access Management","title":"Tutorials"},{"location":"slides/#videos","text":"Generating SSH key pairs for accessing EC2 instances Set up AWS budget alert after logged in through Console Running Python Script on EC2 Instance, connection established through Linux machine Create, attach, and mount a new EBS volume Detach, unmount, and delete an existing EBS volume","title":"Videos"},{"location":"slides/#scripts","text":"Apache: Webserver installation Linux bash script (Script_linux.sh) Apache: Webserver installation Ubuntu bash script (Script_ubuntu_1.sh) Python web flax: Webserver installation Ubuntu bash script (test.sh) User data script for \u201cPython web flax: Webserver installation Ubuntu bash script\u201d (script_ubuntu.sh) Welcome Hydrosat web page (hydro.html) Script for VPC demos Script for RDS database testing and demo MySql database connection information file","title":"Scripts"},{"location":"storage/","text":"Storage in the AWS Web Services \u00b6 Amazon EC2 provides you with flexible, cost effective, and easy-to-use data storage options for your instances. Each option has a unique combination of performance and durability. These storage options can be used independently or in combination to suit your requirements. AWS Storage Options \u00b6 These storage options include the following: Amazon Elastic Block Store (Amazon EBS) Amazon EC2 instance store Amazon Elastic File System (Amazon EFS) Amazon Simple Storage Service (Amazon S3) The following figure shows the relationship between these storage options and your instance. Amazon EBS \u00b6 Amazon EBS provides durable, block-level storage volumes that you can attach to a running instance. You can use Amazon EBS as a primary storage device for data that requires frequent and granular updates. For example, Amazon EBS is the recommended storage option when you run a database on an instance. An EBS volume behaves like a raw, unformatted, external block device that you can attach to a single instance. The volume persists independently from the running life of an instance. After an EBS volume is attached to an instance, you can use it like any other physical hard drive. As illustrated in the previous figure, multiple volumes can be attached to an instance. You can also detach an EBS volume from one instance and attach it to another instance. You can dynamically change the configuration of a volume attached to an instance. To keep a backup copy of your data, you can create a snapshot of an EBS volume, which is stored in Amazon S3. You can create an EBS volume from a snapshot, and attach it to another instance. For more information, see Amazon Elastic Block Store (Amazon EBS) . Amazon EC2 instance store \u00b6 Many instances can access storage from disks that are physically attached to the host computer. This disk storage is referred to as instance store . Instance store provides temporary block-level storage for instances. The data on an instance store volume persists only during the life of the associated instance; if you stop or terminate an instance, any data on instance store volumes is lost. For more information, see Amazon EC2 instance store . Amazon EFS file system \u00b6 Amazon EFS provides scalable file storage for use with Amazon EC2. You can create an EFS file system and configure your instances to mount the file system. You can use an EFS file system as a common data source for workloads and applications running on multiple instances. For more information, see Amazon Elastic File System (Amazon EFS) . Amazon S3 \u00b6 Amazon S3 provides access to reliable and inexpensive data storage infrastructure. It is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within Amazon EC2 or anywhere on the web. For example, you can use Amazon S3 to store backup copies of your data and applications. Amazon EC2 uses Amazon S3 to store EBS snapshots and instance store-backed AMIs. For more information, see Amazon Simple Storage Service (Amazon S3) . Adding Storage \u00b6 Every time you launch an instance from an AMI, a root storage device is created for that instance. The root storage device contains all the information necessary to boot the instance. You can specify storage volumes in addition to the root device volume when you create an AMI or launch an instance using block device mapping . For more information, see Block device mapping . You can also attach EBS volumes to a running instance. For more information, see Attaching an Amazon EBS volume to an instance . Storage Pricing \u00b6 For information about storage pricing, open AWS Pricing , scroll down to Services Pricing , choose Storage , and then choose the storage option to open that storage option\u2019s pricing page. For information about estimating the cost of storage, see the AWS Pricing Calculator .","title":"Dealing with AWS Storage"},{"location":"storage/#storage-in-the-aws-web-services","text":"Amazon EC2 provides you with flexible, cost effective, and easy-to-use data storage options for your instances. Each option has a unique combination of performance and durability. These storage options can be used independently or in combination to suit your requirements.","title":"Storage in the AWS Web Services"},{"location":"storage/#aws-storage-options","text":"These storage options include the following: Amazon Elastic Block Store (Amazon EBS) Amazon EC2 instance store Amazon Elastic File System (Amazon EFS) Amazon Simple Storage Service (Amazon S3) The following figure shows the relationship between these storage options and your instance.","title":"AWS Storage Options"},{"location":"storage/#amazon-ebs","text":"Amazon EBS provides durable, block-level storage volumes that you can attach to a running instance. You can use Amazon EBS as a primary storage device for data that requires frequent and granular updates. For example, Amazon EBS is the recommended storage option when you run a database on an instance. An EBS volume behaves like a raw, unformatted, external block device that you can attach to a single instance. The volume persists independently from the running life of an instance. After an EBS volume is attached to an instance, you can use it like any other physical hard drive. As illustrated in the previous figure, multiple volumes can be attached to an instance. You can also detach an EBS volume from one instance and attach it to another instance. You can dynamically change the configuration of a volume attached to an instance. To keep a backup copy of your data, you can create a snapshot of an EBS volume, which is stored in Amazon S3. You can create an EBS volume from a snapshot, and attach it to another instance. For more information, see Amazon Elastic Block Store (Amazon EBS) .","title":"Amazon EBS"},{"location":"storage/#amazon-ec2-instance-store","text":"Many instances can access storage from disks that are physically attached to the host computer. This disk storage is referred to as instance store . Instance store provides temporary block-level storage for instances. The data on an instance store volume persists only during the life of the associated instance; if you stop or terminate an instance, any data on instance store volumes is lost. For more information, see Amazon EC2 instance store .","title":"Amazon EC2 instance store"},{"location":"storage/#amazon-efs-file-system","text":"Amazon EFS provides scalable file storage for use with Amazon EC2. You can create an EFS file system and configure your instances to mount the file system. You can use an EFS file system as a common data source for workloads and applications running on multiple instances. For more information, see Amazon Elastic File System (Amazon EFS) .","title":"Amazon EFS file system"},{"location":"storage/#amazon-s3","text":"Amazon S3 provides access to reliable and inexpensive data storage infrastructure. It is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within Amazon EC2 or anywhere on the web. For example, you can use Amazon S3 to store backup copies of your data and applications. Amazon EC2 uses Amazon S3 to store EBS snapshots and instance store-backed AMIs. For more information, see Amazon Simple Storage Service (Amazon S3) .","title":"Amazon S3"},{"location":"storage/#adding-storage","text":"Every time you launch an instance from an AMI, a root storage device is created for that instance. The root storage device contains all the information necessary to boot the instance. You can specify storage volumes in addition to the root device volume when you create an AMI or launch an instance using block device mapping . For more information, see Block device mapping . You can also attach EBS volumes to a running instance. For more information, see Attaching an Amazon EBS volume to an instance .","title":"Adding Storage"},{"location":"storage/#storage-pricing","text":"For information about storage pricing, open AWS Pricing , scroll down to Services Pricing , choose Storage , and then choose the storage option to open that storage option\u2019s pricing page. For information about estimating the cost of storage, see the AWS Pricing Calculator .","title":"Storage Pricing"},{"location":"support/","text":"Support Plans in AWS \u00b6 This section shows the pricing and cost management of the AWS cloud services. See the slides that support you while choosing the services and managing your supporting plans: AWS support plans are designed to give you the right mix of tools and access to expertise so that you can be successful with AWS while optimizing performance, managing risk, and keeping costs under control. Four support plans in AWS \u00b6 As follow: Basic Support Developer Support Business Support Enterprise Support In order to compare the 4 plans, please check the AWS Support Plans . Basic Support \u00b6 This plan of support is included for all AWS customers and includes: Customer Service and Communities - 24x7 access to customer service, documentation , whitepapers , and support forums . AWS Trusted Advisor - Access to the 7 core Trusted Advisor checks and guidance to provision your resources following best practices to increase performance and improve security. AWS Personal Health Dashboard - A personalized view of the health of AWS services, and alerts when your resources are impacted.","title":"AWS Support"},{"location":"support/#support-plans-in-aws","text":"This section shows the pricing and cost management of the AWS cloud services. See the slides that support you while choosing the services and managing your supporting plans: AWS support plans are designed to give you the right mix of tools and access to expertise so that you can be successful with AWS while optimizing performance, managing risk, and keeping costs under control.","title":"Support Plans in AWS"},{"location":"support/#four-support-plans-in-aws","text":"As follow: Basic Support Developer Support Business Support Enterprise Support In order to compare the 4 plans, please check the AWS Support Plans .","title":"Four support plans in AWS"},{"location":"support/#basic-support","text":"This plan of support is included for all AWS customers and includes: Customer Service and Communities - 24x7 access to customer service, documentation , whitepapers , and support forums . AWS Trusted Advisor - Access to the 7 core Trusted Advisor checks and guidance to provision your resources following best practices to increase performance and improve security. AWS Personal Health Dashboard - A personalized view of the health of AWS services, and alerts when your resources are impacted.","title":"Basic Support"},{"location":"vpc/","text":"Getting started with Amazon VPCs \u00b6 Different VPC Topology Configurations \u00b6 Two different topoplogies for the AWS VPCs, such as: Accessing the internet through the VPC Accessing a corporate or home network (on-premises Datacenter) through the VPC Accessing the internet \u00b6 You control how the instances that you launch into a VPC access resources outside the VPC. Your default VPC includes an internet gateway, and each default subnet is a public subnet. Each instance that you launch into a default subnet has a private IPv4 address and a public IPv4 address. These instances can communicate with the internet through the internet gateway. An internet gateway enables your instances to connect to the internet through the Amazon EC2 network edge. By default, each instance that you launch into a nondefault subnet has a private IPv4 address, but no public IPv4 address, unless you specifically assign one at launch, or you modify the subnet\u2019s public IP address attribute. These instances can communicate with each other, but can\u2019t access the internet. You can enable internet access for an instance launched into a nondefault subnet by attaching an internet gateway to its VPC (if its VPC is not a default VPC) and associating an Elastic IP address with the instance. Alternatively, to allow an instance in your VPC to initiate outbound connections to the internet but prevent unsolicited inbound connections from the internet, you can use a network address translation (NAT) device for IPv4 traffic. NAT maps multiple private IPv4 addresses to a single public IPv4 address. A NAT device has an Elastic IP address and is connected to the internet through an internet gateway. You can connect an instance in a private subnet to the internet through the NAT device, which routes traffic from the instance to the internet gateway, and routes any responses to the instance. If you associate an IPv6 CIDR block with your VPC and assign IPv6 addresses to your instances, instances can connect to the internet over IPv6 through an internet gateway. Alternatively, instances can initiate outbound connections to the internet over IPv6 using an egress-only internet gateway. IPv6 traffic is separate from IPv4 traffic; your route tables must include separate routes for IPv6 traffic. Accessing a corporate or home network \u00b6 You can optionally connect your VPC to your own corporate data center using an IPsec AWS Site-to-Site VPN connection, making the AWS Cloud an extension of your data center. A Site-to-Site VPN connection consists of two VPN tunnels between a virtual private gateway or transit gateway on the AWS side, and a customer gateway device located in your data center. A customer gateway device is a physical device or software appliance that you configure on your side of the Site-to-Site VPN connection. Amazon VPC console wizard configurations \u00b6 You can use the Amazon VPC Console wizard to create one of the following nondefault VPC configurations: VPC with a single public subnet VPC with public and private subnets (NAT) VPC with public and private subnets and AWS Site-to-Site VPN access VPC with a private subnet only and AWS Site-to-Site VPN access Working with VPCs and subnets \u00b6 The following procedures are for manually creating a VPC and subnets. You also have to manually add gateways and routing tables. Alternatively, you can use the Amazon VPC wizard to create a VPC plus its subnets, gateways, and routing tables in one step. For more information, see Examples for VPC and How to work with VPCs and Subnets . Creating a VPC Creating a subnet in your VPC Associating a secondary IPv4 CIDR block with your VPC Associating an IPv6 CIDR block with your VPC Associating an IPv6 CIDR block with your subnet Launching an instance into your subnet Deleting your subnet Disassociating an IPv4 CIDR block from your VPC Disassociating an IPv6 CIDR block from your VPC or subnet Deleting your VPC Adding an internet gateway to your VPC \u00b6 The following describes how to manually create a public subnet and attach an internet gateway to your VPC to support internet access. Creating a subnet Creating and attaching an internet gateway Creating a custom route table Creating a security group for internet access Adding Elastic IP addresses Detaching an internet gateway from your VPC Deleting an internet gateway API and command overview Adding other Gateways to your VPC \u00b6 There are other types of gateways, you may need during the planning of your VPCs in AWS, such as: Virtual private gateways Egress-only internet gateways Carrier gateways Working with route tables \u00b6 The following tasks show you how to work with route tables. /!\\ Note When you use the VPC wizard in the console to create a VPC with a gateway, the wizard automatically updates the route tables to use the gateway. If you are using the command line tools or API to set up your VPC, you must update the route tables yourself. Determining which route table a subnet is associated with Determining which subnets and or gateways are explicitly associated with a table Creating a custom route table Adding and removing routes from a route table Deleting a route table and more \u2026 Working with shared VPCs \u00b6 VPC sharing allows multiple AWS accounts to create their application resources, such as Amazon EC2 instances, Amazon Relational Database Service (RDS) databases, Amazon Redshift clusters, and AWS Lambda functions, into shared, centrally-managed Amazon Virtual Private Clouds (VPCs). In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner. You can share Amazon VPCs to leverage the implicit routing within a VPC for applications that require a high degree of interconnectivity and are within the same trust boundaries. This reduces the number of VPCs that you create and manage, while using separate accounts for billing and access control. You can simplify network topologies by interconnecting shared Amazon VPCs using connectivity features, such as AWS PrivateLink, AWS Transit Gateway, and Amazon VPC peering. For more information about VPC sharing benefits, see VPC sharing: A new approach to multiple accounts and VPC management . Sharing the VPCs \u00b6 The following will help you to share your VPC with your other accounts: Shared VPCs prerequisites Sharing a subnet Unsharing a shared subnet Identifying the owner of a shared subnet And more \u2026 Example: Sharing public subnets and private subnets \u00b6 Consider this scenario where you want an account to be responsible for the infrastructure, including subnets, route tables, gateways, and CIDR ranges and other accounts that are in the same AWS Organization to use the subnets. A VPC owner (Account A) creates the routing infrastructure, including the VPCs, subnets, route tables, gateways, and network ACLs. Account D wants to create public facing applications. Account B and Account C want to create private applications that do not need to connect to the internet and should reside in private subnets. Account A can use AWS Resource Access Manager to create a Resource Share for the subnets and then share the subnets. Account A shares the public subnet with Account D and the private subnet with Account B, and Account C. Account B, Account C, and Account D can create resources in the subnets. Each account can only see the subnets that are shared with them, for example, Account D can only see the public subnet. Each of the accounts can control their resources, including instances, and security groups. Account A manages the IP infrastructure, including the route tables for the public subnets, and the private subnets. There is no additional configuration required for shared subnets, so the route tables are the same as unshared subnet route tables. Account A (Account ID 111111111111) shares the public subnet with Account D (444444444444). Account D sees the following subnet, and the Owner column provides two indicators that the subnet is shared. The Account ID is the VPC owner (111111111111) and is different from Account D\u2019s ID (444444444444). The word \u201cshared\u201d appears beside the owner account ID. Security best practices for your VPC \u00b6 The following best practices are general guidelines and don\u2019t represent a complete security solution. Because these best practices might not be appropriate or sufficient for your environment, treat them as helpful considerations rather than prescriptions. The following are general best practices: Use multiple Availability Zone deployments so you have high availability. Use security groups and network ACLs. For more information, see Security groups for your VPC and Network ACLs . Use IAM policies to control access. Use Amazon CloudWatch to monitor your VPC components and VPN connections. Use flow logs to capture information about IP traffic going to and from network interfaces in your VPC. For more information, see VPC Flow Logs . Additional resources \u00b6 Manage access to AWS resources and APIs using identity federation, IAM users, and IAM roles. Establish credential management policies and procedures for creating, distributing, rotating, and revoking AWS access credentials. For more information, see IAM best practices in the IAM User Guide. For answers to frequently asked questions for VPC security, see Amazon VPC FAQs .","title":"Getting started with VPCs"},{"location":"vpc/#getting-started-with-amazon-vpcs","text":"","title":"Getting started with Amazon VPCs"},{"location":"vpc/#different-vpc-topology-configurations","text":"Two different topoplogies for the AWS VPCs, such as: Accessing the internet through the VPC Accessing a corporate or home network (on-premises Datacenter) through the VPC","title":"Different VPC Topology Configurations"},{"location":"vpc/#accessing-the-internet","text":"You control how the instances that you launch into a VPC access resources outside the VPC. Your default VPC includes an internet gateway, and each default subnet is a public subnet. Each instance that you launch into a default subnet has a private IPv4 address and a public IPv4 address. These instances can communicate with the internet through the internet gateway. An internet gateway enables your instances to connect to the internet through the Amazon EC2 network edge. By default, each instance that you launch into a nondefault subnet has a private IPv4 address, but no public IPv4 address, unless you specifically assign one at launch, or you modify the subnet\u2019s public IP address attribute. These instances can communicate with each other, but can\u2019t access the internet. You can enable internet access for an instance launched into a nondefault subnet by attaching an internet gateway to its VPC (if its VPC is not a default VPC) and associating an Elastic IP address with the instance. Alternatively, to allow an instance in your VPC to initiate outbound connections to the internet but prevent unsolicited inbound connections from the internet, you can use a network address translation (NAT) device for IPv4 traffic. NAT maps multiple private IPv4 addresses to a single public IPv4 address. A NAT device has an Elastic IP address and is connected to the internet through an internet gateway. You can connect an instance in a private subnet to the internet through the NAT device, which routes traffic from the instance to the internet gateway, and routes any responses to the instance. If you associate an IPv6 CIDR block with your VPC and assign IPv6 addresses to your instances, instances can connect to the internet over IPv6 through an internet gateway. Alternatively, instances can initiate outbound connections to the internet over IPv6 using an egress-only internet gateway. IPv6 traffic is separate from IPv4 traffic; your route tables must include separate routes for IPv6 traffic.","title":"Accessing the internet"},{"location":"vpc/#accessing-a-corporate-or-home-network","text":"You can optionally connect your VPC to your own corporate data center using an IPsec AWS Site-to-Site VPN connection, making the AWS Cloud an extension of your data center. A Site-to-Site VPN connection consists of two VPN tunnels between a virtual private gateway or transit gateway on the AWS side, and a customer gateway device located in your data center. A customer gateway device is a physical device or software appliance that you configure on your side of the Site-to-Site VPN connection.","title":"Accessing a corporate or home network"},{"location":"vpc/#amazon-vpc-console-wizard-configurations","text":"You can use the Amazon VPC Console wizard to create one of the following nondefault VPC configurations: VPC with a single public subnet VPC with public and private subnets (NAT) VPC with public and private subnets and AWS Site-to-Site VPN access VPC with a private subnet only and AWS Site-to-Site VPN access","title":"Amazon VPC console wizard configurations"},{"location":"vpc/#working-with-vpcs-and-subnets","text":"The following procedures are for manually creating a VPC and subnets. You also have to manually add gateways and routing tables. Alternatively, you can use the Amazon VPC wizard to create a VPC plus its subnets, gateways, and routing tables in one step. For more information, see Examples for VPC and How to work with VPCs and Subnets . Creating a VPC Creating a subnet in your VPC Associating a secondary IPv4 CIDR block with your VPC Associating an IPv6 CIDR block with your VPC Associating an IPv6 CIDR block with your subnet Launching an instance into your subnet Deleting your subnet Disassociating an IPv4 CIDR block from your VPC Disassociating an IPv6 CIDR block from your VPC or subnet Deleting your VPC","title":"Working with VPCs and subnets"},{"location":"vpc/#adding-an-internet-gateway-to-your-vpc","text":"The following describes how to manually create a public subnet and attach an internet gateway to your VPC to support internet access. Creating a subnet Creating and attaching an internet gateway Creating a custom route table Creating a security group for internet access Adding Elastic IP addresses Detaching an internet gateway from your VPC Deleting an internet gateway API and command overview","title":"Adding an internet gateway to your VPC"},{"location":"vpc/#adding-other-gateways-to-your-vpc","text":"There are other types of gateways, you may need during the planning of your VPCs in AWS, such as: Virtual private gateways Egress-only internet gateways Carrier gateways","title":"Adding other Gateways to your VPC"},{"location":"vpc/#working-with-route-tables","text":"The following tasks show you how to work with route tables. /!\\ Note When you use the VPC wizard in the console to create a VPC with a gateway, the wizard automatically updates the route tables to use the gateway. If you are using the command line tools or API to set up your VPC, you must update the route tables yourself. Determining which route table a subnet is associated with Determining which subnets and or gateways are explicitly associated with a table Creating a custom route table Adding and removing routes from a route table Deleting a route table and more \u2026","title":"Working with route tables"},{"location":"vpc/#working-with-shared-vpcs","text":"VPC sharing allows multiple AWS accounts to create their application resources, such as Amazon EC2 instances, Amazon Relational Database Service (RDS) databases, Amazon Redshift clusters, and AWS Lambda functions, into shared, centrally-managed Amazon Virtual Private Clouds (VPCs). In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner. You can share Amazon VPCs to leverage the implicit routing within a VPC for applications that require a high degree of interconnectivity and are within the same trust boundaries. This reduces the number of VPCs that you create and manage, while using separate accounts for billing and access control. You can simplify network topologies by interconnecting shared Amazon VPCs using connectivity features, such as AWS PrivateLink, AWS Transit Gateway, and Amazon VPC peering. For more information about VPC sharing benefits, see VPC sharing: A new approach to multiple accounts and VPC management .","title":"Working with shared VPCs"},{"location":"vpc/#sharing-the-vpcs","text":"The following will help you to share your VPC with your other accounts: Shared VPCs prerequisites Sharing a subnet Unsharing a shared subnet Identifying the owner of a shared subnet And more \u2026","title":"Sharing the VPCs"},{"location":"vpc/#example-sharing-public-subnets-and-private-subnets","text":"Consider this scenario where you want an account to be responsible for the infrastructure, including subnets, route tables, gateways, and CIDR ranges and other accounts that are in the same AWS Organization to use the subnets. A VPC owner (Account A) creates the routing infrastructure, including the VPCs, subnets, route tables, gateways, and network ACLs. Account D wants to create public facing applications. Account B and Account C want to create private applications that do not need to connect to the internet and should reside in private subnets. Account A can use AWS Resource Access Manager to create a Resource Share for the subnets and then share the subnets. Account A shares the public subnet with Account D and the private subnet with Account B, and Account C. Account B, Account C, and Account D can create resources in the subnets. Each account can only see the subnets that are shared with them, for example, Account D can only see the public subnet. Each of the accounts can control their resources, including instances, and security groups. Account A manages the IP infrastructure, including the route tables for the public subnets, and the private subnets. There is no additional configuration required for shared subnets, so the route tables are the same as unshared subnet route tables. Account A (Account ID 111111111111) shares the public subnet with Account D (444444444444). Account D sees the following subnet, and the Owner column provides two indicators that the subnet is shared. The Account ID is the VPC owner (111111111111) and is different from Account D\u2019s ID (444444444444). The word \u201cshared\u201d appears beside the owner account ID.","title":"Example: Sharing public subnets and private subnets"},{"location":"vpc/#security-best-practices-for-your-vpc","text":"The following best practices are general guidelines and don\u2019t represent a complete security solution. Because these best practices might not be appropriate or sufficient for your environment, treat them as helpful considerations rather than prescriptions. The following are general best practices: Use multiple Availability Zone deployments so you have high availability. Use security groups and network ACLs. For more information, see Security groups for your VPC and Network ACLs . Use IAM policies to control access. Use Amazon CloudWatch to monitor your VPC components and VPN connections. Use flow logs to capture information about IP traffic going to and from network interfaces in your VPC. For more information, see VPC Flow Logs .","title":"Security best practices for your VPC"},{"location":"vpc/#additional-resources","text":"Manage access to AWS resources and APIs using identity federation, IAM users, and IAM roles. Establish credential management policies and procedures for creating, distributing, rotating, and revoking AWS access credentials. For more information, see IAM best practices in the IAM User Guide. For answers to frequently asked questions for VPC security, see Amazon VPC FAQs .","title":"Additional resources"}]}